{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GQA (Grouped Query Attention) Forward Shape Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/root/vermind')\n",
    "\n",
    "from vermind_models import VerMindConfig, RMSNorm, precompute_freqs_cis, apply_rotary_pos_emb, repeat_kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GQAAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Grouped Query Attention 实现\n",
    "    - Q (Query) heads: 8\n",
    " - KV (Key/Value) heads: 2\n",
    " - 每个KV head 复用 4 次给 Q heads (8 / 2 = 4)\n",
    " \"\"\"\n",
    "    def __init__(self, args: VerMindConfig):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = args.num_attention_heads\n",
    "        self.num_key_value_heads = args.num_key_value_heads\n",
    "        self.n_local_heads = args.num_attention_heads\n",
    "        self.n_local_kv_heads = args.num_key_value_heads\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = args.hidden_size // args.num_attention_heads\n",
    "        \n",
    "        print(f\"=== GQA Attention Configuration ===\")\n",
    "        print(f\"  num_attention_heads (Q): {self.num_attention_heads}\")\n",
    "        print(f\"  num_key_value_heads (K/V): {self.num_key_value_heads}\")\n",
    "        print(f\"  n_rep (repetition): {self.n_rep}\")\n",
    "        print(f\"  head_dim: {self.head_dim}\")\n",
    "        print(f\"  hidden_size: {args.hidden_size}\")\n",
    "        print()\n",
    "        \n",
    "        self.q_proj = nn.Linear(args.hidden_size, args.num_attention_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(args.hidden_size, args.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(args.hidden_size, args.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(args.num_attention_heads * self.head_dim, args.hidden_size, bias=False)\n",
    "        self.dropout = args.dropout\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                position_embeddings: tuple = None,\n",
    "                attention_mask: torch.Tensor = None):\n",
    "        \n",
    "        bsz, seq_len, hidden_size = x.shape\n",
    "        print(f\"[Input] x shape: {x.shape}  (batch_size={bsz}, seq_len={seq_len}, hidden_size={hidden_size})\")\n",
    "        \n",
    "        # ========== 1. Linear Projections ==========\n",
    "        xq = self.q_proj(x)\n",
    "        xk = self.k_proj(x)\n",
    "        xv = self.v_proj(x)\n",
    "        \n",
    "        print(f\"\\n[1] Linear Projections:\")\n",
    "        print(f\"    xq (Q after q_proj): {xq.shape}  → [bsz, seq_len, num_heads * head_dim]\")\n",
    "        print(f\"    xk (K after k_proj): {xk.shape}  → [bsz, seq_len, num_kv_heads * head_dim]\")\n",
    "        print(f\"    xv (V after v_proj): {xv.shape}  → [bsz, seq_len, num_kv_heads * head_dim]\")\n",
    "        \n",
    "        # ========== 2. Reshape to (batch, seq, n_heads, head_dim) ==========\n",
    "        xq = xq.view(bsz, seq_len, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)\n",
    "        \n",
    "        print(f\"\\n[2] Reshape to heads:\")\n",
    "        print(f\"    xq: {xq.shape}  → [bsz, seq_len, n_local_heads, head_dim]\")\n",
    "        print(f\"    xk: {xk.shape}  → [bsz, seq_len, n_local_kv_heads, head_dim]\")\n",
    "        print(f\"    xv: {xv.shape}  → [bsz, seq_len, n_local_kv_heads, head_dim]\")\n",
    "        \n",
    "        # ========== 3. RoPE (Rotary Position Embedding) ==========\n",
    "        if position_embeddings is not None:\n",
    "            cos, sin = position_embeddings\n",
    "            xq, xk = apply_rotary_pos_emb(xq, xk, cos, sin)\n",
    "            print(f\"\\n[3] RoPE applied:\")\n",
    "            print(f\"    xq: {xq.shape}  (unchanged)\")\n",
    "            print(f\"    xk: {xk.shape}  (unchanged)\")\n",
    "        \n",
    "        # ========== 4. Transpose for attention computation ==========\n",
    "        xq = xq.transpose(1, 2)  # [bsz, n_heads, seq_len, head_dim]\n",
    "        xk = repeat_kv(xk, self.n_rep).transpose(1, 2)\n",
    "        xv = repeat_kv(xv, self.n_rep).transpose(1, 2)\n",
    "        \n",
    "        print(f\"\\n[4] Transpose & repeat_kv:\")\n",
    "        print(f\"    xq: {xq.shape}  → [bsz, n_local_heads, seq_len, head_dim]\")\n",
    "        print(f\"    xk (after repeat_kv): {xk.shape}  → [bsz, n_local_heads, seq_len, head_dim]\")\n",
    "        print(f\"    xv (after repeat_kv): {xv.shape}  → [bsz, n_local_heads, seq_len, head_dim]\")\n",
    "        print(f\"    Note: KV heads repeated {self.n_rep} times ({self.n_local_kv_heads} → {self.n_local_heads})\")\n",
    "        \n",
    "        # ========== 5. Scaled Dot-Product Attention ==========\n",
    "        # scores = Q @ K^T / sqrt(d)\n",
    "        scores = torch.matmul(xq, xk.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        print(f\"\\n[5] Attention Scores:\")\n",
    "        print(f\"    scores (Q @ K^T): {scores.shape}  → [bsz, n_heads, seq_len, seq_len]\")\n",
    "        \n",
    "        # Apply causal mask\n",
    "        causal_mask = torch.triu(torch.full((seq_len, seq_len), float(\"-inf\"), device=scores.device), diagonal=1)\n",
    "        scores = scores + causal_mask\n",
    "        \n",
    "        # Apply softmax\n",
    "        scores = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Output = scores @ V\n",
    "        output = torch.matmul(scores, xv)\n",
    "        \n",
    "        print(f\"    output (scores @ V): {output.shape}  → [bsz, n_heads, seq_len, head_dim]\")\n",
    "        \n",
    "        # ========== 6. Reshape and Output Projection ==========\n",
    "        output = output.transpose(1, 2).reshape(bsz, seq_len, -1)\n",
    "        \n",
    "        print(f\"\\n[6] Reshape output:\")\n",
    "        print(f\"    output (transpose + reshape): {output.shape}  → [bsz, seq_len, n_heads * head_dim]\")\n",
    "        \n",
    "        output = self.o_proj(output)\n",
    "        \n",
    "        print(f\"\\n[7] Output Projection:\")\n",
    "        print(f\"    output (after o_proj): {output.shape}  → [bsz, seq_len, hidden_size]\")\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Demo ==========\n",
    "config = VerMindConfig(\n",
    "    hidden_size=512,\n",
    "    num_attention_heads=8,   # 8 Query heads\n",
    "    num_key_value_heads=2,   # 2 Key/Value heads\n",
    "    num_hidden_layers=1,\n",
    "    vocab_size=6400,\n",
    "    dropout=0.0,\n",
    "    flash_attn=False,\n",
    ")\n",
    "\n",
    "attention = GQAAttention(config)\n",
    "\n",
    "# 准备输入\n",
    "batch_size = 2\n",
    "seq_len = 8\n",
    "hidden_size = 512\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "# 准备位置编码\n",
    "max_seq_len = seq_len\n",
    "head_dim = config.hidden_size // config.num_attention_heads\n",
    "freqs_cos, freqs_sin = precompute_freqs_cis(head_dim, max_seq_len)\n",
    "\n",
    "print(\"Running GQA forward pass...\\n\")\n",
    "output = attention(x, position_embeddings=(freqs_cos, freqs_sin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GQA 关键特性总结\n",
    "\n",
    "| 步骤 | 操作 | Shape 变化 |\n",
    "|------|------|-----------|\n",
    "| 1 | Input x | `[bsz, seq_len, hidden_size]` |\n",
    "| 2 | Q/K/V Linear | Q: `[bsz, seq_len, h*h]` → K/V: `[bsz, seq_len, kv*h]` |\n",
    "| 3 | Reshape to heads | `[bsz, seq_len, n_heads, head_dim]` |\n",
    "| 4 | RoPE (可选) | Shape 不变 |\n",
    "| 5 | transpose + repeat_kv | `[bsz, n_heads, seq_len, head_dim]` |\n",
    "| 6 | Attention scores | `[bsz, n_heads, seq_len, seq_len]` |\n",
    "| 7 | Output | `[bsz, seq_len, hidden_size]` |\n",
    "\n",
    "### 核心优势\n",
    "- **减少 KV cache**: 2 个 KV heads 替代 8 个，显存占用减少 4x\n",
    "- **保持质量**: 通过 repeat_kv 复用，每个 Q head 都能访问所有 KV 信息"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
