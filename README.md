#
<div align="center">
  <a href="https://github.com/nev8rz/vermind">
    <img src="https://raw.githubusercontent.com/nev8rz/vermind/main/docs/assets/logo.png" alt="VerMind Logo" width="800">
  </a>
  <!-- <h1 align="center">VerMind</h1> -->
  <p align="center">
    ä¸€ä¸ªä»é›¶å¼€å§‹ã€åŸºäº PyTorch æ„å»ºçš„è½»é‡çº§ç°ä»£è¯­è¨€æ¨¡å‹ã€‚
    <br />
    <a href="https://nev8rz.github.io/vermind/"><strong>æŸ¥çœ‹æ¼”ç¤º Â»</strong></a>
    Â·
    <a href="https://github.com/nev8rz/vermind/issues">æŠ¥å‘Š Bug</a>
    Â·
    <a href="https://github.com/nev8rz/vermind/issues">è¯·æ±‚åŠŸèƒ½</a>
  </p>
</div>

<div align="center">

**ç®€ä½“ä¸­æ–‡** Â· [English](./docs/README_en.md) Â· [README_VLM](./docs/README_v.md)

</div>

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.12+-blue?logo=python)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.8.0+-ee4c2c?logo=pytorch)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow)](https://opensource.org/licenses/MIT)
[![GitHub Stars](https://img.shields.io/github/stars/nev8rz/vermind?logo=github)](https://github.com/nev8rz/vermind/stargazers)
[![HF LLM](https://img.shields.io/badge/HF-LLM%20%7C%20vermind-yellow?logo=huggingface)](https://huggingface.co/nev8r/vermind)



</div>

---

## ğŸ› ï¸ æ ¸å¿ƒåŠŸèƒ½

| åŠŸèƒ½ | æè¿° |
|---|---|
| âš¡ **åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ› (GQA)** | é€šè¿‡å…±äº«é”®å€¼å¤´æ¥å‡å°‘æ¨ç†æ‰€éœ€çš„å†…å­˜å¸¦å®½ï¼Œä»è€Œå®ç°æ˜¾è‘—çš„é€Ÿåº¦æå‡ã€‚ |
| ğŸ”¥ **SwiGLU æ¿€æ´»å‡½æ•°** | ä¸€ç§ç°ä»£æ¿€æ´»å‡½æ•°ï¼Œé€šå¸¸æ¯”ä¼ ç»Ÿçš„ ReLU æˆ– GeLU å¸¦æ¥æ›´å¥½çš„æ€§èƒ½ã€‚ |
| ğŸ“ **æ—‹è½¬ä½ç½®åµŒå…¥ (RoPE)** | ä¸€ç§ç›¸å¯¹ä½ç½®ç¼–ç æ–¹æ¡ˆï¼Œå·²æˆä¸ºé«˜æ€§èƒ½è¯­è¨€æ¨¡å‹çš„æ ‡é…ã€‚åŒ…å« YaRN ç¼©æ”¾ä»¥æ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦ã€‚ |
| ğŸš€ **vLLM é€‚é…å™¨** | æ”¯æŒæé€Ÿæ¨ç†ï¼Œå¹¶æä¾›ä¸ OpenAI å…¼å®¹çš„ API æœåŠ¡å™¨ï¼Œå¼€ç®±å³ç”¨ã€‚ |
| ğŸ¨ **LoRA å¾®è°ƒ** | æ”¯æŒä½¿ç”¨ä½ç§©è‡ªé€‚åº” (LoRA) è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ (PEFT)ï¼Œå®ç°å¿«é€Ÿã€ä½å†…å­˜å ç”¨çš„å®šåˆ¶åŒ–ã€‚ |
| ğŸŒ **åˆ†å¸ƒå¼è®­ç»ƒ** | å†…ç½®å¯¹åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ (DDP) çš„æ”¯æŒï¼Œå¯å°†è®­ç»ƒæ‰©å±•åˆ°å¤šä¸ª GPUã€‚ |
| ğŸ“¦ **æ‰“åŒ…å¼ SFT è®­ç»ƒ** | ä½¿ç”¨ Varlen FlashAttention çš„åºåˆ—æ‰“åŒ… SFTï¼Œå‡å°‘å¡«å……æµªè´¹ï¼Œæå‡ GPU åˆ©ç”¨ç‡ã€‚ |
| ğŸ¯ **ç›´æ¥åå¥½ä¼˜åŒ– (DPO)** | ä½¿ç”¨åå¥½å¯¹å¯¹é½äººç±»åå¥½ï¼Œæ— éœ€å¥–åŠ±æ¨¡å‹å³å¯æå‡è¾“å‡ºè´¨é‡ã€‚ |
| ğŸ® **è¿‘ç«¯ç­–ç•¥ä¼˜åŒ– (PPO)** | ä½¿ç”¨å¥–åŠ±æ¨¡å‹è¿›è¡Œ RLHF è®­ç»ƒï¼Œå¢å¼ºæ¨ç†èƒ½åŠ›å’Œå›å¤è´¨é‡ã€‚ |
| ğŸ¯ **ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– (GRPO)** | æ— éœ€ Critic æ¨¡å‹çš„é«˜æ•ˆ RL è®­ç»ƒï¼Œä½¿ç”¨ç»„å†…ç›¸å¯¹ä¼˜åŠ¿è¿›è¡Œç­–ç•¥ä¼˜åŒ–ã€‚ |

## ğŸ—ï¸ æ¶æ„æ¦‚è§ˆ

VerMind çš„æ¶æ„æ˜¯ä¸€ä¸ªä¸ºæ€§èƒ½å’Œå¯æ‰©å±•æ€§è€Œä¼˜åŒ–çš„ä»…è§£ç å™¨ Transformer æ¨¡å‹ã€‚æ ¸å¿ƒç»„ä»¶è®¾è®¡å¾—æ—¢é«˜æ•ˆåˆæ˜“äºç†è§£ã€‚

![VerMind Architecture](https://raw.githubusercontent.com/nev8rz/vermind/main/docs/assets/vermind_st.png)
## ğŸ“Š è¯„ä¼°ç»“æœ

VerMind åœ¨ä¸­æ–‡è¯­è¨€ç†è§£åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ï¼ˆ768 éšè—å±‚å¤§å°æ¨¡å‹ï¼‰ï¼š

| åŸºå‡†æµ‹è¯• | ç‰ˆæœ¬ | SFT | DPO | PPO | GRPO |
|---------|------|-----|-----|-----|------|
| ACLUE | v1 | 25.67% Â± 0.62% | 25.41% Â± 0.62% | **25.82%** Â± 0.62% | 25.76% Â± 0.62% |
| CEval-Valid | v2 | 23.85% Â± 1.17% | 23.55% Â± 1.16% | **23.92%** Â± 1.16% | 23.78% Â± 1.16% |
| CMMLU | v1 | 24.79% Â± 0.40% | **25.19%** Â± 0.40% | 25.17% Â± 0.40% | 24.95% Â± 0.40% |
| TMMLUPlus | v2 | 25.15% Â± 0.22% | **25.33%** Â± 0.22% | 25.17% Â± 0.22% | 25.21% Â± 0.22% |

*æ•°å€¼è¶Šé«˜è¶Šå¥½ã€‚æœ€ä¼˜ç»“æœåŠ ç²—æ˜¾ç¤ºã€‚*
> ç©å…·è¯„æµ‹ï¼Œé€‰æ‹©é¢˜1/4æ¦‚ç‡å·¦å³

## ğŸš€ å¿«é€Ÿå¼€å§‹

åªéœ€å‡ ä¸ªç®€å•æ­¥éª¤å³å¯åœ¨æœ¬åœ°è¿è¡Œã€‚

### ç¯å¢ƒè¦æ±‚

-   Python 3.12+
-   PyTorch 2.8.0+
-   `uv` åŒ…ç®¡ç†å™¨ (æ¨è)

### å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/nev8rz/vermind.git
cd vermind

# åˆ›å»ºå¹¶æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
uv venv
source .venv/bin/activate

# å®‰è£…ä¾èµ–
uv pip install -e .
```

## ğŸƒâ€â™€ï¸ ä½¿ç”¨ç¤ºä¾‹

VerMind æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼Œå¹¶åœ¨ `examples/` ç›®å½•ä¸­æä¾›äº†ä¾¿æ·çš„ Shell è„šæœ¬ã€‚è®­ç»ƒå·¥ä½œæµå¦‚ä¸‹ï¼š**åˆ†è¯å™¨ â†’ é¢„è®­ç»ƒ â†’ SFT â†’ DPO/PPO/GRPOï¼ˆå¯é€‰ï¼‰â†’ LoRA â†’ éƒ¨ç½²**ã€‚

### 1. è®­ç»ƒåˆ†è¯å™¨

é¦–å…ˆï¼Œåœ¨ä½ çš„è¯­æ–™åº“ä¸Šè®­ç»ƒä¸€ä¸ªè‡ªå®šä¹‰åˆ†è¯å™¨ï¼š

```bash
python train/train_tokenizer.py \
    --data_path /path/to/training_corpus.jsonl \
    --tokenizer_dir ./vermind_tokenizer \
    --vocab_size 6400
```

### 2. é¢„è®­ç»ƒ

åœ¨å¤§è§„æ¨¡è¯­æ–™åº“ä¸Šä»å¤´å¼€å§‹é¢„è®­ç»ƒæ¨¡å‹ã€‚ä½¿ç”¨æä¾›çš„è„šæœ¬æˆ–ç›´æ¥è¿è¡Œï¼š

```bash
# æ–¹å¼ä¸€ï¼šä½¿ç”¨å¯åŠ¨è„šæœ¬ (åœ¨ tmux ä¸­è¿è¡Œ)
bash examples/pretrain.sh

# æ–¹å¼äºŒï¼šä½¿ç”¨è‡ªå®šä¹‰å‚æ•°ç›´æ¥è¿è¡Œ
python train/pretrain.py \
    --data_path /path/to/pretrain_data.jsonl \
    --save_dir ./output/pretrain \
    --tokenizer_path ./vermind_tokenizer \
    --epochs 5 \
    --batch_size 128 \
    --learning_rate 1e-3
```

### 3. ç›‘ç£å¾®è°ƒ (SFT)

åœ¨æŒ‡ä»¤éµå¾ªæ•°æ®ä¸Šå¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼š

```bash
# æ–¹å¼ä¸€ï¼šä½¿ç”¨å¯åŠ¨è„šæœ¬ (åœ¨ tmux ä¸­è¿è¡Œ)
bash examples/sft.sh

# æ–¹å¼äºŒï¼šä½¿ç”¨è‡ªå®šä¹‰å‚æ•°ç›´æ¥è¿è¡Œ
python train/sft.py \
    --data_path /path/to/sft_data.jsonl \
    --save_dir ./output/sft \
    --tokenizer_path ./vermind_tokenizer \
    --from_weight ./output/pretrain/pretrain_768 \
    --epochs 3 \
    --learning_rate 5e-6
```

#### æ‰“åŒ…å¼ SFT è®­ç»ƒ

ä½¿ç”¨æ‰“åŒ…å¼ SFT è®­ç»ƒæ¨¡å¼ï¼Œé€šè¿‡ Varlen FlashAttention å°†å¤šä¸ªåºåˆ—æ‰“åŒ…åˆ°å•ä¸ªæ‰¹æ¬¡ä¸­ï¼Œå®ç°æ›´é«˜æ•ˆçš„è®­ç»ƒå’Œæ›´å¥½çš„ GPU åˆ©ç”¨ç‡ï¼š

```bash
# æ–¹å¼ä¸€ï¼šä½¿ç”¨å¯åŠ¨è„šæœ¬ (åœ¨ tmux ä¸­è¿è¡Œ)
bash examples/sft_packed.sh

# æ–¹å¼äºŒï¼šä½¿ç”¨è‡ªå®šä¹‰å‚æ•°ç›´æ¥è¿è¡Œ
python train/sft_packed.py \
    --data_path /path/to/sft_data.jsonl \
    --parquet_path ./cache/sft_packed/sft.parquet \
    --save_dir ./output/sft_packed \
    --tokenizer_path ./vermind_tokenizer \
    --from_weight ./output/pretrain/pretrain_768 \
    --epochs 3 \
    --learning_rate 5e-6 \
    --use_packed 1 \
    --max_seq_len 2048
```

**æ‰“åŒ…è®­ç»ƒçš„æ•°æ®é¢„å¤„ç†ï¼š**

```bash
# é¦–å…ˆï¼Œå°† JSONL æ•°æ®é¢„å¤„ç†ä¸ºæ‰“åŒ…çš„ Parquet æ ¼å¼
python scripts/pre_sftdatapacked.py \
    --jsonl_path /path/to/sft_data.jsonl \
    --output_path ./cache/sft_packed/sft.parquet \
    --tokenizer_path ./vermind_tokenizer \
    --max_seq_len 2048
```

æ‰“åŒ…å¼ SFT è®­ç»ƒé€šè¿‡å°†å¤šä¸ªä¸åŒé•¿åº¦çš„åºåˆ—æ‰“åŒ…åˆ°å›ºå®šå¤§å°çš„æ‰¹æ¬¡ä¸­ï¼Œå‡å°‘å¡«å……æµªè´¹å¹¶æé«˜ GPU åˆ©ç”¨ç‡ã€‚å®ƒä½¿ç”¨ Varlen FlashAttention è¿›è¡Œé«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—ï¼Œæ— éœ€æ˜¾å¼çš„æ³¨æ„åŠ›æ©ç ã€‚

### 4. LoRA å¾®è°ƒ

ä½¿ç”¨ LoRA è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼Œç”¨æœ€å°‘çš„èµ„æºé€‚é…æ¨¡å‹ï¼š

```bash
# æ–¹å¼ä¸€ï¼šä½¿ç”¨å¯åŠ¨è„šæœ¬ (åœ¨ tmux ä¸­è¿è¡Œ)
bash examples/lora.sh

# æ–¹å¼äºŒï¼šä½¿ç”¨è‡ªå®šä¹‰å‚æ•°ç›´æ¥è¿è¡Œ
python train/lora.py \
    --data_path /path/to/lora_data.jsonl \
    --save_dir ./output/lora \
    --tokenizer_path ./vermind_tokenizer \
    --from_weight ./output/sft/full_sft_768 \
    --epochs 5 \
    --learning_rate 1e-4 \
    --lora_rank 
    
# ç„¶åå¯ä»¥ ä½¿ç”¨ ./scripts/merge_lora.py è¿›è¡Œåˆå¹¶ -> 8
```

### 5. ç›´æ¥åå¥½ä¼˜åŒ– (DPO)

ä½¿ç”¨åå¥½å¯¹ï¼ˆchosen/rejectedï¼‰å¯¹é½æ¨¡å‹ä¸äººç±»åå¥½ï¼Œæ— éœ€å¥–åŠ±æ¨¡å‹ï¼š

```bash
# æ–¹å¼ä¸€ï¼šä½¿ç”¨å¯åŠ¨è„šæœ¬ (åœ¨ tmux ä¸­è¿è¡Œï¼Œé»˜è®¤ --dpo_aggregate mean)
bash examples/dpo.sh

# æ–¹å¼äºŒï¼šä½¿ç”¨è‡ªå®šä¹‰å‚æ•°ç›´æ¥è¿è¡Œ
python train/dpo.py \
    --data_path /path/to/dpo_data.jsonl \
    --save_dir ./output/dpo \
    --tokenizer_path ./vermind_tokenizer \
    --ref_weight ./output/sft/full_sft_768 \
    --from_weight ./output/sft/full_sft_768 \
    --epochs 3 \
    --learning_rate 1e-6 \
    --beta 0.1 \
    --dpo_aggregate mean \
    --batch_size 16 \
    --max_seq_len 340
```

ä½¿ç”¨ `--dpo_aggregate mean`ï¼ˆå°æ¨¡å‹é»˜è®¤ï¼‰æˆ– `sum` æ§åˆ¶åºåˆ—çº§ log æ¦‚ç‡èšåˆæ–¹å¼ã€‚

### 6. è¿‘ç«¯ç­–ç•¥ä¼˜åŒ– (PPO)

ä½¿ç”¨ PPO ç®—æ³•å’Œå¥–åŠ±æ¨¡å‹è¿›è¡Œ RLHF è®­ç»ƒï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹æ€§èƒ½ï¼š

```bash
# æ–¹å¼ä¸€ï¼šä½¿ç”¨å¯åŠ¨è„šæœ¬ (åœ¨ tmux ä¸­è¿è¡Œ)
bash examples/ppo.sh

# æ–¹å¼äºŒï¼šä½¿ç”¨è‡ªå®šä¹‰å‚æ•°ç›´æ¥è¿è¡Œ
python train/ppo.py \
    --data_path /path/to/rlaif_data.jsonl \
    --save_dir ./output/ppo \
    --tokenizer_path ./vermind_tokenizer \
    --from_weight ./output/sft/full_sft_768 \
    --ref_weight ./output/sft/full_sft_768 \
    --reward_model_path /path/to/reward_model \
    --epochs 3 \
    --learning_rate 1e-6 \
    --batch_size 8 \
    --max_seq_len 512 \
    --max_gen_len 1536 \
    --clip_epsilon 0.2 \
    --kl_coef 0.01
```

**PPO å…³é”®å‚æ•°è¯´æ˜ï¼š**

- `--reward_model_path`: å¥–åŠ±æ¨¡å‹è·¯å¾„ï¼Œç”¨äºè®¡ç®—å¥–åŠ±å€¼
- `--clip_epsilon`: PPO è£å‰ªå‚æ•°ï¼ˆé»˜è®¤ï¼š0.2ï¼‰
- `--kl_coef`: KL æ•£åº¦æƒ©ç½šç³»æ•°ï¼ˆé»˜è®¤ï¼š0.01ï¼‰
- `--vf_coef`: ä»·å€¼å‡½æ•°æŸå¤±ç³»æ•°ï¼ˆé»˜è®¤ï¼š0.5ï¼‰
- `--critic_lr_ratio`: Critic å­¦ä¹ ç‡ä¸ Actor çš„æ¯”ä¾‹ï¼ˆé»˜è®¤ï¼š1.0ï¼‰
- `--update_old_actor_freq`: æ›´æ–°æ—§ Actor çš„é¢‘ç‡ï¼ˆé»˜è®¤ï¼š10 æ­¥ï¼‰
- `--reasoning`: è®¾ä¸º 1 å¯ç”¨æ¨ç†æ¨¡å¼ï¼Œå¢åŠ æ ¼å¼å¥–åŠ±

PPO è®­ç»ƒä½¿ç”¨å¥–åŠ±æ¨¡å‹æ¥å¼•å¯¼ç­–ç•¥ä¼˜åŒ–ï¼Œé€‚ç”¨äºå¤æ‚çš„å¯¹é½ä»»åŠ¡ã€‚è®­ç»ƒé‡‡ç”¨ Actor-Critic æ¶æ„ï¼Œå¹¶é€šè¿‡ KL æƒ©ç½šé˜²æ­¢æ¨¡å‹åç¦»å‚è€ƒç­–ç•¥è¿‡è¿œã€‚

### 7. ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– (GRPO)

æ— éœ€ Critic æ¨¡å‹çš„é«˜æ•ˆ RL è®­ç»ƒï¼Œä½¿ç”¨ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ï¼š

```bash
# é€‰é¡¹1ï¼šä½¿ç”¨å¯åŠ¨è„šæœ¬ï¼ˆåœ¨ tmux ä¸­è¿è¡Œï¼‰
bash examples/grpo.sh

# é€‰é¡¹2ï¼šç›´æ¥ä½¿ç”¨è‡ªå®šä¹‰å‚æ•°è¿è¡Œ
python train/grpo.py \
    --data_path /path/to/rlaif_data.jsonl \
    --save_dir ./output/grpo \
    --tokenizer_path ./vermind_tokenizer \
    --from_weight ./output/sft/full_sft_768 \
    --ref_weight ./output/sft/full_sft_768 \
    --reward_model_path /path/to/reward_model \
    --epochs 3 \
    --learning_rate 1e-6 \
    --batch_size 4 \
    --num_generations 4 \
    --max_seq_len 512 \
    --max_gen_len 1536 \
    --beta 0.04
```

**GRPO å…³é”®å‚æ•°è¯´æ˜ï¼š**

- `--reward_model_path`: ç”¨äºè®¡ç®—å¥–åŠ±çš„å¥–åŠ±æ¨¡å‹è·¯å¾„
- `--num_generations`: æ¯ä¸ªæç¤ºç”Ÿæˆçš„å“åº”æ•°é‡ï¼ˆé»˜è®¤ï¼š4ï¼‰
- `--beta`: KL æ•£åº¦æƒ©ç½šç³»æ•°ï¼ˆé»˜è®¤ï¼š0.04ï¼‰
- `--reasoning`: è®¾ç½®ä¸º 1 å¯ç”¨å¸¦æ ¼å¼å¥–åŠ±çš„æ¨ç†æ¨¡å¼

GRPO é€šè¿‡åœ¨å“åº”ç»„å†…è®¡ç®—ç›¸å¯¹ä¼˜åŠ¿ï¼Œæ¶ˆé™¤äº†å¯¹ Critic æ¨¡å‹çš„éœ€æ±‚ã€‚è¿™å‡å°‘äº†å†…å­˜ä½¿ç”¨å¹¶ç®€åŒ–äº†è®­ç»ƒï¼ŒåŒæ—¶ä¿æŒäº†å¯¹é½è´¨é‡ã€‚

### 8. åˆå¹¶ LoRA æƒé‡

LoRA è®­ç»ƒåï¼Œå°†é€‚é…å™¨æƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­ï¼š

```bash
python scripts/merge_lora.py \
    --model_path ./output/sft/full_sft_768 \
    --lora_path ./output/lora/lora_768
```

### 9. æ¨¡å‹è¯„ä¼°

ä»¥äº¤äº’æ–¹å¼æˆ–è‡ªåŠ¨æµ‹è¯•æ¨¡å¼è¯„ä¼°æ¨¡å‹ï¼š

```bash
# äº¤äº’å¼èŠå¤©æ¨¡å¼
python scripts/eval_llm.py \
    --load_from ./output/lora/lora_768/checkpoint_merged \
    --use_chat_template 1
```

### 10. ä½¿ç”¨ vLLM éƒ¨ç½²

å¯åŠ¨ä¸ OpenAI å®¢æˆ·ç«¯å…¼å®¹çš„é«˜æ€§èƒ½ API æœåŠ¡å™¨ï¼š

```bash
# å¯åŠ¨æœåŠ¡å™¨
python vllm_adapter/start_server.py ./output/lora/lora_768/checkpoint_merged

# æœåŠ¡å™¨ç°åœ¨è¿è¡Œåœ¨ http://localhost:8000
```

### 11. å‘èµ· API è¯·æ±‚

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="dummy",
)

response = client.chat.completions.create(
    model="./output/lora/lora_768/checkpoint_merged",
    messages=[
        {"role": "user", "content": "è§£é‡Šä¸€ä¸‹åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›çš„é‡è¦æ€§ã€‚"}
    ],
)
print(response.choices[0].message.content)
```

## ğŸ¤ è´¡çŒ®

æ¬¢è¿å„ç§è´¡çŒ®ï¼

1.  Fork æœ¬é¡¹ç›®
2.  åˆ›å»ºæ‚¨çš„åŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3.  æäº¤æ‚¨çš„æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4.  æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5.  å¼€å¯ä¸€ä¸ª Pull Request

## ğŸ“œ è®¸å¯è¯

æ ¹æ® MIT è®¸å¯è¯åˆ†å‘ã€‚è¯¦è§ `LICENSE` æ–‡ä»¶ã€‚

## âœ’ï¸ å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶æˆ–å·¥ä½œä¸­ä½¿ç”¨äº† VerMindï¼Œè¯·è€ƒè™‘å¼•ç”¨ï¼š

```bibtex
@software{vermind2026,
  title={VerMind: A High-Performance, Lightweight Language Model with GQA},
  author={nev8rz},
  year={2026},
  url={https://github.com/nev8rz/vermind}
}
```

---

<p align="center">ç”± nev8rz ç”¨ â¤ï¸ åˆ¶ä½œ</p>
