{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcccfad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/vermind/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae47ddd",
   "metadata": {},
   "source": [
    "## pretrain dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34db8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"json\",data_files=\"../dataset/pretrain_hq.jsonl\",split=\"train\",cache_dir=\"../.cache/pretrain_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66a14cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '鉴别一组中文文章的风格和特点，例如官方、口语、文言等。需要提供样例文章才能准确鉴别不同的风格和特点。<|im_end|> 好的，现在帮我查一下今天的天气怎么样?今天的天气依据地区而异。请问你需要我帮你查询哪个地区的天气呢？<|im_end|> 打开闹钟功能，定一个明天早上七点的闹钟。好的，我已经帮您打开闹钟功能，闹钟将在明天早上七点准时响起。<|im_end|> 为以下场景写一句话描述：一个孤独的老人坐在公园长椅上看着远处。一位孤独的老人坐在公园长椅上凝视远方。<|im_end|> 非常感谢你的回答。请告诉我，这些数据是关于什么主题的？这些数据是关于不同年龄段的男女人口比例分布的。<|im_end|> 帮我想一个有趣的标题。这个挺有趣的：\"如何成为一名成功的魔术师\" 调皮的标题往往会吸引读者的注意力。<|im_end|> 回答一个问题，地球的半径是多少？地球的平均半径约为6371公里，这是地球自赤道到两极的距离的平均值。<|im_end|> 识别文本中的语气，并将其分类为喜悦、悲伤、惊异等。\\n文本：“今天是我的生日！”这个文本的语气是喜悦。<|im_end|>'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840ddd67",
   "metadata": {},
   "source": [
    "原始数据使用datasets库记载后的格式是 text + eos，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e728b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../vermind_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80228bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鉴别一组中文文章的风格和特点，例如官方、口语、文言等。需要提供样例文章才能准确鉴别不同的风格和特点。<|im_end|> 好的，现在帮我查一下今天的天气怎么样?今天的天气依据地区而异。请问你需要我帮你查询哪个地区的天气呢？<|im_end|> 打开闹钟功能，定一个明天早上七点的闹钟。好的，我已经帮您打开闹钟功能，闹钟将在明天早上七点准时响起。<|im_end|> 为以下场景写一句话描述：一个孤独的老人坐在公园长椅上看着远处。一位孤独的老人坐在公园长椅上凝视远方。<|im_end|> 非常感谢你的回答。请告诉我，这些数据是关于什么主题的？这些数据是关于不同年龄段的男女人口比例分布的。<|im_end|> 帮我想一个有趣的标题。这个挺有趣的：\"如何成为一名成功的魔术师\" 调皮的标题往往会吸引读者的注意力。<|im_end|> 回答一个问题，地球的半径是多少？地球的平均半径约为6371公里，这是地球自赤道到两极的距离的平均值。<|im_end|> 识别文本中的语气，并将其分类为喜悦、悲伤、惊异等。\n",
      "文本：“今天是我的生日！”这个文本的语气是喜悦。<|im_end|>\n",
      "{'bos_token': '<|im_start|>', 'eos_token': '<|im_end|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(\n",
    "            str(ds[0][\"text\"]),\n",
    "            add_special_tokens=False,\n",
    "            max_length= 512  - 2, # 因为后面需要手动添加bos，eos\n",
    "            truncation=True # 超出长度截断\n",
    "        ).input_ids\n",
    "print(tokenizer.decode(tokens))\n",
    "print(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3fcb3a",
   "metadata": {},
   "source": [
    "> 可以看到一模一样，这个例子是没有截断的，感觉最后有时候没必要强行加一个eos啊，甚至说，这个bos有意义吗？？？\n",
    "\n",
    "> ai的回答: \n",
    "\n",
    "| 特性 | 加上 BOS 和 EOS | 不加（错误示范） |\n",
    "|------|-----------------|------------------|\n",
    "| 训练目标 | 模型学会从零开始启动，并在说完后准确停止。 | 模型可能难以进入状态，且学会了“话唠”（无法停止）。 |\n",
    "| 推理表现 | 用户提问 → 模型回答 → 模型输出 `<im_end>` | `im_end` |\n",
    "| Attention | 首个 Token 关注 BOS，注意力分配正常。 | 首个 Token 无处关注，可能导致内部参数分布异常。 |\n",
    "\n",
    "感觉主要是这个attn的问题\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9363af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, max_length=512, cache_dir=\"../.cache/pretrain_dataset\"):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "        self.samples = load_dataset(\n",
    "            \"json\",\n",
    "            data_files=data_path,\n",
    "            split=\"train\",\n",
    "            cache_dir=cache_dir,   \n",
    "        )\n",
    "        print(f\"[PretrainDataset] HF cache dir: {self.cache_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples[index]\n",
    "        tokens = self.tokenizer(\n",
    "            str(sample[\"text\"]),\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_length - 2, # 后面需要手动添加bos，eos\n",
    "            truncation=True # 超出长度截断\n",
    "        ).input_ids\n",
    "\n",
    "        tokens = [self.tokenizer.bos_token_id] + tokens + [self.tokenizer.eos_token_id] # 加上 bos和eos\n",
    "        input_ids = tokens + [self.tokenizer.pad_token_id] * (self.max_length - len(tokens)) # len(tokens) <= 512\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long) # 凡是索引，必须用 long（64位整数）类型\n",
    "\n",
    "        labels = input_ids.clone() # 这里先不做shift\n",
    "        labels[input_ids == self.tokenizer.pad_token_id] = -100 # 对于padding 的位置，label设置为-1\n",
    "\n",
    "        return input_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "719ec135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PretrainDataset] HF cache dir: ../.cache/pretrain_dataset\n"
     ]
    }
   ],
   "source": [
    "dataset = PretrainDataset(\n",
    "    data_path=\"../dataset/pretrain_hq.jsonl\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vermind",
   "language": "python",
   "name": "vermind"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
