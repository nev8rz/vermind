{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7b254d46",
      "metadata": {},
      "source": [
        "# DPO (Direct Preference Optimization)\n",
        "\n",
        "**直接偏好优化** 使用偏好对 (chosen / rejected) 对齐模型与人类偏好，无需训练单独的 reward model。\n",
        "\n",
        "- **Chosen**：人类偏好的回复（更好）\n",
        "- **Rejected**：人类不偏好的回复（更差）\n",
        "- **Reference model**：冻结的参考模型，用于计算 \\(\\log \\pi_{\\text{ref}}(y|x)\\)\n",
        "- **Policy model**：待训练模型，优化使其对 chosen 的归一化似然相对 rejected 更高\n",
        "- **β**：温度参数，控制偏离参考模型的程度"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ec5b35c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "ROOT = Path.cwd()\n",
        "if ROOT.name == \"docs\":\n",
        "    ROOT = ROOT.parent\n",
        "sys.path.insert(0, str(ROOT))\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from data_loader.dpo_dataset import DPODataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1054ed19",
      "metadata": {},
      "source": [
        "## 数据格式：`dpo.jsonl`\n",
        "\n",
        "每行一个 JSON 对象，包含 `chosen` 与 `rejected`，均为对话消息列表（与 SFT chat 格式一致）：\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"chosen\": [\n",
        "    {\"role\": \"user\", \"content\": \"1+1等于几？\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"1+1等于2。\"}\n",
        "  ],\n",
        "  \"rejected\": [\n",
        "    {\"role\": \"user\", \"content\": \"1+1等于几？\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"不知道。\"}\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "- `DPODataset` 用 `apply_chat_template` 将 chosen/rejected 转为文本，再 tokenize、截断、padding。\n",
        "- **Labels**：仅对 **assistant 回复部分** 计算 loss；prompt 与 padding 的 label 为 `-100`。\n",
        "- **Mask**：`mask_chosen` / `mask_rejected` 标记哪些位置参与 DPO 损失（1=助理回复，0=其余）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7a6c249",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 创建临时 dpo.jsonl 用于演示\n",
        "demo_data = [\n",
        "    {\n",
        "        \"chosen\": [\n",
        "            {\"role\": \"user\", \"content\": \"1+1等于几？\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"1+1等于2。\"}\n",
        "        ],\n",
        "        \"rejected\": [\n",
        "            {\"role\": \"user\", \"content\": \"1+1等于几？\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"不知道。\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"chosen\": [\n",
        "            {\"role\": \"user\", \"content\": \"你好\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"你好！有什么可以帮你的？\"}\n",
        "        ],\n",
        "        \"rejected\": [\n",
        "            {\"role\": \"user\", \"content\": \"你好\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"哦。\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "tmpdir = Path(tempfile.gettempdir()) / \"dpo_demo\"\n",
        "tmpdir.mkdir(exist_ok=True)\n",
        "dpo_jsonl = tmpdir / \"dpo_demo.jsonl\"\n",
        "with open(dpo_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in demo_data:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "tokenizer_path = ROOT / \"vermind_tokenizer\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(str(tokenizer_path), trust_remote_code=True)\n",
        "ds = DPODataset(str(dpo_jsonl), tokenizer, max_length=256)\n",
        "print(f\"DPODataset 长度: {len(ds)}\")\n",
        "sample = ds[0]\n",
        "print(f\"keys: {list(sample.keys())}\")\n",
        "print(f\"x_chosen shape: {sample['x_chosen'].shape}, mask_chosen sum: {sample['mask_chosen'].sum().item():.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca8a19c5",
      "metadata": {},
      "source": [
        "## DPO 损失\n",
        "\n",
        "1. **logits → log probs（每个 token）**：  \n",
        "   `log_probs[b,s] = log_softmax(logits[b,s,:])[labels[b,s]]`  \n",
        "   即模型对「真实下一个 token」的 log 概率。\n",
        "\n",
        "2. **序列级 log prob**：支持 **sum** 或 **mean**（`aggregate` 参数）。  \n",
        "   - **sum**：`seq_log_prob = (log_probs * mask).sum(dim=1)`，不除长度。  \n",
        "   - **mean**：`seq_log_prob = (log_probs * mask).sum(dim=1) / mask.sum(dim=1).clamp_min(1e-8)`，对 mask 位置求平均。\n",
        "\n",
        "3. **DPO 损失**：  \n",
        "   - 将 batch 前半视为 chosen、后半视为 rejected（训练时 `torch.cat([chosen, rejected], dim=0)`）。  \n",
        "   - 定义 log-ratio：  \n",
        "     - \\(\\pi\\) log-ratio = `chosen_policy_log_prob - rejected_policy_log_prob`  \n",
        "     - ref log-ratio = `chosen_ref_log_prob - rejected_ref_log_prob`  \n",
        "   - `logits = π_log_ratio - ref_log_ratio`，  \n",
        "     `loss = -log σ(β * logits)`，  \n",
        "     即希望 policy 相对 ref 更偏好 chosen。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55109232",
      "metadata": {},
      "outputs": [],
      "source": [
        "def logits_to_log_probs(logits, labels):\n",
        "    \"\"\"logits: (B, S, V), labels: (B, S) -> (B, S)\"\"\"\n",
        "    log_probs = F.log_softmax(logits, dim=2)\n",
        "    return torch.gather(log_probs, dim=2, index=labels.unsqueeze(2)).squeeze(-1)\n",
        "\n",
        "\n",
        "def logits_to_log_probs(logits, labels):\n",
        "    \"\"\"logits: (B, S, V), labels: (B, S) -> (B, S)\"\"\"\n",
        "    log_probs = F.log_softmax(logits, dim=2)\n",
        "    return torch.gather(log_probs, dim=2, index=labels.unsqueeze(2)).squeeze(-1)\n",
        "\n",
        "\n",
        "def dpo_loss(ref_log_probs, policy_log_probs, mask, beta=0.1, aggregate=\"sum\"):\n",
        "    \"\"\"aggregate: sum 不除长度, mean 对 mask 位置求平均。返回 loss, chosen_rewards, rejected_rewards。\"\"\"\n",
        "    policy_raw = (policy_log_probs * mask).sum(dim=1)\n",
        "    ref_raw = (ref_log_probs * mask).sum(dim=1)\n",
        "    if aggregate == \"mean\":\n",
        "        seq_lengths = mask.sum(dim=1).clamp_min(1e-8)\n",
        "        policy_sum = policy_raw / seq_lengths\n",
        "        ref_sum = ref_raw / seq_lengths\n",
        "    else:\n",
        "        policy_sum, ref_sum = policy_raw, ref_raw\n",
        "    n = ref_log_probs.shape[0] // 2\n",
        "    policy_chosen, policy_rejected = policy_sum[:n], policy_sum[n:]\n",
        "    ref_chosen, ref_rejected = ref_sum[:n], ref_sum[n:]\n",
        "    pi_logratios = policy_chosen - policy_rejected\n",
        "    ref_logratios = ref_chosen - ref_rejected\n",
        "    logits = pi_logratios - ref_logratios\n",
        "    losses = -F.logsigmoid(beta * logits)\n",
        "    with torch.no_grad():\n",
        "        chosen_rewards = (beta * (policy_chosen - ref_chosen)).detach()\n",
        "        rejected_rewards = (beta * (policy_rejected - ref_rejected)).detach()\n",
        "    return losses.mean(), chosen_rewards.mean(), rejected_rewards.mean()\n",
        "\n",
        "\n",
        "# 合成数据演示\n",
        "B, S, V = 4, 8, 100\n",
        "logits = torch.randn(B, S, V)\n",
        "labels = torch.randint(0, V, (B, S))\n",
        "mask = torch.zeros(B, S)\n",
        "mask[:, :4] = 1.0\n",
        "lp = logits_to_log_probs(logits, labels)\n",
        "ref_lp = lp + 0.1 * torch.randn_like(lp)\n",
        "policy_lp = lp + 0.2 * torch.randn_like(lp)\n",
        "ref_lp = torch.cat([ref_lp, ref_lp], dim=0)\n",
        "policy_lp = torch.cat([policy_lp, policy_lp], dim=0)\n",
        "mask = torch.cat([mask, mask], dim=0)\n",
        "loss_s, cr_s, rr_s = dpo_loss(ref_lp, policy_lp, mask, beta=0.1, aggregate=\"sum\")\n",
        "loss_m, cr_m, rr_m = dpo_loss(ref_lp, policy_lp, mask, beta=0.1, aggregate=\"mean\")\n",
        "print(\"aggregate=sum:  loss:\", loss_s.item(), \"chosen_reward:\", cr_s.item(), \"rejected_reward:\", rr_s.item())\n",
        "print(\"aggregate=mean: loss:\", loss_m.item(), \"chosen_reward:\", cr_m.item(), \"rejected_reward:\", rr_m.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9d2318b",
      "metadata": {},
      "source": [
        "## 训练流程概要（train/dpo.py）\n",
        "\n",
        "- **Reference model**：加载 `--ref_weight`，eval 且冻结，只算 `ref_log_probs`。\n",
        "- **Policy model**：由 `--from_weight` 或 resume 加载，正常训练。\n",
        "- 每 step：`x = cat([x_chosen, x_rejected])`，同样 cat `y`、`mask`；用 ref 算 `ref_log_probs`，用 policy 算 `policy_log_probs`；`loss = dpo_loss(...) + aux_loss`，只对 policy 反向传播。\n",
        "\n",
        "运行：`python train/dpo.py --data_path /path/to/dpo.jsonl --ref_weight /path/to/sft_checkpoint`  \n",
        "可选 `--dpo_aggregate sum`（默认）或 `--dpo_aggregate mean`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2b702b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "loader = DataLoader(ds, batch_size=2, shuffle=False)\n",
        "batch = next(iter(loader))\n",
        "x_chosen = batch[\"x_chosen\"]\n",
        "x_rejected = batch[\"x_rejected\"]\n",
        "y_chosen, y_rejected = batch[\"y_chosen\"], batch[\"y_rejected\"]\n",
        "mask_chosen, mask_rejected = batch[\"mask_chosen\"], batch[\"mask_rejected\"]\n",
        "\n",
        "x = torch.cat([x_chosen, x_rejected], dim=0)\n",
        "y = torch.cat([y_chosen, y_rejected], dim=0)\n",
        "mask = torch.cat([mask_chosen, mask_rejected], dim=0)\n",
        "print(\"Training batch: x\", x.shape, \"y\", y.shape, \"mask\", mask.shape)\n",
        "print(\"mask sum (tokens to score):\", mask.sum().item())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vermind (3.11.14)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
