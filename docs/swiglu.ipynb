{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) SwiGLU 简介\n",
    "\n",
    "SwiGLU 是 **Swish 激活函数 + GLU 门控线性单元** 的组合，是 LLM 中最常用的 Feed-Forward 架构之一。\n",
    "\n",
    "**传统 FFN**: `FFN(x) = W2 * Act(W1 * x)`\n",
    "\n",
    "**SwiGLU**: `SwiGLU(x) = (W1 * x) ⊙ (Act(W2 * x))`  ->  `W3 * output`\n",
    "\n",
    "其中 ⊙ 表示逐元素乘法，Act 通常是 **SiLU/Swish**：`silu(x) = x * sigmoid(x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silu(x):\n",
    "    \"\"\"SiLU / Swish 激活函数: x * sigmoid(x)\"\"\"\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"GELU 激活函数 (近似实现)\"\"\"\n",
    "    return 0.5 * x * (1.0 + torch.tanh(torch.sqrt(2.0 / torch.tensor(3.1415926535)) * (x + 0.044715 * x ** 3)))\n",
    "\n",
    "# 对比可视化\n",
    "x = torch.linspace(-4, 4, 200)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x.numpy(), silu(x).numpy(), label=\"SiLU (Swish)\", linewidth=2)\n",
    "plt.plot(x.numpy(), torch.relu(x).numpy(), label=\"ReLU\", linewidth=2, linestyle=\"--\")\n",
    "plt.axhline(0, color=\"gray\", alpha=0.3)\n",
    "plt.axvline(0, color=\"gray\", alpha=0.3)\n",
    "plt.title(\"激活函数对比\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x.numpy(), silu(x).derivative().numpy(), label=\"SiLU 导数\", linewidth=2)\n",
    "plt.plot(x.numpy(), (x > 0).float().numpy(), label=\"ReLU 导数\", linewidth=2, linestyle=\"--\")\n",
    "plt.title(\"导数对比\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"SiLU 特点: 平滑、非单调、有负值区域（相比ReLU有更好的梯度流）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) GLU 门控机制\n",
    "\n",
    "GLU (Gated Linear Unit) 的核心思想：\n",
    "1. 用一个线性变换 `gate_proj` 生成门控信号\n",
    "2. 门控信号经过激活函数后，与另一个线性变换 `up_proj` 的输出做**逐元素乘法**\n",
    "3. 最后用 `down_proj` 投影回原维度\n",
    "\n",
    "数学表达式：\n",
    "$$\n",
    "\\text{SwiGLU}(x) = W_3 \\left( (W_1 x) \\odot \\text{SiLU}(W_2 x) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"SwiGLU Feed Forward Network\"\"\"\n",
    "    def __init__(self, hidden_size: int, intermediate_size: int = None, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        if intermediate_size is None:\n",
    "            # 默认: intermediate_size ≈ hidden_size * 8/3 (参考 LLaMA)\n",
    "            intermediate_size = int(hidden_size * 8 / 3)\n",
    "            intermediate_size = 64 * ((intermediate_size + 64 - 1) // 64)  # 对齐到 64\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        \n",
    "        # 三个投影矩阵\n",
    "        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)  # W1 (门控)\n",
    "        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)    # W2 (上投影)\n",
    "        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)  # W3 (下投影)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # SwiGLU 前向传播\n",
    "        # gate: [B, T, hidden_size] -> [B, T, intermediate_size]\n",
    "        # up:   [B, T, hidden_size] -> [B, T, intermediate_size]\n",
    "        # gate_act: 经过 SiLU 激活\n",
    "        # output: gate_act * up -> down_proj -> [B, T, hidden_size]\n",
    "        return self.dropout(self.down_proj(silu(self.gate_proj(x)) * self.up_proj(x)))\n",
    "\n",
    "# 参数统计\n",
    "def count_parameters(ffn: FeedForward) -> dict:\n",
    "    \"\"\"统计参数数量\"\"\"\n",
    "    params = {\n",
    "        \"gate_proj\": sum(p.numel() for p in ffn.gate_proj.parameters()),\n",
    "        \"up_proj\": sum(p.numel() for p in ffn.up_proj.parameters()),\n",
    "        \"down_proj\": sum(p.numel() for p in ffn.down_proj.parameters()),\n",
    "    }\n",
    "    params[\"total\"] = sum(params.values())\n",
    "    return params\n",
    "\n",
    "# 测试\n",
    "hidden_size = 4096\n",
    "ffn = FeedForward(hidden_size=hidden_size)\n",
    "params = count_parameters(ffn)\n",
    "\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"intermediate_size: {ffn.intermediate_size}\")\n",
    "print(f\"参数统计:\")\n",
    "for k, v in params.items():\n",
    "    print(f\"  {k}: {v:,} ({v/1e6:.2f}M)\")\n",
    "print(f\"\\n对比传统 FFN (2个 linear): {2 * hidden_size * ffn.intermediate_size + ffn.intermediate_size * hidden_size:,}\")\n",
    "print(f\"SwiGLU (3个 linear): {params['total']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 为什么 SwiGLU 效果好？\n",
    "\n",
    "### 3.1 门控机制的优势\n",
    "- SiLU 作为门控：`gate = SiLU(W1 x)`，输出范围约 (-0.28, ∞)\n",
    "- 负值可以**抑制**某些特征通道，正值可以**增强**某些特征\n",
    "- 相比 ReLU 的硬截断，SiLU 更平滑，梯度更稳定\n",
    "\n",
    "### 3.2 逐元素乘法的信息交互\n",
    "- `gate * up` 让两个不同的线性变换结果相互作用\n",
    "- 相当于一种**自适应特征选择**机制\n",
    "\n",
    "### 3.3 参数量与计算量的权衡\n",
    "- SwiGLU 比传统 FFN 多一个投影，但中间层更大\n",
    "- 整体参数量约为 3 * hidden_size * intermediate_size\n",
    "- LLaMA 等模型选择 intermediate_size ≈ hidden_size * 8/3，兼顾容量与效率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化门控效果\n",
    "batch_size, seq_len, hidden_size = 2, 10, 4096\n",
    "intermediate_size = ffn.intermediate_size\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "# 前向传播，获取中间结果\n",
    "gate = ffn.gate_proj(x)           # [B, T, intermediate_size]\n",
    "gate_activated = silu(gate)       # 经过 SiLU 激活\n",
    "up = ffn.up_proj(x)               # [B, T, intermediate_size]\n",
    "gated = gate_activated * up       # 逐元素乘法\n",
    "output = ffn.down_proj(gated)     # [B, T, hidden_size]\n",
    "\n",
    "print(f\"输入形状: {x.shape}\")\n",
    "print(f\"gate_proj 输出形状: {gate.shape}\")\n",
    "print(f\"up_proj 输出形状: {up.shape}\")\n",
    "print(f\"门控后形状: {gated.shape}\")\n",
    "print(f\"最终输出形状: {output.shape}\")\n",
    "\n",
    "# 分析门控信号分布\n",
    "print(\"\\n门控信号统计 (SiLU(gate)):\")\n",
    "print(f\"  均值: {gate_activated.mean().item():.4f}\")\n",
    "print(f\"  标准差: {gate_activated.std().item():.4f}\")\n",
    "print(f\"  最小值: {gate_activated.min().item():.4f}\")\n",
    "print(f\"  最大值: {gate_activated.max().item():.4f}\")\n",
    "print(f\"  负值比例: {(gate_activated < 0).float().mean().item():.2%}\")\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(gate_activated.flatten().cpu().numpy(), bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(0, color='red', linestyle='--', label='zero')\n",
    "plt.title(\"门控信号 SiLU(gate) 分布\")\n",
    "plt.xlabel(\"值\")\n",
    "plt.ylabel(\"频次\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(up.flatten().cpu().numpy(), bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "plt.title(\"up_proj 输出分布\")\n",
    "plt.xlabel(\"值\")\n",
    "plt.ylabel(\"频次\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) 梯度流分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对比传统 FFN 与 SwiGLU 的梯度流\n",
    "class TraditionalFFN(nn.Module):\n",
    "    \"\"\"传统 FFN: ReLU(W2(W1 x))\"\"\"\n",
    "    def __init__(self, hidden_size, intermediate_size):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.w2 = nn.Linear(intermediate_size, hidden_size, bias=False)\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.w2(self.act(self.w1(x)))\n",
    "\n",
    "class SwiGLUFFN(nn.Module):\n",
    "    \"\"\"SwiGLU FFN\"\"\"\n",
    "    def __init__(self, hidden_size, intermediate_size):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.down_proj(silu(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "# 创建模型\n",
    "hidden_size = 512\n",
    "intermediate_size = 2048\n",
    "\n",
    "ffn_trad = TraditionalFFN(hidden_size, intermediate_size)\n",
    "ffn_swiglu = SwiGLUFFN(hidden_size, intermediate_size)\n",
    "\n",
    "# 输入\n",
    "x = torch.randn(4, 32, hidden_size, requires_grad=True)\n",
    "\n",
    "# 传统 FFN 梯度\n",
    "x_trad = x.clone()\n",
    "y_trad = ffn_trad(x_trad)\n",
    "loss_trad = y_trad.sum()\n",
    "loss_trad.backward()\n",
    "grad_trad = x_trad.grad.abs().mean(dim=-1)  # [B, T]\n",
    "\n",
    "# SwiGLU 梯度\n",
    "x_swiglu = x.clone()\n",
    "y_swiglu = ffn_swiglu(x_swiglu)\n",
    "loss_swiglu = y_swiglu.sum()\n",
    "loss_swiglu.backward()\n",
    "grad_swiglu = x_swiglu.grad.abs().mean(dim=-1)  # [B, T]\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(grad_trad[0].detach().cpu().numpy(), label=\"Traditional FFN (ReLU)\", alpha=0.7)\n",
    "plt.plot(grad_swiglu[0].detach().cpu().numpy(), label=\"SwiGLU\", alpha=0.7)\n",
    "plt.title(\"梯度强度对比 (第一个 batch)\")\n",
    "plt.xlabel(\"Token 位置\")\n",
    "plt.ylabel(\"平均梯度强度\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(grad_trad.flatten().detach().cpu().numpy(), \n",
    "            grad_swiglu.flatten().detach().cpu().numpy(), alpha=0.1, s=1)\n",
    "plt.plot([0, grad_trad.max()], [0, grad_trad.max()], 'r--', label=\"y=x\")\n",
    "plt.xlabel(\"Traditional FFN 梯度\")\n",
    "plt.ylabel(\"SwiGLU 梯度\")\n",
    "plt.title(\"梯度分布对比\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"传统 FFN 平均梯度: {grad_trad.mean().item():.6f}\")\n",
    "print(f\"SwiGLU 平均梯度: {grad_swiglu.mean().item():.6f}\")\n",
    "print(\"\\n结论: SwiGLU 的门控机制使得梯度流更加平滑，减少了死神经元的问题\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) 代码实现对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 实现\n",
    "class PyTorchSwiGLU(nn.Module):\n",
    "    \"\"\"使用 nn.Module 实现 SwiGLU\"\"\"\n",
    "    def __init__(self, hidden_size: int, intermediate_size: int):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n",
    "        self.silu = nn.SiLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.down_proj(self.silu(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "# 使用 torch.nn.functional 实现\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FSwiGLU(nn.Module):\n",
    "    \"\"\"使用 F.silu 实现 SwiGLU\"\"\"\n",
    "    def __init__(self, hidden_size: int, intermediate_size: int):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.silu(self.gate_proj(x)) * self.up_proj(x) @ self.down_proj.weight.t()\n",
    "\n",
    "# 验证两种实现输出一致\n",
    "hidden_size = 256\n",
    "intermediate_size = 512\n",
    "\n",
    "model1 = PyTorchSwiGLU(hidden_size, intermediate_size)\n",
    "model2 = FSwiGLU(hidden_size, intermediate_size)\n",
    "\n",
    "# 复制权重\n",
    "model2.gate_proj.weight.data = model1.gate_proj.weight.data.clone()\n",
    "model2.up_proj.weight.data = model1.up_proj.weight.data.clone()\n",
    "model2.down_proj.weight.data = model1.down_proj.weight.data.clone()\n",
    "\n",
    "# 测试\n",
    "x = torch.randn(2, 10, hidden_size)\n",
    "y1 = model1(x)\n",
    "y2 = model2(x)\n",
    "\n",
    "print(f\"PyTorch 实现输出形状: {y1.shape}\")\n",
    "print(f\"F.silu 实现输出形状: {y2.shape}\")\n",
    "print(f\"输出差异: {(y1 - y2).abs().max().item():.2e}\")\n",
    "\n",
    "# 参数统计\n",
    "total_params = sum(p.numel() for p in model1.parameters())\n",
    "print(f\"总参数量: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) 总结\n",
    "\n",
    "### SwiGLU 的核心特点\n",
    "1. **门控机制**: SiLU 作为软门控，比 ReLU 更平滑\n",
    "2. **逐元素交互**: gate * up 实现特征自适应选择\n",
    "3. **参数效率**: 3个投影，但中间层更大（LLaMA: 8/3倍）\n",
    "4. **梯度流**: 负值区域允许梯度流动，减少死神经元\n",
    "\n",
    "### 实际应用\n",
    "- LLaMA, PaLM, Mistral 等主流 LLM 都采用 SwiGLU\n",
    "- 中间层大小通常是 hidden_size 的 8/3 倍，并向上取整到 64 的倍数"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
