{
  "results": {
    "aclue": {
      "acc,none": 0.2494965767217076,
      "acc_stderr,none": 0.006137887696298453,
      "acc_norm,none": 0.2494965767217076,
      "acc_norm_stderr,none": 0.006137887696298453,
      "alias": "aclue"
    },
    "aclue_ancient_chinese_culture": {
      "alias": "aclue_ancient_chinese_culture",
      "acc,none": 0.19117647058823528,
      "acc_stderr,none": 0.03384365225033987,
      "acc_norm,none": 0.19117647058823528,
      "acc_norm_stderr,none": 0.03384365225033987
    },
    "aclue_ancient_literature": {
      "alias": "aclue_ancient_literature",
      "acc,none": 0.23125,
      "acc_stderr,none": 0.03343758265727746,
      "acc_norm,none": 0.23125,
      "acc_norm_stderr,none": 0.03343758265727746
    },
    "aclue_ancient_medical": {
      "alias": "aclue_ancient_medical",
      "acc,none": 0.3033175355450237,
      "acc_stderr,none": 0.03172170716716874,
      "acc_norm,none": 0.3033175355450237,
      "acc_norm_stderr,none": 0.03172170716716874
    },
    "aclue_ancient_phonetics": {
      "alias": "aclue_ancient_phonetics",
      "acc,none": 0.35,
      "acc_stderr,none": 0.047937248544110175,
      "acc_norm,none": 0.35,
      "acc_norm_stderr,none": 0.047937248544110175
    },
    "aclue_basic_ancient_chinese": {
      "alias": "aclue_basic_ancient_chinese",
      "acc,none": 0.2570281124497992,
      "acc_stderr,none": 0.027749212562228807,
      "acc_norm,none": 0.2570281124497992,
      "acc_norm_stderr,none": 0.027749212562228807
    },
    "aclue_couplet_prediction": {
      "alias": "aclue_couplet_prediction",
      "acc,none": 0.252,
      "acc_stderr,none": 0.01943572728224953,
      "acc_norm,none": 0.252,
      "acc_norm_stderr,none": 0.01943572728224953
    },
    "aclue_homographic_character_resolution": {
      "alias": "aclue_homographic_character_resolution",
      "acc,none": 0.224,
      "acc_stderr,none": 0.01866399446471079,
      "acc_norm,none": 0.224,
      "acc_norm_stderr,none": 0.01866399446471079
    },
    "aclue_named_entity_recognition": {
      "alias": "aclue_named_entity_recognition",
      "acc,none": 0.266,
      "acc_stderr,none": 0.019780559675655493,
      "acc_norm,none": 0.266,
      "acc_norm_stderr,none": 0.019780559675655493
    },
    "aclue_poetry_appreciate": {
      "alias": "aclue_poetry_appreciate",
      "acc,none": 0.24271844660194175,
      "acc_stderr,none": 0.04245022486384495,
      "acc_norm,none": 0.24271844660194175,
      "acc_norm_stderr,none": 0.04245022486384495
    },
    "aclue_poetry_context_prediction": {
      "alias": "aclue_poetry_context_prediction",
      "acc,none": 0.266,
      "acc_stderr,none": 0.019780559675655493,
      "acc_norm,none": 0.266,
      "acc_norm_stderr,none": 0.019780559675655493
    },
    "aclue_poetry_quality_assessment": {
      "alias": "aclue_poetry_quality_assessment",
      "acc,none": 0.2413793103448276,
      "acc_stderr,none": 0.02126350486668445,
      "acc_norm,none": 0.2413793103448276,
      "acc_norm_stderr,none": 0.02126350486668445
    },
    "aclue_poetry_sentiment_analysis": {
      "alias": "aclue_poetry_sentiment_analysis",
      "acc,none": 0.226,
      "acc_stderr,none": 0.018722956449139926,
      "acc_norm,none": 0.226,
      "acc_norm_stderr,none": 0.018722956449139926
    },
    "aclue_polysemy_resolution": {
      "alias": "aclue_polysemy_resolution",
      "acc,none": 0.272,
      "acc_stderr,none": 0.019920483209566072,
      "acc_norm,none": 0.272,
      "acc_norm_stderr,none": 0.019920483209566072
    },
    "aclue_reading_comprehension": {
      "alias": "aclue_reading_comprehension",
      "acc,none": 0.21782178217821782,
      "acc_stderr,none": 0.041276561555793755,
      "acc_norm,none": 0.21782178217821782,
      "acc_norm_stderr,none": 0.041276561555793755
    },
    "aclue_sentence_segmentation": {
      "alias": "aclue_sentence_segmentation",
      "acc,none": 0.23,
      "acc_stderr,none": 0.01883905039112314,
      "acc_norm,none": 0.23,
      "acc_norm_stderr,none": 0.01883905039112314
    },
    "ceval-valid": {
      "acc,none": 0.24145616641901932,
      "acc_stderr,none": 0.011730152635910575,
      "acc_norm,none": 0.24145616641901932,
      "acc_norm_stderr,none": 0.011730152635910575,
      "alias": "ceval-valid"
    },
    "ceval-valid_accountant": {
      "alias": "ceval-valid_accountant",
      "acc,none": 0.24489795918367346,
      "acc_stderr,none": 0.062069005411206336,
      "acc_norm,none": 0.24489795918367346,
      "acc_norm_stderr,none": 0.062069005411206336
    },
    "ceval-valid_advanced_mathematics": {
      "alias": "ceval-valid_advanced_mathematics",
      "acc,none": 0.3157894736842105,
      "acc_stderr,none": 0.10956136839295434,
      "acc_norm,none": 0.3157894736842105,
      "acc_norm_stderr,none": 0.10956136839295434
    },
    "ceval-valid_art_studies": {
      "alias": "ceval-valid_art_studies",
      "acc,none": 0.3333333333333333,
      "acc_stderr,none": 0.08333333333333333,
      "acc_norm,none": 0.3333333333333333,
      "acc_norm_stderr,none": 0.08333333333333333
    },
    "ceval-valid_basic_medicine": {
      "alias": "ceval-valid_basic_medicine",
      "acc,none": 0.42105263157894735,
      "acc_stderr,none": 0.11637279966159299,
      "acc_norm,none": 0.42105263157894735,
      "acc_norm_stderr,none": 0.11637279966159299
    },
    "ceval-valid_business_administration": {
      "alias": "ceval-valid_business_administration",
      "acc,none": 0.24242424242424243,
      "acc_stderr,none": 0.07575757575757577,
      "acc_norm,none": 0.24242424242424243,
      "acc_norm_stderr,none": 0.07575757575757577
    },
    "ceval-valid_chinese_language_and_literature": {
      "alias": "ceval-valid_chinese_language_and_literature",
      "acc,none": 0.2608695652173913,
      "acc_stderr,none": 0.09361833424764437,
      "acc_norm,none": 0.2608695652173913,
      "acc_norm_stderr,none": 0.09361833424764437
    },
    "ceval-valid_civil_servant": {
      "alias": "ceval-valid_civil_servant",
      "acc,none": 0.2553191489361702,
      "acc_stderr,none": 0.06429065810876616,
      "acc_norm,none": 0.2553191489361702,
      "acc_norm_stderr,none": 0.06429065810876616
    },
    "ceval-valid_clinical_medicine": {
      "alias": "ceval-valid_clinical_medicine",
      "acc,none": 0.22727272727272727,
      "acc_stderr,none": 0.09144861547306321,
      "acc_norm,none": 0.22727272727272727,
      "acc_norm_stderr,none": 0.09144861547306321
    },
    "ceval-valid_college_chemistry": {
      "alias": "ceval-valid_college_chemistry",
      "acc,none": 0.2916666666666667,
      "acc_stderr,none": 0.09477598811252415,
      "acc_norm,none": 0.2916666666666667,
      "acc_norm_stderr,none": 0.09477598811252415
    },
    "ceval-valid_college_economics": {
      "alias": "ceval-valid_college_economics",
      "acc,none": 0.2909090909090909,
      "acc_stderr,none": 0.06180629713445796,
      "acc_norm,none": 0.2909090909090909,
      "acc_norm_stderr,none": 0.06180629713445796
    },
    "ceval-valid_college_physics": {
      "alias": "ceval-valid_college_physics",
      "acc,none": 0.21052631578947367,
      "acc_stderr,none": 0.0960916767552923,
      "acc_norm,none": 0.21052631578947367,
      "acc_norm_stderr,none": 0.0960916767552923
    },
    "ceval-valid_college_programming": {
      "alias": "ceval-valid_college_programming",
      "acc,none": 0.2972972972972973,
      "acc_stderr,none": 0.07617808344724215,
      "acc_norm,none": 0.2972972972972973,
      "acc_norm_stderr,none": 0.07617808344724215
    },
    "ceval-valid_computer_architecture": {
      "alias": "ceval-valid_computer_architecture",
      "acc,none": 0.2857142857142857,
      "acc_stderr,none": 0.10101525445522108,
      "acc_norm,none": 0.2857142857142857,
      "acc_norm_stderr,none": 0.10101525445522108
    },
    "ceval-valid_computer_network": {
      "alias": "ceval-valid_computer_network",
      "acc,none": 0.10526315789473684,
      "acc_stderr,none": 0.07233518641434492,
      "acc_norm,none": 0.10526315789473684,
      "acc_norm_stderr,none": 0.07233518641434492
    },
    "ceval-valid_discrete_mathematics": {
      "alias": "ceval-valid_discrete_mathematics",
      "acc,none": 0.375,
      "acc_stderr,none": 0.125,
      "acc_norm,none": 0.375,
      "acc_norm_stderr,none": 0.125
    },
    "ceval-valid_education_science": {
      "alias": "ceval-valid_education_science",
      "acc,none": 0.27586206896551724,
      "acc_stderr,none": 0.08446516354424752,
      "acc_norm,none": 0.27586206896551724,
      "acc_norm_stderr,none": 0.08446516354424752
    },
    "ceval-valid_electrical_engineer": {
      "alias": "ceval-valid_electrical_engineer",
      "acc,none": 0.2702702702702703,
      "acc_stderr,none": 0.07401656182502246,
      "acc_norm,none": 0.2702702702702703,
      "acc_norm_stderr,none": 0.07401656182502246
    },
    "ceval-valid_environmental_impact_assessment_engineer": {
      "alias": "ceval-valid_environmental_impact_assessment_engineer",
      "acc,none": 0.22580645161290322,
      "acc_stderr,none": 0.07633651333031764,
      "acc_norm,none": 0.22580645161290322,
      "acc_norm_stderr,none": 0.07633651333031764
    },
    "ceval-valid_fire_engineer": {
      "alias": "ceval-valid_fire_engineer",
      "acc,none": 0.22580645161290322,
      "acc_stderr,none": 0.07633651333031764,
      "acc_norm,none": 0.22580645161290322,
      "acc_norm_stderr,none": 0.07633651333031764
    },
    "ceval-valid_high_school_biology": {
      "alias": "ceval-valid_high_school_biology",
      "acc,none": 0.2631578947368421,
      "acc_stderr,none": 0.10379087338771256,
      "acc_norm,none": 0.2631578947368421,
      "acc_norm_stderr,none": 0.10379087338771256
    },
    "ceval-valid_high_school_chemistry": {
      "alias": "ceval-valid_high_school_chemistry",
      "acc,none": 0.21052631578947367,
      "acc_stderr,none": 0.0960916767552923,
      "acc_norm,none": 0.21052631578947367,
      "acc_norm_stderr,none": 0.0960916767552923
    },
    "ceval-valid_high_school_chinese": {
      "alias": "ceval-valid_high_school_chinese",
      "acc,none": 0.21052631578947367,
      "acc_stderr,none": 0.0960916767552923,
      "acc_norm,none": 0.21052631578947367,
      "acc_norm_stderr,none": 0.0960916767552923
    },
    "ceval-valid_high_school_geography": {
      "alias": "ceval-valid_high_school_geography",
      "acc,none": 0.3684210526315789,
      "acc_stderr,none": 0.1136972052352256,
      "acc_norm,none": 0.3684210526315789,
      "acc_norm_stderr,none": 0.1136972052352256
    },
    "ceval-valid_high_school_history": {
      "alias": "ceval-valid_high_school_history",
      "acc,none": 0.2,
      "acc_stderr,none": 0.09176629354822471,
      "acc_norm,none": 0.2,
      "acc_norm_stderr,none": 0.09176629354822471
    },
    "ceval-valid_high_school_mathematics": {
      "alias": "ceval-valid_high_school_mathematics",
      "acc,none": 0.2222222222222222,
      "acc_stderr,none": 0.1008316903303367,
      "acc_norm,none": 0.2222222222222222,
      "acc_norm_stderr,none": 0.1008316903303367
    },
    "ceval-valid_high_school_physics": {
      "alias": "ceval-valid_high_school_physics",
      "acc,none": 0.21052631578947367,
      "acc_stderr,none": 0.0960916767552923,
      "acc_norm,none": 0.21052631578947367,
      "acc_norm_stderr,none": 0.0960916767552923
    },
    "ceval-valid_high_school_politics": {
      "alias": "ceval-valid_high_school_politics",
      "acc,none": 0.21052631578947367,
      "acc_stderr,none": 0.0960916767552923,
      "acc_norm,none": 0.21052631578947367,
      "acc_norm_stderr,none": 0.0960916767552923
    },
    "ceval-valid_ideological_and_moral_cultivation": {
      "alias": "ceval-valid_ideological_and_moral_cultivation",
      "acc,none": 0.15789473684210525,
      "acc_stderr,none": 0.08594700851870798,
      "acc_norm,none": 0.15789473684210525,
      "acc_norm_stderr,none": 0.08594700851870798
    },
    "ceval-valid_law": {
      "alias": "ceval-valid_law",
      "acc,none": 0.16666666666666666,
      "acc_stderr,none": 0.07770873402002615,
      "acc_norm,none": 0.16666666666666666,
      "acc_norm_stderr,none": 0.07770873402002615
    },
    "ceval-valid_legal_professional": {
      "alias": "ceval-valid_legal_professional",
      "acc,none": 0.043478260869565216,
      "acc_stderr,none": 0.04347826086956523,
      "acc_norm,none": 0.043478260869565216,
      "acc_norm_stderr,none": 0.04347826086956523
    },
    "ceval-valid_logic": {
      "alias": "ceval-valid_logic",
      "acc,none": 0.18181818181818182,
      "acc_stderr,none": 0.08416546361568647,
      "acc_norm,none": 0.18181818181818182,
      "acc_norm_stderr,none": 0.08416546361568647
    },
    "ceval-valid_mao_zedong_thought": {
      "alias": "ceval-valid_mao_zedong_thought",
      "acc,none": 0.2916666666666667,
      "acc_stderr,none": 0.09477598811252415,
      "acc_norm,none": 0.2916666666666667,
      "acc_norm_stderr,none": 0.09477598811252415
    },
    "ceval-valid_marxism": {
      "alias": "ceval-valid_marxism",
      "acc,none": 0.15789473684210525,
      "acc_stderr,none": 0.08594700851870798,
      "acc_norm,none": 0.15789473684210525,
      "acc_norm_stderr,none": 0.08594700851870798
    },
    "ceval-valid_metrology_engineer": {
      "alias": "ceval-valid_metrology_engineer",
      "acc,none": 0.2916666666666667,
      "acc_stderr,none": 0.09477598811252413,
      "acc_norm,none": 0.2916666666666667,
      "acc_norm_stderr,none": 0.09477598811252413
    },
    "ceval-valid_middle_school_biology": {
      "alias": "ceval-valid_middle_school_biology",
      "acc,none": 0.2857142857142857,
      "acc_stderr,none": 0.10101525445522108,
      "acc_norm,none": 0.2857142857142857,
      "acc_norm_stderr,none": 0.10101525445522108
    },
    "ceval-valid_middle_school_chemistry": {
      "alias": "ceval-valid_middle_school_chemistry",
      "acc,none": 0.2,
      "acc_stderr,none": 0.09176629354822471,
      "acc_norm,none": 0.2,
      "acc_norm_stderr,none": 0.09176629354822471
    },
    "ceval-valid_middle_school_geography": {
      "alias": "ceval-valid_middle_school_geography",
      "acc,none": 0.16666666666666666,
      "acc_stderr,none": 0.1123666437438737,
      "acc_norm,none": 0.16666666666666666,
      "acc_norm_stderr,none": 0.1123666437438737
    },
    "ceval-valid_middle_school_history": {
      "alias": "ceval-valid_middle_school_history",
      "acc,none": 0.13636363636363635,
      "acc_stderr,none": 0.07488677009526491,
      "acc_norm,none": 0.13636363636363635,
      "acc_norm_stderr,none": 0.07488677009526491
    },
    "ceval-valid_middle_school_mathematics": {
      "alias": "ceval-valid_middle_school_mathematics",
      "acc,none": 0.15789473684210525,
      "acc_stderr,none": 0.08594700851870798,
      "acc_norm,none": 0.15789473684210525,
      "acc_norm_stderr,none": 0.08594700851870798
    },
    "ceval-valid_middle_school_physics": {
      "alias": "ceval-valid_middle_school_physics",
      "acc,none": 0.21052631578947367,
      "acc_stderr,none": 0.0960916767552923,
      "acc_norm,none": 0.21052631578947367,
      "acc_norm_stderr,none": 0.0960916767552923
    },
    "ceval-valid_middle_school_politics": {
      "alias": "ceval-valid_middle_school_politics",
      "acc,none": 0.2857142857142857,
      "acc_stderr,none": 0.10101525445522108,
      "acc_norm,none": 0.2857142857142857,
      "acc_norm_stderr,none": 0.10101525445522108
    },
    "ceval-valid_modern_chinese_history": {
      "alias": "ceval-valid_modern_chinese_history",
      "acc,none": 0.08695652173913043,
      "acc_stderr,none": 0.06007385040937022,
      "acc_norm,none": 0.08695652173913043,
      "acc_norm_stderr,none": 0.06007385040937022
    },
    "ceval-valid_operating_system": {
      "alias": "ceval-valid_operating_system",
      "acc,none": 0.3157894736842105,
      "acc_stderr,none": 0.10956136839295434,
      "acc_norm,none": 0.3157894736842105,
      "acc_norm_stderr,none": 0.10956136839295434
    },
    "ceval-valid_physician": {
      "alias": "ceval-valid_physician",
      "acc,none": 0.2653061224489796,
      "acc_stderr,none": 0.06372446937141221,
      "acc_norm,none": 0.2653061224489796,
      "acc_norm_stderr,none": 0.06372446937141221
    },
    "ceval-valid_plant_protection": {
      "alias": "ceval-valid_plant_protection",
      "acc,none": 0.4090909090909091,
      "acc_stderr,none": 0.10729033533674223,
      "acc_norm,none": 0.4090909090909091,
      "acc_norm_stderr,none": 0.10729033533674223
    },
    "ceval-valid_probability_and_statistics": {
      "alias": "ceval-valid_probability_and_statistics",
      "acc,none": 0.16666666666666666,
      "acc_stderr,none": 0.09038769075777339,
      "acc_norm,none": 0.16666666666666666,
      "acc_norm_stderr,none": 0.09038769075777339
    },
    "ceval-valid_professional_tour_guide": {
      "alias": "ceval-valid_professional_tour_guide",
      "acc,none": 0.27586206896551724,
      "acc_stderr,none": 0.08446516354424752,
      "acc_norm,none": 0.27586206896551724,
      "acc_norm_stderr,none": 0.08446516354424752
    },
    "ceval-valid_sports_science": {
      "alias": "ceval-valid_sports_science",
      "acc,none": 0.15789473684210525,
      "acc_stderr,none": 0.085947008518708,
      "acc_norm,none": 0.15789473684210525,
      "acc_norm_stderr,none": 0.085947008518708
    },
    "ceval-valid_tax_accountant": {
      "alias": "ceval-valid_tax_accountant",
      "acc,none": 0.16326530612244897,
      "acc_stderr,none": 0.05334825558285076,
      "acc_norm,none": 0.16326530612244897,
      "acc_norm_stderr,none": 0.05334825558285076
    },
    "ceval-valid_teacher_qualification": {
      "alias": "ceval-valid_teacher_qualification",
      "acc,none": 0.3181818181818182,
      "acc_stderr,none": 0.07102933373079214,
      "acc_norm,none": 0.3181818181818182,
      "acc_norm_stderr,none": 0.07102933373079214
    },
    "ceval-valid_urban_and_rural_planner": {
      "alias": "ceval-valid_urban_and_rural_planner",
      "acc,none": 0.21739130434782608,
      "acc_stderr,none": 0.061487546190134544,
      "acc_norm,none": 0.21739130434782608,
      "acc_norm_stderr,none": 0.061487546190134544
    },
    "ceval-valid_veterinary_medicine": {
      "alias": "ceval-valid_veterinary_medicine",
      "acc,none": 0.17391304347826086,
      "acc_stderr,none": 0.08081046758996394,
      "acc_norm,none": 0.17391304347826086,
      "acc_norm_stderr,none": 0.08081046758996394
    },
    "cmmlu": {
      "acc,none": 0.25125194266965983,
      "acc_stderr,none": 0.004034458768106313,
      "acc_norm,none": 0.25125194266965983,
      "acc_norm_stderr,none": 0.004034458768106313,
      "alias": "cmmlu"
    },
    "cmmlu_agronomy": {
      "alias": "cmmlu_agronomy",
      "acc,none": 0.30177514792899407,
      "acc_stderr,none": 0.03541479614288121,
      "acc_norm,none": 0.30177514792899407,
      "acc_norm_stderr,none": 0.03541479614288121
    },
    "cmmlu_anatomy": {
      "alias": "cmmlu_anatomy",
      "acc,none": 0.22972972972972974,
      "acc_stderr,none": 0.03469536825407607,
      "acc_norm,none": 0.22972972972972974,
      "acc_norm_stderr,none": 0.03469536825407607
    },
    "cmmlu_ancient_chinese": {
      "alias": "cmmlu_ancient_chinese",
      "acc,none": 0.27439024390243905,
      "acc_stderr,none": 0.034949590161775394,
      "acc_norm,none": 0.27439024390243905,
      "acc_norm_stderr,none": 0.034949590161775394
    },
    "cmmlu_arts": {
      "alias": "cmmlu_arts",
      "acc,none": 0.25,
      "acc_stderr,none": 0.03434014098717226,
      "acc_norm,none": 0.25,
      "acc_norm_stderr,none": 0.03434014098717226
    },
    "cmmlu_astronomy": {
      "alias": "cmmlu_astronomy",
      "acc,none": 0.2606060606060606,
      "acc_stderr,none": 0.034277431758165236,
      "acc_norm,none": 0.2606060606060606,
      "acc_norm_stderr,none": 0.034277431758165236
    },
    "cmmlu_business_ethics": {
      "alias": "cmmlu_business_ethics",
      "acc,none": 0.24401913875598086,
      "acc_stderr,none": 0.029780753228706103,
      "acc_norm,none": 0.24401913875598086,
      "acc_norm_stderr,none": 0.029780753228706103
    },
    "cmmlu_chinese_civil_service_exam": {
      "alias": "cmmlu_chinese_civil_service_exam",
      "acc,none": 0.2625,
      "acc_stderr,none": 0.03489370652018759,
      "acc_norm,none": 0.2625,
      "acc_norm_stderr,none": 0.03489370652018759
    },
    "cmmlu_chinese_driving_rule": {
      "alias": "cmmlu_chinese_driving_rule",
      "acc,none": 0.24427480916030533,
      "acc_stderr,none": 0.037683359597287434,
      "acc_norm,none": 0.24427480916030533,
      "acc_norm_stderr,none": 0.037683359597287434
    },
    "cmmlu_chinese_food_culture": {
      "alias": "cmmlu_chinese_food_culture",
      "acc,none": 0.23529411764705882,
      "acc_stderr,none": 0.036507817107892686,
      "acc_norm,none": 0.23529411764705882,
      "acc_norm_stderr,none": 0.036507817107892686
    },
    "cmmlu_chinese_foreign_policy": {
      "alias": "cmmlu_chinese_foreign_policy",
      "acc,none": 0.2523364485981308,
      "acc_stderr,none": 0.04218811928205305,
      "acc_norm,none": 0.2523364485981308,
      "acc_norm_stderr,none": 0.04218811928205305
    },
    "cmmlu_chinese_history": {
      "alias": "cmmlu_chinese_history",
      "acc,none": 0.2476780185758514,
      "acc_stderr,none": 0.024055681892974835,
      "acc_norm,none": 0.2476780185758514,
      "acc_norm_stderr,none": 0.024055681892974835
    },
    "cmmlu_chinese_literature": {
      "alias": "cmmlu_chinese_literature",
      "acc,none": 0.24509803921568626,
      "acc_stderr,none": 0.03019028245350195,
      "acc_norm,none": 0.24509803921568626,
      "acc_norm_stderr,none": 0.03019028245350195
    },
    "cmmlu_chinese_teacher_qualification": {
      "alias": "cmmlu_chinese_teacher_qualification",
      "acc,none": 0.24022346368715083,
      "acc_stderr,none": 0.03202142463804494,
      "acc_norm,none": 0.24022346368715083,
      "acc_norm_stderr,none": 0.03202142463804494
    },
    "cmmlu_clinical_knowledge": {
      "alias": "cmmlu_clinical_knowledge",
      "acc,none": 0.270042194092827,
      "acc_stderr,none": 0.028900721906293426,
      "acc_norm,none": 0.270042194092827,
      "acc_norm_stderr,none": 0.028900721906293426
    },
    "cmmlu_college_actuarial_science": {
      "alias": "cmmlu_college_actuarial_science",
      "acc,none": 0.24528301886792453,
      "acc_stderr,none": 0.04198857662371223,
      "acc_norm,none": 0.24528301886792453,
      "acc_norm_stderr,none": 0.04198857662371223
    },
    "cmmlu_college_education": {
      "alias": "cmmlu_college_education",
      "acc,none": 0.2616822429906542,
      "acc_stderr,none": 0.042692919157281094,
      "acc_norm,none": 0.2616822429906542,
      "acc_norm_stderr,none": 0.042692919157281094
    },
    "cmmlu_college_engineering_hydrology": {
      "alias": "cmmlu_college_engineering_hydrology",
      "acc,none": 0.3584905660377358,
      "acc_stderr,none": 0.04679998780012862,
      "acc_norm,none": 0.3584905660377358,
      "acc_norm_stderr,none": 0.04679998780012862
    },
    "cmmlu_college_law": {
      "alias": "cmmlu_college_law",
      "acc,none": 0.18518518518518517,
      "acc_stderr,none": 0.03755265865037183,
      "acc_norm,none": 0.18518518518518517,
      "acc_norm_stderr,none": 0.03755265865037183
    },
    "cmmlu_college_mathematics": {
      "alias": "cmmlu_college_mathematics",
      "acc,none": 0.23809523809523808,
      "acc_stderr,none": 0.041764667586049006,
      "acc_norm,none": 0.23809523809523808,
      "acc_norm_stderr,none": 0.041764667586049006
    },
    "cmmlu_college_medical_statistics": {
      "alias": "cmmlu_college_medical_statistics",
      "acc,none": 0.2169811320754717,
      "acc_stderr,none": 0.04022559246936713,
      "acc_norm,none": 0.2169811320754717,
      "acc_norm_stderr,none": 0.04022559246936713
    },
    "cmmlu_college_medicine": {
      "alias": "cmmlu_college_medicine",
      "acc,none": 0.23443223443223443,
      "acc_stderr,none": 0.02568715645908419,
      "acc_norm,none": 0.23443223443223443,
      "acc_norm_stderr,none": 0.02568715645908419
    },
    "cmmlu_computer_science": {
      "alias": "cmmlu_computer_science",
      "acc,none": 0.2647058823529412,
      "acc_stderr,none": 0.030964517926923403,
      "acc_norm,none": 0.2647058823529412,
      "acc_norm_stderr,none": 0.030964517926923403
    },
    "cmmlu_computer_security": {
      "alias": "cmmlu_computer_security",
      "acc,none": 0.21052631578947367,
      "acc_stderr,none": 0.03126781714663179,
      "acc_norm,none": 0.21052631578947367,
      "acc_norm_stderr,none": 0.03126781714663179
    },
    "cmmlu_conceptual_physics": {
      "alias": "cmmlu_conceptual_physics",
      "acc,none": 0.23809523809523808,
      "acc_stderr,none": 0.035249200485600284,
      "acc_norm,none": 0.23809523809523808,
      "acc_norm_stderr,none": 0.035249200485600284
    },
    "cmmlu_construction_project_management": {
      "alias": "cmmlu_construction_project_management",
      "acc,none": 0.23741007194244604,
      "acc_stderr,none": 0.036220593237998276,
      "acc_norm,none": 0.23741007194244604,
      "acc_norm_stderr,none": 0.036220593237998276
    },
    "cmmlu_economics": {
      "alias": "cmmlu_economics",
      "acc,none": 0.25157232704402516,
      "acc_stderr,none": 0.03452055811164905,
      "acc_norm,none": 0.25157232704402516,
      "acc_norm_stderr,none": 0.03452055811164905
    },
    "cmmlu_education": {
      "alias": "cmmlu_education",
      "acc,none": 0.25766871165644173,
      "acc_stderr,none": 0.03436150827846917,
      "acc_norm,none": 0.25766871165644173,
      "acc_norm_stderr,none": 0.03436150827846917
    },
    "cmmlu_electrical_engineering": {
      "alias": "cmmlu_electrical_engineering",
      "acc,none": 0.26744186046511625,
      "acc_stderr,none": 0.033848364281578586,
      "acc_norm,none": 0.26744186046511625,
      "acc_norm_stderr,none": 0.033848364281578586
    },
    "cmmlu_elementary_chinese": {
      "alias": "cmmlu_elementary_chinese",
      "acc,none": 0.28174603174603174,
      "acc_stderr,none": 0.02839429305079051,
      "acc_norm,none": 0.28174603174603174,
      "acc_norm_stderr,none": 0.02839429305079051
    },
    "cmmlu_elementary_commonsense": {
      "alias": "cmmlu_elementary_commonsense",
      "acc,none": 0.2474747474747475,
      "acc_stderr,none": 0.030746300742124495,
      "acc_norm,none": 0.2474747474747475,
      "acc_norm_stderr,none": 0.030746300742124495
    },
    "cmmlu_elementary_information_and_technology": {
      "alias": "cmmlu_elementary_information_and_technology",
      "acc,none": 0.2773109243697479,
      "acc_stderr,none": 0.029079374539480007,
      "acc_norm,none": 0.2773109243697479,
      "acc_norm_stderr,none": 0.029079374539480007
    },
    "cmmlu_elementary_mathematics": {
      "alias": "cmmlu_elementary_mathematics",
      "acc,none": 0.24782608695652175,
      "acc_stderr,none": 0.02853086259541006,
      "acc_norm,none": 0.24782608695652175,
      "acc_norm_stderr,none": 0.02853086259541006
    },
    "cmmlu_ethnology": {
      "alias": "cmmlu_ethnology",
      "acc,none": 0.26666666666666666,
      "acc_stderr,none": 0.038201699145179055,
      "acc_norm,none": 0.26666666666666666,
      "acc_norm_stderr,none": 0.038201699145179055
    },
    "cmmlu_food_science": {
      "alias": "cmmlu_food_science",
      "acc,none": 0.27972027972027974,
      "acc_stderr,none": 0.03766763889539852,
      "acc_norm,none": 0.27972027972027974,
      "acc_norm_stderr,none": 0.03766763889539852
    },
    "cmmlu_genetics": {
      "alias": "cmmlu_genetics",
      "acc,none": 0.23863636363636365,
      "acc_stderr,none": 0.0322214701789951,
      "acc_norm,none": 0.23863636363636365,
      "acc_norm_stderr,none": 0.0322214701789951
    },
    "cmmlu_global_facts": {
      "alias": "cmmlu_global_facts",
      "acc,none": 0.2348993288590604,
      "acc_stderr,none": 0.034847315046501876,
      "acc_norm,none": 0.2348993288590604,
      "acc_norm_stderr,none": 0.034847315046501876
    },
    "cmmlu_high_school_biology": {
      "alias": "cmmlu_high_school_biology",
      "acc,none": 0.28994082840236685,
      "acc_stderr,none": 0.03500638924911012,
      "acc_norm,none": 0.28994082840236685,
      "acc_norm_stderr,none": 0.03500638924911012
    },
    "cmmlu_high_school_chemistry": {
      "alias": "cmmlu_high_school_chemistry",
      "acc,none": 0.16666666666666666,
      "acc_stderr,none": 0.032561027715190184,
      "acc_norm,none": 0.16666666666666666,
      "acc_norm_stderr,none": 0.032561027715190184
    },
    "cmmlu_high_school_geography": {
      "alias": "cmmlu_high_school_geography",
      "acc,none": 0.2796610169491525,
      "acc_stderr,none": 0.04149459161011112,
      "acc_norm,none": 0.2796610169491525,
      "acc_norm_stderr,none": 0.04149459161011112
    },
    "cmmlu_high_school_mathematics": {
      "alias": "cmmlu_high_school_mathematics",
      "acc,none": 0.24390243902439024,
      "acc_stderr,none": 0.03363591048272823,
      "acc_norm,none": 0.24390243902439024,
      "acc_norm_stderr,none": 0.03363591048272823
    },
    "cmmlu_high_school_physics": {
      "alias": "cmmlu_high_school_physics",
      "acc,none": 0.2545454545454545,
      "acc_stderr,none": 0.04172343038705383,
      "acc_norm,none": 0.2545454545454545,
      "acc_norm_stderr,none": 0.04172343038705383
    },
    "cmmlu_high_school_politics": {
      "alias": "cmmlu_high_school_politics",
      "acc,none": 0.3076923076923077,
      "acc_stderr,none": 0.03873144730600104,
      "acc_norm,none": 0.3076923076923077,
      "acc_norm_stderr,none": 0.03873144730600104
    },
    "cmmlu_human_sexuality": {
      "alias": "cmmlu_human_sexuality",
      "acc,none": 0.24603174603174602,
      "acc_stderr,none": 0.03852273364924315,
      "acc_norm,none": 0.24603174603174602,
      "acc_norm_stderr,none": 0.03852273364924315
    },
    "cmmlu_international_law": {
      "alias": "cmmlu_international_law",
      "acc,none": 0.25405405405405407,
      "acc_stderr,none": 0.032092816451453864,
      "acc_norm,none": 0.25405405405405407,
      "acc_norm_stderr,none": 0.032092816451453864
    },
    "cmmlu_journalism": {
      "alias": "cmmlu_journalism",
      "acc,none": 0.23255813953488372,
      "acc_stderr,none": 0.03230654083203449,
      "acc_norm,none": 0.23255813953488372,
      "acc_norm_stderr,none": 0.03230654083203449
    },
    "cmmlu_jurisprudence": {
      "alias": "cmmlu_jurisprudence",
      "acc,none": 0.2384428223844282,
      "acc_stderr,none": 0.021045122419532578,
      "acc_norm,none": 0.2384428223844282,
      "acc_norm_stderr,none": 0.021045122419532578
    },
    "cmmlu_legal_and_moral_basis": {
      "alias": "cmmlu_legal_and_moral_basis",
      "acc,none": 0.24766355140186916,
      "acc_stderr,none": 0.029576535293164483,
      "acc_norm,none": 0.24766355140186916,
      "acc_norm_stderr,none": 0.029576535293164483
    },
    "cmmlu_logical": {
      "alias": "cmmlu_logical",
      "acc,none": 0.22764227642276422,
      "acc_stderr,none": 0.03796258624175263,
      "acc_norm,none": 0.22764227642276422,
      "acc_norm_stderr,none": 0.03796258624175263
    },
    "cmmlu_machine_learning": {
      "alias": "cmmlu_machine_learning",
      "acc,none": 0.27049180327868855,
      "acc_stderr,none": 0.04038308168357443,
      "acc_norm,none": 0.27049180327868855,
      "acc_norm_stderr,none": 0.04038308168357443
    },
    "cmmlu_management": {
      "alias": "cmmlu_management",
      "acc,none": 0.21428571428571427,
      "acc_stderr,none": 0.028382836222822356,
      "acc_norm,none": 0.21428571428571427,
      "acc_norm_stderr,none": 0.028382836222822356
    },
    "cmmlu_marketing": {
      "alias": "cmmlu_marketing",
      "acc,none": 0.2611111111111111,
      "acc_stderr,none": 0.032830366339668404,
      "acc_norm,none": 0.2611111111111111,
      "acc_norm_stderr,none": 0.032830366339668404
    },
    "cmmlu_marxist_theory": {
      "alias": "cmmlu_marxist_theory",
      "acc,none": 0.2804232804232804,
      "acc_stderr,none": 0.03276171742795849,
      "acc_norm,none": 0.2804232804232804,
      "acc_norm_stderr,none": 0.03276171742795849
    },
    "cmmlu_modern_chinese": {
      "alias": "cmmlu_modern_chinese",
      "acc,none": 0.25,
      "acc_stderr,none": 0.04037864265436242,
      "acc_norm,none": 0.25,
      "acc_norm_stderr,none": 0.04037864265436242
    },
    "cmmlu_nutrition": {
      "alias": "cmmlu_nutrition",
      "acc,none": 0.27586206896551724,
      "acc_stderr,none": 0.037245636197746325,
      "acc_norm,none": 0.27586206896551724,
      "acc_norm_stderr,none": 0.037245636197746325
    },
    "cmmlu_philosophy": {
      "alias": "cmmlu_philosophy",
      "acc,none": 0.29523809523809524,
      "acc_stderr,none": 0.044729159560441434,
      "acc_norm,none": 0.29523809523809524,
      "acc_norm_stderr,none": 0.044729159560441434
    },
    "cmmlu_professional_accounting": {
      "alias": "cmmlu_professional_accounting",
      "acc,none": 0.28,
      "acc_stderr,none": 0.034038517735870494,
      "acc_norm,none": 0.28,
      "acc_norm_stderr,none": 0.034038517735870494
    },
    "cmmlu_professional_law": {
      "alias": "cmmlu_professional_law",
      "acc,none": 0.22748815165876776,
      "acc_stderr,none": 0.028928260405094858,
      "acc_norm,none": 0.22748815165876776,
      "acc_norm_stderr,none": 0.028928260405094858
    },
    "cmmlu_professional_medicine": {
      "alias": "cmmlu_professional_medicine",
      "acc,none": 0.2526595744680851,
      "acc_stderr,none": 0.0224394125827864,
      "acc_norm,none": 0.2526595744680851,
      "acc_norm_stderr,none": 0.0224394125827864
    },
    "cmmlu_professional_psychology": {
      "alias": "cmmlu_professional_psychology",
      "acc,none": 0.24568965517241378,
      "acc_stderr,none": 0.028324514684171156,
      "acc_norm,none": 0.24568965517241378,
      "acc_norm_stderr,none": 0.028324514684171156
    },
    "cmmlu_public_relations": {
      "alias": "cmmlu_public_relations",
      "acc,none": 0.27586206896551724,
      "acc_stderr,none": 0.033980799395855854,
      "acc_norm,none": 0.27586206896551724,
      "acc_norm_stderr,none": 0.033980799395855854
    },
    "cmmlu_security_study": {
      "alias": "cmmlu_security_study",
      "acc,none": 0.22962962962962963,
      "acc_stderr,none": 0.03633384414073462,
      "acc_norm,none": 0.22962962962962963,
      "acc_norm_stderr,none": 0.03633384414073462
    },
    "cmmlu_sociology": {
      "alias": "cmmlu_sociology",
      "acc,none": 0.26991150442477874,
      "acc_stderr,none": 0.029594239995417392,
      "acc_norm,none": 0.26991150442477874,
      "acc_norm_stderr,none": 0.029594239995417392
    },
    "cmmlu_sports_science": {
      "alias": "cmmlu_sports_science",
      "acc,none": 0.18181818181818182,
      "acc_stderr,none": 0.030117688929503592,
      "acc_norm,none": 0.18181818181818182,
      "acc_norm_stderr,none": 0.030117688929503592
    },
    "cmmlu_traditional_chinese_medicine": {
      "alias": "cmmlu_traditional_chinese_medicine",
      "acc,none": 0.23783783783783785,
      "acc_stderr,none": 0.031387393683304815,
      "acc_norm,none": 0.23783783783783785,
      "acc_norm_stderr,none": 0.031387393683304815
    },
    "cmmlu_virology": {
      "alias": "cmmlu_virology",
      "acc,none": 0.2485207100591716,
      "acc_stderr,none": 0.03334150198101962,
      "acc_norm,none": 0.2485207100591716,
      "acc_norm_stderr,none": 0.03334150198101962
    },
    "cmmlu_world_history": {
      "alias": "cmmlu_world_history",
      "acc,none": 0.21739130434782608,
      "acc_stderr,none": 0.0326086956521739,
      "acc_norm,none": 0.21739130434782608,
      "acc_norm_stderr,none": 0.0326086956521739
    },
    "cmmlu_world_religions": {
      "alias": "cmmlu_world_religions",
      "acc,none": 0.24375,
      "acc_stderr,none": 0.03404916326237584,
      "acc_norm,none": 0.24375,
      "acc_norm_stderr,none": 0.03404916326237584
    },
    "tmmluplus": {
      "acc,none": 0.24384920634920634,
      "acc_stderr,none": 0.002140082560788273,
      "acc_norm,none": 0.24384920634920634,
      "acc_norm_stderr,none": 0.002140082560788273,
      "alias": "tmmluplus"
    },
    "tmmluplus_STEM": {
      "acc,none": 0.24828571428571428,
      "acc_stderr,none": 0.0051705397459689585,
      "acc_norm,none": 0.24828571428571428,
      "acc_norm_stderr,none": 0.0051705397459689585,
      "alias": "tmmluplus_STEM"
    },
    "tmmluplus_advance_chemistry": {
      "alias": "advance chemistry",
      "acc,none": 0.23577235772357724,
      "acc_stderr,none": 0.03843066495214836,
      "acc_norm,none": 0.23577235772357724,
      "acc_norm_stderr,none": 0.03843066495214836
    },
    "tmmluplus_basic_medical_science": {
      "alias": "basic medical science",
      "acc,none": 0.22955974842767296,
      "acc_stderr,none": 0.01362294182225123,
      "acc_norm,none": 0.22955974842767296,
      "acc_norm_stderr,none": 0.01362294182225123
    },
    "tmmluplus_computer_science": {
      "alias": "computer science",
      "acc,none": 0.26436781609195403,
      "acc_stderr,none": 0.03352830517660787,
      "acc_norm,none": 0.26436781609195403,
      "acc_norm_stderr,none": 0.03352830517660787
    },
    "tmmluplus_engineering_math": {
      "alias": "engineering math",
      "acc,none": 0.27184466019417475,
      "acc_stderr,none": 0.044052680241409216,
      "acc_norm,none": 0.27184466019417475,
      "acc_norm_stderr,none": 0.044052680241409216
    },
    "tmmluplus_junior_chemistry": {
      "alias": "junior chemistry",
      "acc,none": 0.2535885167464115,
      "acc_stderr,none": 0.030166316298847997,
      "acc_norm,none": 0.2535885167464115,
      "acc_norm_stderr,none": 0.030166316298847997
    },
    "tmmluplus_junior_math_exam": {
      "alias": "junior math exam",
      "acc,none": 0.22285714285714286,
      "acc_stderr,none": 0.0315492532959613,
      "acc_norm,none": 0.22285714285714286,
      "acc_norm_stderr,none": 0.0315492532959613
    },
    "tmmluplus_junior_science_exam": {
      "alias": "junior science exam",
      "acc,none": 0.27230046948356806,
      "acc_stderr,none": 0.030572595618338316,
      "acc_norm,none": 0.27230046948356806,
      "acc_norm_stderr,none": 0.030572595618338316
    },
    "tmmluplus_linear_algebra": {
      "alias": "linear algebra",
      "acc,none": 0.21428571428571427,
      "acc_stderr,none": 0.06408213992247222,
      "acc_norm,none": 0.21428571428571427,
      "acc_norm_stderr,none": 0.06408213992247222
    },
    "tmmluplus_organic_chemistry": {
      "alias": "organic chemistry",
      "acc,none": 0.23853211009174313,
      "acc_stderr,none": 0.041009771029340336,
      "acc_norm,none": 0.23853211009174313,
      "acc_norm_stderr,none": 0.041009771029340336
    },
    "tmmluplus_pharmacy": {
      "alias": "pharmacy",
      "acc,none": 0.2557544757033248,
      "acc_stderr,none": 0.022092122036009933,
      "acc_norm,none": 0.2557544757033248,
      "acc_norm_stderr,none": 0.022092122036009933
    },
    "tmmluplus_physics": {
      "alias": "physics",
      "acc,none": 0.27835051546391754,
      "acc_stderr,none": 0.04574288492087335,
      "acc_norm,none": 0.27835051546391754,
      "acc_norm_stderr,none": 0.04574288492087335
    },
    "tmmluplus_secondary_physics": {
      "alias": "secondary physics",
      "acc,none": 0.26785714285714285,
      "acc_stderr,none": 0.04203277291467764,
      "acc_norm,none": 0.26785714285714285,
      "acc_norm_stderr,none": 0.04203277291467764
    },
    "tmmluplus_statistics_and_machine_learning": {
      "alias": "statistics and machine learning",
      "acc,none": 0.24553571428571427,
      "acc_stderr,none": 0.028822008850045506,
      "acc_norm,none": 0.24553571428571427,
      "acc_norm_stderr,none": 0.028822008850045506
    },
    "tmmluplus_tve_mathematics": {
      "alias": "tve mathematics",
      "acc,none": 0.24666666666666667,
      "acc_stderr,none": 0.035314713763569365,
      "acc_norm,none": 0.24666666666666667,
      "acc_norm_stderr,none": 0.035314713763569365
    },
    "tmmluplus_tve_natural_sciences": {
      "alias": "tve natural sciences",
      "acc,none": 0.2665094339622642,
      "acc_stderr,none": 0.0214972732105235,
      "acc_norm,none": 0.2665094339622642,
      "acc_norm_stderr,none": 0.0214972732105235
    },
    "tmmluplus_humanities": {
      "acc,none": 0.25865002836074874,
      "acc_stderr,none": 0.007381962280697481,
      "acc_norm,none": 0.25865002836074874,
      "acc_norm_stderr,none": 0.007381962280697481,
      "alias": "tmmluplus_humanities"
    },
    "tmmluplus_administrative_law": {
      "alias": " - administrative law",
      "acc,none": 0.25476190476190474,
      "acc_stderr,none": 0.021286671384349327,
      "acc_norm,none": 0.25476190476190474,
      "acc_norm_stderr,none": 0.021286671384349327
    },
    "tmmluplus_anti_money_laundering": {
      "alias": " - anti money laundering",
      "acc,none": 0.2835820895522388,
      "acc_stderr,none": 0.03908380123629713,
      "acc_norm,none": 0.2835820895522388,
      "acc_norm_stderr,none": 0.03908380123629713
    },
    "tmmluplus_general_principles_of_law": {
      "alias": " - general principles of law",
      "acc,none": 0.25471698113207547,
      "acc_stderr,none": 0.042520162237633115,
      "acc_norm,none": 0.25471698113207547,
      "acc_norm_stderr,none": 0.042520162237633115
    },
    "tmmluplus_introduction_to_law": {
      "alias": "introduction to law",
      "acc,none": 0.25738396624472576,
      "acc_stderr,none": 0.028458820991460288,
      "acc_norm,none": 0.25738396624472576,
      "acc_norm_stderr,none": 0.028458820991460288
    },
    "tmmluplus_jce_humanities": {
      "alias": "jce humanities",
      "acc,none": 0.3111111111111111,
      "acc_stderr,none": 0.04907240553386407,
      "acc_norm,none": 0.3111111111111111,
      "acc_norm_stderr,none": 0.04907240553386407
    },
    "tmmluplus_taxation": {
      "alias": "taxation",
      "acc,none": 0.232,
      "acc_stderr,none": 0.021826736290895578,
      "acc_norm,none": 0.232,
      "acc_norm_stderr,none": 0.021826736290895578
    },
    "tmmluplus_trust_practice": {
      "alias": "trust practice",
      "acc,none": 0.26932668329177056,
      "acc_stderr,none": 0.022180499372130034,
      "acc_norm,none": 0.26932668329177056,
      "acc_norm_stderr,none": 0.022180499372130034
    },
    "tmmluplus_other": {
      "acc,none": 0.2374986016332923,
      "acc_stderr,none": 0.0031856296558034776,
      "acc_norm,none": 0.2374986016332923,
      "acc_norm_stderr,none": 0.0031856296558034776,
      "alias": "tmmluplus_other"
    },
    "tmmluplus_accounting": {
      "alias": " - accounting",
      "acc,none": 0.25654450261780104,
      "acc_stderr,none": 0.031683419272745185,
      "acc_norm,none": 0.25654450261780104,
      "acc_norm_stderr,none": 0.031683419272745185
    },
    "tmmluplus_agriculture": {
      "alias": " - agriculture",
      "acc,none": 0.2847682119205298,
      "acc_stderr,none": 0.03684881521389023,
      "acc_norm,none": 0.2847682119205298,
      "acc_norm_stderr,none": 0.03684881521389023
    },
    "tmmluplus_auditing": {
      "alias": " - auditing",
      "acc,none": 0.2381818181818182,
      "acc_stderr,none": 0.018179996596943675,
      "acc_norm,none": 0.2381818181818182,
      "acc_norm_stderr,none": 0.018179996596943675
    },
    "tmmluplus_business_management": {
      "alias": " - business management",
      "acc,none": 0.23741007194244604,
      "acc_stderr,none": 0.036220593237998276,
      "acc_norm,none": 0.23741007194244604,
      "acc_norm_stderr,none": 0.036220593237998276
    },
    "tmmluplus_culinary_skills": {
      "alias": " - culinary skills",
      "acc,none": 0.2465753424657534,
      "acc_stderr,none": 0.02526668364163018,
      "acc_norm,none": 0.2465753424657534,
      "acc_norm_stderr,none": 0.02526668364163018
    },
    "tmmluplus_dentistry": {
      "alias": " - dentistry",
      "acc,none": 0.24561403508771928,
      "acc_stderr,none": 0.02157654254229619,
      "acc_norm,none": 0.24561403508771928,
      "acc_norm_stderr,none": 0.02157654254229619
    },
    "tmmluplus_finance_banking": {
      "alias": " - finance banking",
      "acc,none": 0.23703703703703705,
      "acc_stderr,none": 0.03673731683969506,
      "acc_norm,none": 0.23703703703703705,
      "acc_norm_stderr,none": 0.03673731683969506
    },
    "tmmluplus_financial_analysis": {
      "alias": " - financial analysis",
      "acc,none": 0.24607329842931938,
      "acc_stderr,none": 0.02206654397061107,
      "acc_norm,none": 0.24607329842931938,
      "acc_norm_stderr,none": 0.02206654397061107
    },
    "tmmluplus_fire_science": {
      "alias": " - fire science",
      "acc,none": 0.22580645161290322,
      "acc_stderr,none": 0.0376999167606953,
      "acc_norm,none": 0.22580645161290322,
      "acc_norm_stderr,none": 0.0376999167606953
    },
    "tmmluplus_insurance_studies": {
      "alias": " - insurance studies",
      "acc,none": 0.2236842105263158,
      "acc_stderr,none": 0.015125717547958658,
      "acc_norm,none": 0.2236842105263158,
      "acc_norm_stderr,none": 0.015125717547958658
    },
    "tmmluplus_junior_social_studies": {
      "alias": " - junior social studies",
      "acc,none": 0.24603174603174602,
      "acc_stderr,none": 0.03852273364924316,
      "acc_norm,none": 0.24603174603174602,
      "acc_norm_stderr,none": 0.03852273364924316
    },
    "tmmluplus_logic_reasoning": {
      "alias": " - logic reasoning",
      "acc,none": 0.23741007194244604,
      "acc_stderr,none": 0.036220593237998276,
      "acc_norm,none": 0.23741007194244604,
      "acc_norm_stderr,none": 0.036220593237998276
    },
    "tmmluplus_management_accounting": {
      "alias": " - management accounting",
      "acc,none": 0.2558139534883721,
      "acc_stderr,none": 0.029826067026139463,
      "acc_norm,none": 0.2558139534883721,
      "acc_norm_stderr,none": 0.029826067026139463
    },
    "tmmluplus_marketing_management": {
      "alias": " - marketing management",
      "acc,none": 0.20430107526881722,
      "acc_stderr,none": 0.04203545939892302,
      "acc_norm,none": 0.20430107526881722,
      "acc_norm_stderr,none": 0.04203545939892302
    },
    "tmmluplus_mechanical": {
      "alias": " - mechanical",
      "acc,none": 0.2288135593220339,
      "acc_stderr,none": 0.0388353872453885,
      "acc_norm,none": 0.2288135593220339,
      "acc_norm_stderr,none": 0.0388353872453885
    },
    "tmmluplus_music": {
      "alias": " - music",
      "acc,none": 0.20503597122302158,
      "acc_stderr,none": 0.024257658861836413,
      "acc_norm,none": 0.20503597122302158,
      "acc_norm_stderr,none": 0.024257658861836413
    },
    "tmmluplus_nautical_science": {
      "alias": " - nautical science",
      "acc,none": 0.2395644283121597,
      "acc_stderr,none": 0.018199566078724697,
      "acc_norm,none": 0.2395644283121597,
      "acc_norm_stderr,none": 0.018199566078724697
    },
    "tmmluplus_official_document_management": {
      "alias": " - official document management",
      "acc,none": 0.21171171171171171,
      "acc_stderr,none": 0.0274801337403637,
      "acc_norm,none": 0.21171171171171171,
      "acc_norm_stderr,none": 0.0274801337403637
    },
    "tmmluplus_optometry": {
      "alias": " - optometry",
      "acc,none": 0.22065217391304348,
      "acc_stderr,none": 0.013679241401935079,
      "acc_norm,none": 0.22065217391304348,
      "acc_norm_stderr,none": 0.013679241401935079
    },
    "tmmluplus_pharmacology": {
      "alias": "pharmacology",
      "acc,none": 0.2391681109185442,
      "acc_stderr,none": 0.017773982509077326,
      "acc_norm,none": 0.2391681109185442,
      "acc_norm_stderr,none": 0.017773982509077326
    },
    "tmmluplus_real_estate": {
      "alias": "real estate",
      "acc,none": 0.22826086956521738,
      "acc_stderr,none": 0.04399773283251793,
      "acc_norm,none": 0.22826086956521738,
      "acc_norm_stderr,none": 0.04399773283251793
    },
    "tmmluplus_technical": {
      "alias": "technical",
      "acc,none": 0.24875621890547264,
      "acc_stderr,none": 0.021587643231778638,
      "acc_norm,none": 0.24875621890547264,
      "acc_norm_stderr,none": 0.021587643231778638
    },
    "tmmluplus_trade": {
      "alias": "trade",
      "acc,none": 0.2410358565737052,
      "acc_stderr,none": 0.01910876383047096,
      "acc_norm,none": 0.2410358565737052,
      "acc_norm_stderr,none": 0.01910876383047096
    },
    "tmmluplus_traditional_chinese_medicine_clinical_medicine": {
      "alias": "traditional chinese medicine clinical medicine",
      "acc,none": 0.2446043165467626,
      "acc_stderr,none": 0.02582732474559216,
      "acc_norm,none": 0.2446043165467626,
      "acc_norm_stderr,none": 0.02582732474559216
    },
    "tmmluplus_tve_design": {
      "alias": "tve design",
      "acc,none": 0.23541666666666666,
      "acc_stderr,none": 0.019384886377778347,
      "acc_norm,none": 0.23541666666666666,
      "acc_norm_stderr,none": 0.019384886377778347
    },
    "tmmluplus_veterinary_pathology": {
      "alias": "veterinary pathology",
      "acc,none": 0.24028268551236748,
      "acc_stderr,none": 0.02544267068000068,
      "acc_norm,none": 0.24028268551236748,
      "acc_norm_stderr,none": 0.02544267068000068
    },
    "tmmluplus_veterinary_pharmacology": {
      "alias": "veterinary pharmacology",
      "acc,none": 0.25925925925925924,
      "acc_stderr,none": 0.01887582190509728,
      "acc_norm,none": 0.25925925925925924,
      "acc_norm_stderr,none": 0.01887582190509728
    },
    "tmmluplus_social_sciences": {
      "acc,none": 0.24639140651225244,
      "acc_stderr,none": 0.003949265678834969,
      "acc_norm,none": 0.24639140651225244,
      "acc_norm_stderr,none": 0.003949265678834969,
      "alias": "tmmluplus_social_sciences"
    },
    "tmmluplus_chinese_language_and_literature": {
      "alias": " - chinese language and literature",
      "acc,none": 0.22613065326633167,
      "acc_stderr,none": 0.02972904413617896,
      "acc_norm,none": 0.22613065326633167,
      "acc_norm_stderr,none": 0.02972904413617896
    },
    "tmmluplus_clinical_psychology": {
      "alias": " - clinical psychology",
      "acc,none": 0.28,
      "acc_stderr,none": 0.04032129030193478,
      "acc_norm,none": 0.28,
      "acc_norm_stderr,none": 0.04032129030193478
    },
    "tmmluplus_economics": {
      "alias": " - economics",
      "acc,none": 0.26208651399491095,
      "acc_stderr,none": 0.022211711677747294,
      "acc_norm,none": 0.26208651399491095,
      "acc_norm_stderr,none": 0.022211711677747294
    },
    "tmmluplus_education": {
      "alias": " - education",
      "acc,none": 0.27419354838709675,
      "acc_stderr,none": 0.040224112657777866,
      "acc_norm,none": 0.27419354838709675,
      "acc_norm_stderr,none": 0.040224112657777866
    },
    "tmmluplus_education_(profession_level)": {
      "alias": " - education (profession level)",
      "acc,none": 0.24897119341563786,
      "acc_stderr,none": 0.01963504852153283,
      "acc_norm,none": 0.24897119341563786,
      "acc_norm_stderr,none": 0.01963504852153283
    },
    "tmmluplus_educational_psychology": {
      "alias": " - educational psychology",
      "acc,none": 0.23295454545454544,
      "acc_stderr,none": 0.031954139030501774,
      "acc_norm,none": 0.23295454545454544,
      "acc_norm_stderr,none": 0.031954139030501774
    },
    "tmmluplus_geography_of_taiwan": {
      "alias": " - geography of taiwan",
      "acc,none": 0.24348958333333334,
      "acc_stderr,none": 0.015497083299680966,
      "acc_norm,none": 0.24348958333333334,
      "acc_norm_stderr,none": 0.015497083299680966
    },
    "tmmluplus_human_behavior": {
      "alias": " - human behavior",
      "acc,none": 0.255663430420712,
      "acc_stderr,none": 0.024856708883392368,
      "acc_norm,none": 0.255663430420712,
      "acc_norm_stderr,none": 0.024856708883392368
    },
    "tmmluplus_junior_chinese_exam": {
      "alias": " - junior chinese exam",
      "acc,none": 0.2571428571428571,
      "acc_stderr,none": 0.033133343292217204,
      "acc_norm,none": 0.2571428571428571,
      "acc_norm_stderr,none": 0.033133343292217204
    },
    "tmmluplus_macroeconomics": {
      "alias": " - macroeconomics",
      "acc,none": 0.25790754257907544,
      "acc_stderr,none": 0.021605737836583285,
      "acc_norm,none": 0.25790754257907544,
      "acc_norm_stderr,none": 0.021605737836583285
    },
    "tmmluplus_national_protection": {
      "alias": " - national protection",
      "acc,none": 0.22748815165876776,
      "acc_stderr,none": 0.028928260405094858,
      "acc_norm,none": 0.22748815165876776,
      "acc_norm_stderr,none": 0.028928260405094858
    },
    "tmmluplus_occupational_therapy_for_psychological_disorders": {
      "alias": " - occupational therapy for psychological disorders",
      "acc,none": 0.23572744014732966,
      "acc_stderr,none": 0.018231800514936787,
      "acc_norm,none": 0.23572744014732966,
      "acc_norm_stderr,none": 0.018231800514936787
    },
    "tmmluplus_physical_education": {
      "alias": " - physical education",
      "acc,none": 0.31843575418994413,
      "acc_stderr,none": 0.03491839802265681,
      "acc_norm,none": 0.31843575418994413,
      "acc_norm_stderr,none": 0.03491839802265681
    },
    "tmmluplus_politic_science": {
      "alias": " - politic science",
      "acc,none": 0.24020100502512562,
      "acc_stderr,none": 0.013550133538957777,
      "acc_norm,none": 0.24020100502512562,
      "acc_norm_stderr,none": 0.013550133538957777
    },
    "tmmluplus_taiwanese_hokkien": {
      "alias": "taiwanese hokkien",
      "acc,none": 0.24031007751937986,
      "acc_stderr,none": 0.03776584354632765,
      "acc_norm,none": 0.24031007751937986,
      "acc_norm_stderr,none": 0.03776584354632765
    },
    "tmmluplus_three_principles_of_people": {
      "alias": "three principles of people",
      "acc,none": 0.18705035971223022,
      "acc_stderr,none": 0.03319491433937174,
      "acc_norm,none": 0.18705035971223022,
      "acc_norm_stderr,none": 0.03319491433937174
    },
    "tmmluplus_ttqav2": {
      "alias": "ttqav2",
      "acc,none": 0.2743362831858407,
      "acc_stderr,none": 0.0421599724384926,
      "acc_norm,none": 0.2743362831858407,
      "acc_norm_stderr,none": 0.0421599724384926
    },
    "tmmluplus_tve_chinese_language": {
      "alias": "tve chinese language",
      "acc,none": 0.2318840579710145,
      "acc_stderr,none": 0.019223183705185245,
      "acc_norm,none": 0.2318840579710145,
      "acc_norm_stderr,none": 0.019223183705185245
    }
  },
  "groups": {
    "aclue": {
      "acc,none": 0.2494965767217076,
      "acc_stderr,none": 0.006137887696298453,
      "acc_norm,none": 0.2494965767217076,
      "acc_norm_stderr,none": 0.006137887696298453,
      "alias": "aclue"
    },
    "ceval-valid": {
      "acc,none": 0.24145616641901932,
      "acc_stderr,none": 0.011730152635910575,
      "acc_norm,none": 0.24145616641901932,
      "acc_norm_stderr,none": 0.011730152635910575,
      "alias": "ceval-valid"
    },
    "cmmlu": {
      "acc,none": 0.25125194266965983,
      "acc_stderr,none": 0.004034458768106313,
      "acc_norm,none": 0.25125194266965983,
      "acc_norm_stderr,none": 0.004034458768106313,
      "alias": "cmmlu"
    },
    "tmmluplus": {
      "acc,none": 0.24384920634920634,
      "acc_stderr,none": 0.002140082560788273,
      "acc_norm,none": 0.24384920634920634,
      "acc_norm_stderr,none": 0.002140082560788273,
      "alias": "tmmluplus"
    },
    "tmmluplus_STEM": {
      "acc,none": 0.24828571428571428,
      "acc_stderr,none": 0.0051705397459689585,
      "acc_norm,none": 0.24828571428571428,
      "acc_norm_stderr,none": 0.0051705397459689585,
      "alias": "tmmluplus_STEM"
    },
    "tmmluplus_humanities": {
      "acc,none": 0.25865002836074874,
      "acc_stderr,none": 0.007381962280697481,
      "acc_norm,none": 0.25865002836074874,
      "acc_norm_stderr,none": 0.007381962280697481,
      "alias": "tmmluplus_humanities"
    },
    "tmmluplus_other": {
      "acc,none": 0.2374986016332923,
      "acc_stderr,none": 0.0031856296558034776,
      "acc_norm,none": 0.2374986016332923,
      "acc_norm_stderr,none": 0.0031856296558034776,
      "alias": "tmmluplus_other"
    },
    "tmmluplus_social_sciences": {
      "acc,none": 0.24639140651225244,
      "acc_stderr,none": 0.003949265678834969,
      "acc_norm,none": 0.24639140651225244,
      "acc_norm_stderr,none": 0.003949265678834969,
      "alias": "tmmluplus_social_sciences"
    }
  },
  "group_subtasks": {
    "ceval-valid": [
      "ceval-valid_computer_network",
      "ceval-valid_operating_system",
      "ceval-valid_computer_architecture",
      "ceval-valid_college_programming",
      "ceval-valid_college_physics",
      "ceval-valid_college_chemistry",
      "ceval-valid_advanced_mathematics",
      "ceval-valid_probability_and_statistics",
      "ceval-valid_discrete_mathematics",
      "ceval-valid_electrical_engineer",
      "ceval-valid_metrology_engineer",
      "ceval-valid_high_school_mathematics",
      "ceval-valid_high_school_physics",
      "ceval-valid_high_school_chemistry",
      "ceval-valid_high_school_biology",
      "ceval-valid_middle_school_mathematics",
      "ceval-valid_middle_school_biology",
      "ceval-valid_middle_school_physics",
      "ceval-valid_middle_school_chemistry",
      "ceval-valid_veterinary_medicine",
      "ceval-valid_college_economics",
      "ceval-valid_business_administration",
      "ceval-valid_marxism",
      "ceval-valid_mao_zedong_thought",
      "ceval-valid_education_science",
      "ceval-valid_teacher_qualification",
      "ceval-valid_high_school_politics",
      "ceval-valid_high_school_geography",
      "ceval-valid_middle_school_politics",
      "ceval-valid_middle_school_geography",
      "ceval-valid_modern_chinese_history",
      "ceval-valid_ideological_and_moral_cultivation",
      "ceval-valid_logic",
      "ceval-valid_law",
      "ceval-valid_chinese_language_and_literature",
      "ceval-valid_art_studies",
      "ceval-valid_professional_tour_guide",
      "ceval-valid_legal_professional",
      "ceval-valid_high_school_chinese",
      "ceval-valid_high_school_history",
      "ceval-valid_middle_school_history",
      "ceval-valid_civil_servant",
      "ceval-valid_sports_science",
      "ceval-valid_plant_protection",
      "ceval-valid_basic_medicine",
      "ceval-valid_clinical_medicine",
      "ceval-valid_urban_and_rural_planner",
      "ceval-valid_accountant",
      "ceval-valid_fire_engineer",
      "ceval-valid_environmental_impact_assessment_engineer",
      "ceval-valid_tax_accountant",
      "ceval-valid_physician"
    ],
    "ceval-valid_accountant": [],
    "ceval-valid_advanced_mathematics": [],
    "ceval-valid_art_studies": [],
    "ceval-valid_basic_medicine": [],
    "ceval-valid_business_administration": [],
    "ceval-valid_chinese_language_and_literature": [],
    "ceval-valid_civil_servant": [],
    "ceval-valid_clinical_medicine": [],
    "ceval-valid_college_chemistry": [],
    "ceval-valid_college_economics": [],
    "ceval-valid_college_physics": [],
    "ceval-valid_college_programming": [],
    "ceval-valid_computer_architecture": [],
    "ceval-valid_computer_network": [],
    "ceval-valid_discrete_mathematics": [],
    "ceval-valid_education_science": [],
    "ceval-valid_electrical_engineer": [],
    "ceval-valid_environmental_impact_assessment_engineer": [],
    "ceval-valid_fire_engineer": [],
    "ceval-valid_high_school_biology": [],
    "ceval-valid_high_school_chemistry": [],
    "ceval-valid_high_school_chinese": [],
    "ceval-valid_high_school_geography": [],
    "ceval-valid_high_school_history": [],
    "ceval-valid_high_school_mathematics": [],
    "ceval-valid_high_school_physics": [],
    "ceval-valid_high_school_politics": [],
    "ceval-valid_ideological_and_moral_cultivation": [],
    "ceval-valid_law": [],
    "ceval-valid_legal_professional": [],
    "ceval-valid_logic": [],
    "ceval-valid_mao_zedong_thought": [],
    "ceval-valid_marxism": [],
    "ceval-valid_metrology_engineer": [],
    "ceval-valid_middle_school_biology": [],
    "ceval-valid_middle_school_chemistry": [],
    "ceval-valid_middle_school_geography": [],
    "ceval-valid_middle_school_history": [],
    "ceval-valid_middle_school_mathematics": [],
    "ceval-valid_middle_school_physics": [],
    "ceval-valid_middle_school_politics": [],
    "ceval-valid_modern_chinese_history": [],
    "ceval-valid_operating_system": [],
    "ceval-valid_physician": [],
    "ceval-valid_plant_protection": [],
    "ceval-valid_probability_and_statistics": [],
    "ceval-valid_professional_tour_guide": [],
    "ceval-valid_sports_science": [],
    "ceval-valid_tax_accountant": [],
    "ceval-valid_teacher_qualification": [],
    "ceval-valid_urban_and_rural_planner": [],
    "ceval-valid_veterinary_medicine": [],
    "cmmlu": [
      "cmmlu_agronomy",
      "cmmlu_anatomy",
      "cmmlu_ancient_chinese",
      "cmmlu_arts",
      "cmmlu_astronomy",
      "cmmlu_business_ethics",
      "cmmlu_chinese_civil_service_exam",
      "cmmlu_chinese_driving_rule",
      "cmmlu_chinese_food_culture",
      "cmmlu_chinese_foreign_policy",
      "cmmlu_chinese_history",
      "cmmlu_chinese_literature",
      "cmmlu_chinese_teacher_qualification",
      "cmmlu_clinical_knowledge",
      "cmmlu_college_actuarial_science",
      "cmmlu_college_education",
      "cmmlu_college_engineering_hydrology",
      "cmmlu_college_law",
      "cmmlu_college_mathematics",
      "cmmlu_college_medical_statistics",
      "cmmlu_college_medicine",
      "cmmlu_computer_science",
      "cmmlu_computer_security",
      "cmmlu_conceptual_physics",
      "cmmlu_construction_project_management",
      "cmmlu_economics",
      "cmmlu_education",
      "cmmlu_electrical_engineering",
      "cmmlu_elementary_chinese",
      "cmmlu_elementary_commonsense",
      "cmmlu_elementary_information_and_technology",
      "cmmlu_elementary_mathematics",
      "cmmlu_ethnology",
      "cmmlu_food_science",
      "cmmlu_genetics",
      "cmmlu_global_facts",
      "cmmlu_high_school_biology",
      "cmmlu_high_school_chemistry",
      "cmmlu_high_school_geography",
      "cmmlu_high_school_mathematics",
      "cmmlu_high_school_physics",
      "cmmlu_high_school_politics",
      "cmmlu_human_sexuality",
      "cmmlu_international_law",
      "cmmlu_journalism",
      "cmmlu_jurisprudence",
      "cmmlu_legal_and_moral_basis",
      "cmmlu_logical",
      "cmmlu_machine_learning",
      "cmmlu_management",
      "cmmlu_marketing",
      "cmmlu_marxist_theory",
      "cmmlu_modern_chinese",
      "cmmlu_nutrition",
      "cmmlu_philosophy",
      "cmmlu_professional_accounting",
      "cmmlu_professional_law",
      "cmmlu_professional_medicine",
      "cmmlu_professional_psychology",
      "cmmlu_public_relations",
      "cmmlu_security_study",
      "cmmlu_sociology",
      "cmmlu_sports_science",
      "cmmlu_traditional_chinese_medicine",
      "cmmlu_virology",
      "cmmlu_world_history",
      "cmmlu_world_religions"
    ],
    "cmmlu_agronomy": [],
    "cmmlu_anatomy": [],
    "cmmlu_ancient_chinese": [],
    "cmmlu_arts": [],
    "cmmlu_astronomy": [],
    "cmmlu_business_ethics": [],
    "cmmlu_chinese_civil_service_exam": [],
    "cmmlu_chinese_driving_rule": [],
    "cmmlu_chinese_food_culture": [],
    "cmmlu_chinese_foreign_policy": [],
    "cmmlu_chinese_history": [],
    "cmmlu_chinese_literature": [],
    "cmmlu_chinese_teacher_qualification": [],
    "cmmlu_clinical_knowledge": [],
    "cmmlu_college_actuarial_science": [],
    "cmmlu_college_education": [],
    "cmmlu_college_engineering_hydrology": [],
    "cmmlu_college_law": [],
    "cmmlu_college_mathematics": [],
    "cmmlu_college_medical_statistics": [],
    "cmmlu_college_medicine": [],
    "cmmlu_computer_science": [],
    "cmmlu_computer_security": [],
    "cmmlu_conceptual_physics": [],
    "cmmlu_construction_project_management": [],
    "cmmlu_economics": [],
    "cmmlu_education": [],
    "cmmlu_electrical_engineering": [],
    "cmmlu_elementary_chinese": [],
    "cmmlu_elementary_commonsense": [],
    "cmmlu_elementary_information_and_technology": [],
    "cmmlu_elementary_mathematics": [],
    "cmmlu_ethnology": [],
    "cmmlu_food_science": [],
    "cmmlu_genetics": [],
    "cmmlu_global_facts": [],
    "cmmlu_high_school_biology": [],
    "cmmlu_high_school_chemistry": [],
    "cmmlu_high_school_geography": [],
    "cmmlu_high_school_mathematics": [],
    "cmmlu_high_school_physics": [],
    "cmmlu_high_school_politics": [],
    "cmmlu_human_sexuality": [],
    "cmmlu_international_law": [],
    "cmmlu_journalism": [],
    "cmmlu_jurisprudence": [],
    "cmmlu_legal_and_moral_basis": [],
    "cmmlu_logical": [],
    "cmmlu_machine_learning": [],
    "cmmlu_management": [],
    "cmmlu_marketing": [],
    "cmmlu_marxist_theory": [],
    "cmmlu_modern_chinese": [],
    "cmmlu_nutrition": [],
    "cmmlu_philosophy": [],
    "cmmlu_professional_accounting": [],
    "cmmlu_professional_law": [],
    "cmmlu_professional_medicine": [],
    "cmmlu_professional_psychology": [],
    "cmmlu_public_relations": [],
    "cmmlu_security_study": [],
    "cmmlu_sociology": [],
    "cmmlu_sports_science": [],
    "cmmlu_traditional_chinese_medicine": [],
    "cmmlu_virology": [],
    "cmmlu_world_history": [],
    "cmmlu_world_religions": [],
    "aclue": [
      "aclue_ancient_chinese_culture",
      "aclue_ancient_literature",
      "aclue_ancient_medical",
      "aclue_ancient_phonetics",
      "aclue_basic_ancient_chinese",
      "aclue_couplet_prediction",
      "aclue_homographic_character_resolution",
      "aclue_named_entity_recognition",
      "aclue_poetry_appreciate",
      "aclue_poetry_context_prediction",
      "aclue_poetry_quality_assessment",
      "aclue_poetry_sentiment_analysis",
      "aclue_polysemy_resolution",
      "aclue_reading_comprehension",
      "aclue_sentence_segmentation"
    ],
    "aclue_ancient_chinese_culture": [],
    "aclue_ancient_literature": [],
    "aclue_ancient_medical": [],
    "aclue_ancient_phonetics": [],
    "aclue_basic_ancient_chinese": [],
    "aclue_couplet_prediction": [],
    "aclue_homographic_character_resolution": [],
    "aclue_named_entity_recognition": [],
    "aclue_poetry_appreciate": [],
    "aclue_poetry_context_prediction": [],
    "aclue_poetry_quality_assessment": [],
    "aclue_poetry_sentiment_analysis": [],
    "aclue_polysemy_resolution": [],
    "aclue_reading_comprehension": [],
    "aclue_sentence_segmentation": [],
    "tmmluplus": [
      "tmmluplus_other",
      "tmmluplus_social_sciences",
      "tmmluplus_humanities",
      "tmmluplus_STEM"
    ],
    "tmmluplus_STEM": [
      "tmmluplus_advance_chemistry",
      "tmmluplus_basic_medical_science",
      "tmmluplus_computer_science",
      "tmmluplus_engineering_math",
      "tmmluplus_junior_chemistry",
      "tmmluplus_junior_math_exam",
      "tmmluplus_junior_science_exam",
      "tmmluplus_linear_algebra",
      "tmmluplus_organic_chemistry",
      "tmmluplus_pharmacy",
      "tmmluplus_physics",
      "tmmluplus_secondary_physics",
      "tmmluplus_statistics_and_machine_learning",
      "tmmluplus_tve_mathematics",
      "tmmluplus_tve_natural_sciences"
    ],
    "tmmluplus_advance_chemistry": [],
    "tmmluplus_basic_medical_science": [],
    "tmmluplus_computer_science": [],
    "tmmluplus_engineering_math": [],
    "tmmluplus_humanities": [
      "tmmluplus_administrative_law",
      "tmmluplus_anti_money_laundering",
      "tmmluplus_general_principles_of_law",
      "tmmluplus_introduction_to_law",
      "tmmluplus_jce_humanities",
      "tmmluplus_taxation",
      "tmmluplus_trust_practice"
    ],
    "tmmluplus_general_principles_of_law": [],
    "tmmluplus_anti_money_laundering": [],
    "tmmluplus_administrative_law": [],
    "tmmluplus_introduction_to_law": [],
    "tmmluplus_jce_humanities": [],
    "tmmluplus_junior_chemistry": [],
    "tmmluplus_junior_math_exam": [],
    "tmmluplus_junior_science_exam": [],
    "tmmluplus_linear_algebra": [],
    "tmmluplus_organic_chemistry": [],
    "tmmluplus_other": [
      "tmmluplus_accounting",
      "tmmluplus_agriculture",
      "tmmluplus_auditing",
      "tmmluplus_business_management",
      "tmmluplus_culinary_skills",
      "tmmluplus_dentistry",
      "tmmluplus_finance_banking",
      "tmmluplus_financial_analysis",
      "tmmluplus_fire_science",
      "tmmluplus_insurance_studies",
      "tmmluplus_junior_social_studies",
      "tmmluplus_logic_reasoning",
      "tmmluplus_management_accounting",
      "tmmluplus_marketing_management",
      "tmmluplus_mechanical",
      "tmmluplus_music",
      "tmmluplus_nautical_science",
      "tmmluplus_official_document_management",
      "tmmluplus_optometry",
      "tmmluplus_pharmacology",
      "tmmluplus_real_estate",
      "tmmluplus_technical",
      "tmmluplus_trade",
      "tmmluplus_traditional_chinese_medicine_clinical_medicine",
      "tmmluplus_tve_design",
      "tmmluplus_veterinary_pathology",
      "tmmluplus_veterinary_pharmacology"
    ],
    "tmmluplus_optometry": [],
    "tmmluplus_official_document_management": [],
    "tmmluplus_nautical_science": [],
    "tmmluplus_music": [],
    "tmmluplus_mechanical": [],
    "tmmluplus_marketing_management": [],
    "tmmluplus_management_accounting": [],
    "tmmluplus_logic_reasoning": [],
    "tmmluplus_junior_social_studies": [],
    "tmmluplus_insurance_studies": [],
    "tmmluplus_fire_science": [],
    "tmmluplus_financial_analysis": [],
    "tmmluplus_finance_banking": [],
    "tmmluplus_dentistry": [],
    "tmmluplus_culinary_skills": [],
    "tmmluplus_business_management": [],
    "tmmluplus_auditing": [],
    "tmmluplus_agriculture": [],
    "tmmluplus_accounting": [],
    "tmmluplus_pharmacology": [],
    "tmmluplus_pharmacy": [],
    "tmmluplus_physics": [],
    "tmmluplus_real_estate": [],
    "tmmluplus_secondary_physics": [],
    "tmmluplus_social_sciences": [
      "tmmluplus_chinese_language_and_literature",
      "tmmluplus_clinical_psychology",
      "tmmluplus_economics",
      "tmmluplus_education",
      "tmmluplus_education_(profession_level)",
      "tmmluplus_educational_psychology",
      "tmmluplus_geography_of_taiwan",
      "tmmluplus_human_behavior",
      "tmmluplus_junior_chinese_exam",
      "tmmluplus_macroeconomics",
      "tmmluplus_national_protection",
      "tmmluplus_occupational_therapy_for_psychological_disorders",
      "tmmluplus_physical_education",
      "tmmluplus_politic_science",
      "tmmluplus_taiwanese_hokkien",
      "tmmluplus_three_principles_of_people",
      "tmmluplus_ttqav2",
      "tmmluplus_tve_chinese_language"
    ],
    "tmmluplus_politic_science": [],
    "tmmluplus_physical_education": [],
    "tmmluplus_occupational_therapy_for_psychological_disorders": [],
    "tmmluplus_national_protection": [],
    "tmmluplus_macroeconomics": [],
    "tmmluplus_junior_chinese_exam": [],
    "tmmluplus_human_behavior": [],
    "tmmluplus_geography_of_taiwan": [],
    "tmmluplus_educational_psychology": [],
    "tmmluplus_education_(profession_level)": [],
    "tmmluplus_education": [],
    "tmmluplus_economics": [],
    "tmmluplus_clinical_psychology": [],
    "tmmluplus_chinese_language_and_literature": [],
    "tmmluplus_statistics_and_machine_learning": [],
    "tmmluplus_taiwanese_hokkien": [],
    "tmmluplus_taxation": [],
    "tmmluplus_technical": [],
    "tmmluplus_three_principles_of_people": [],
    "tmmluplus_trade": [],
    "tmmluplus_traditional_chinese_medicine_clinical_medicine": [],
    "tmmluplus_trust_practice": [],
    "tmmluplus_ttqav2": [],
    "tmmluplus_tve_chinese_language": [],
    "tmmluplus_tve_design": [],
    "tmmluplus_tve_mathematics": [],
    "tmmluplus_tve_natural_sciences": [],
    "tmmluplus_veterinary_pathology": [],
    "tmmluplus_veterinary_pharmacology": []
  },
  "configs": {
    "aclue_ancient_chinese_culture": {
      "task": "aclue_ancient_chinese_culture",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "ancient_chinese_culture",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "aclue_ancient_literature": {
      "task": "aclue_ancient_literature",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "ancient_literature",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "aclue_ancient_medical": {
      "task": "aclue_ancient_medical",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "ancient_medical",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "aclue_ancient_phonetics": {
      "task": "aclue_ancient_phonetics",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "ancient_phonetics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "aclue_basic_ancient_chinese": {
      "task": "aclue_basic_ancient_chinese",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "basic_ancient_chinese",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "aclue_couplet_prediction": {
      "task": "aclue_couplet_prediction",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "couplet_prediction",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "aclue_homographic_character_resolution": {
      "task": "aclue_homographic_character_resolution",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "homographic_character_resolution",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "aclue_named_entity_recognition": {
      "task": "aclue_named_entity_recognition",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "named_entity_recognition",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "aclue_poetry_appreciate": {
      "task": "aclue_poetry_appreciate",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "poetry_appreciate",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "aclue_poetry_context_prediction": {
      "task": "aclue_poetry_context_prediction",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "poetry_context_prediction",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "aclue_poetry_quality_assessment": {
      "task": "aclue_poetry_quality_assessment",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "poetry_quality_assessment",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "aclue_poetry_sentiment_analysis": {
      "task": "aclue_poetry_sentiment_analysis",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "poetry_sentiment_analysis",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "aclue_polysemy_resolution": {
      "task": "aclue_polysemy_resolution",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "polysemy_resolution",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "aclue_reading_comprehension": {
      "task": "aclue_reading_comprehension",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "reading_comprehension",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "aclue_sentence_segmentation": {
      "task": "aclue_sentence_segmentation",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "sentence_segmentation",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_accountant": {
      "task": "ceval-valid_accountant",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "accountant",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_advanced_mathematics": {
      "task": "ceval-valid_advanced_mathematics",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "advanced_mathematics",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_art_studies": {
      "task": "ceval-valid_art_studies",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "art_studies",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_basic_medicine": {
      "task": "ceval-valid_basic_medicine",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "basic_medicine",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_business_administration": {
      "task": "ceval-valid_business_administration",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "business_administration",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_chinese_language_and_literature": {
      "task": "ceval-valid_chinese_language_and_literature",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "chinese_language_and_literature",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_civil_servant": {
      "task": "ceval-valid_civil_servant",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "civil_servant",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_clinical_medicine": {
      "task": "ceval-valid_clinical_medicine",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "clinical_medicine",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_college_chemistry": {
      "task": "ceval-valid_college_chemistry",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "college_chemistry",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_college_economics": {
      "task": "ceval-valid_college_economics",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "college_economics",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_college_physics": {
      "task": "ceval-valid_college_physics",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "college_physics",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_college_programming": {
      "task": "ceval-valid_college_programming",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "college_programming",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_computer_architecture": {
      "task": "ceval-valid_computer_architecture",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "computer_architecture",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_computer_network": {
      "task": "ceval-valid_computer_network",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "computer_network",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_discrete_mathematics": {
      "task": "ceval-valid_discrete_mathematics",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "discrete_mathematics",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_education_science": {
      "task": "ceval-valid_education_science",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "education_science",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_electrical_engineer": {
      "task": "ceval-valid_electrical_engineer",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "electrical_engineer",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_environmental_impact_assessment_engineer": {
      "task": "ceval-valid_environmental_impact_assessment_engineer",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "environmental_impact_assessment_engineer",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_fire_engineer": {
      "task": "ceval-valid_fire_engineer",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "fire_engineer",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_high_school_biology": {
      "task": "ceval-valid_high_school_biology",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "high_school_biology",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_high_school_chemistry": {
      "task": "ceval-valid_high_school_chemistry",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "high_school_chemistry",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_high_school_chinese": {
      "task": "ceval-valid_high_school_chinese",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "high_school_chinese",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_high_school_geography": {
      "task": "ceval-valid_high_school_geography",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "high_school_geography",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_high_school_history": {
      "task": "ceval-valid_high_school_history",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "high_school_history",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_high_school_mathematics": {
      "task": "ceval-valid_high_school_mathematics",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "high_school_mathematics",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_high_school_physics": {
      "task": "ceval-valid_high_school_physics",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "high_school_physics",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_high_school_politics": {
      "task": "ceval-valid_high_school_politics",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "high_school_politics",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_ideological_and_moral_cultivation": {
      "task": "ceval-valid_ideological_and_moral_cultivation",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "ideological_and_moral_cultivation",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_law": {
      "task": "ceval-valid_law",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "law",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_legal_professional": {
      "task": "ceval-valid_legal_professional",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "legal_professional",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_logic": {
      "task": "ceval-valid_logic",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "logic",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_mao_zedong_thought": {
      "task": "ceval-valid_mao_zedong_thought",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "mao_zedong_thought",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_marxism": {
      "task": "ceval-valid_marxism",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "marxism",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_metrology_engineer": {
      "task": "ceval-valid_metrology_engineer",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "metrology_engineer",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_middle_school_biology": {
      "task": "ceval-valid_middle_school_biology",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "middle_school_biology",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_middle_school_chemistry": {
      "task": "ceval-valid_middle_school_chemistry",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "middle_school_chemistry",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_middle_school_geography": {
      "task": "ceval-valid_middle_school_geography",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "middle_school_geography",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_middle_school_history": {
      "task": "ceval-valid_middle_school_history",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "middle_school_history",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_middle_school_mathematics": {
      "task": "ceval-valid_middle_school_mathematics",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "middle_school_mathematics",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_middle_school_physics": {
      "task": "ceval-valid_middle_school_physics",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "middle_school_physics",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_middle_school_politics": {
      "task": "ceval-valid_middle_school_politics",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "middle_school_politics",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_modern_chinese_history": {
      "task": "ceval-valid_modern_chinese_history",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "modern_chinese_history",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_operating_system": {
      "task": "ceval-valid_operating_system",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "operating_system",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_physician": {
      "task": "ceval-valid_physician",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "physician",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_plant_protection": {
      "task": "ceval-valid_plant_protection",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "plant_protection",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_probability_and_statistics": {
      "task": "ceval-valid_probability_and_statistics",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "probability_and_statistics",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_professional_tour_guide": {
      "task": "ceval-valid_professional_tour_guide",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "professional_tour_guide",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_sports_science": {
      "task": "ceval-valid_sports_science",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "sports_science",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_tax_accountant": {
      "task": "ceval-valid_tax_accountant",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "tax_accountant",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_teacher_qualification": {
      "task": "ceval-valid_teacher_qualification",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "teacher_qualification",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_urban_and_rural_planner": {
      "task": "ceval-valid_urban_and_rural_planner",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "urban_and_rural_planner",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "ceval-valid_veterinary_medicine": {
      "task": "ceval-valid_veterinary_medicine",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "veterinary_medicine",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_agronomy": {
      "task": "cmmlu_agronomy",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "agronomy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_anatomy": {
      "task": "cmmlu_anatomy",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "anatomy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_ancient_chinese": {
      "task": "cmmlu_ancient_chinese",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "ancient_chinese",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_arts": {
      "task": "cmmlu_arts",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "arts",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_astronomy": {
      "task": "cmmlu_astronomy",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "astronomy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_business_ethics": {
      "task": "cmmlu_business_ethics",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "business_ethics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_chinese_civil_service_exam": {
      "task": "cmmlu_chinese_civil_service_exam",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "chinese_civil_service_exam",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_chinese_driving_rule": {
      "task": "cmmlu_chinese_driving_rule",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "chinese_driving_rule",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_chinese_food_culture": {
      "task": "cmmlu_chinese_food_culture",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "chinese_food_culture",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_chinese_foreign_policy": {
      "task": "cmmlu_chinese_foreign_policy",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "chinese_foreign_policy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_chinese_history": {
      "task": "cmmlu_chinese_history",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "chinese_history",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_chinese_literature": {
      "task": "cmmlu_chinese_literature",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "chinese_literature",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_chinese_teacher_qualification": {
      "task": "cmmlu_chinese_teacher_qualification",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "chinese_teacher_qualification",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_clinical_knowledge": {
      "task": "cmmlu_clinical_knowledge",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "clinical_knowledge",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_college_actuarial_science": {
      "task": "cmmlu_college_actuarial_science",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "college_actuarial_science",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_college_education": {
      "task": "cmmlu_college_education",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "college_education",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_college_engineering_hydrology": {
      "task": "cmmlu_college_engineering_hydrology",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "college_engineering_hydrology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_college_law": {
      "task": "cmmlu_college_law",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "college_law",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_college_mathematics": {
      "task": "cmmlu_college_mathematics",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "college_mathematics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_college_medical_statistics": {
      "task": "cmmlu_college_medical_statistics",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "college_medical_statistics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_college_medicine": {
      "task": "cmmlu_college_medicine",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "college_medicine",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_computer_science": {
      "task": "cmmlu_computer_science",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "computer_science",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_computer_security": {
      "task": "cmmlu_computer_security",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "computer_security",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_conceptual_physics": {
      "task": "cmmlu_conceptual_physics",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "conceptual_physics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_construction_project_management": {
      "task": "cmmlu_construction_project_management",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "construction_project_management",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_economics": {
      "task": "cmmlu_economics",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "economics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_education": {
      "task": "cmmlu_education",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "education",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_electrical_engineering": {
      "task": "cmmlu_electrical_engineering",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "electrical_engineering",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_elementary_chinese": {
      "task": "cmmlu_elementary_chinese",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "elementary_chinese",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_elementary_commonsense": {
      "task": "cmmlu_elementary_commonsense",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "elementary_commonsense",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_elementary_information_and_technology": {
      "task": "cmmlu_elementary_information_and_technology",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "elementary_information_and_technology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_elementary_mathematics": {
      "task": "cmmlu_elementary_mathematics",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "elementary_mathematics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_ethnology": {
      "task": "cmmlu_ethnology",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "ethnology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_food_science": {
      "task": "cmmlu_food_science",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "food_science",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_genetics": {
      "task": "cmmlu_genetics",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "genetics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_global_facts": {
      "task": "cmmlu_global_facts",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "global_facts",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_high_school_biology": {
      "task": "cmmlu_high_school_biology",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "high_school_biology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_high_school_chemistry": {
      "task": "cmmlu_high_school_chemistry",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "high_school_chemistry",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_high_school_geography": {
      "task": "cmmlu_high_school_geography",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "high_school_geography",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_high_school_mathematics": {
      "task": "cmmlu_high_school_mathematics",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "high_school_mathematics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_high_school_physics": {
      "task": "cmmlu_high_school_physics",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "high_school_physics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_high_school_politics": {
      "task": "cmmlu_high_school_politics",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "high_school_politics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_human_sexuality": {
      "task": "cmmlu_human_sexuality",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "human_sexuality",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_international_law": {
      "task": "cmmlu_international_law",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "international_law",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_journalism": {
      "task": "cmmlu_journalism",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "journalism",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_jurisprudence": {
      "task": "cmmlu_jurisprudence",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "jurisprudence",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_legal_and_moral_basis": {
      "task": "cmmlu_legal_and_moral_basis",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "legal_and_moral_basis",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_logical": {
      "task": "cmmlu_logical",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "logical",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_machine_learning": {
      "task": "cmmlu_machine_learning",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "machine_learning",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_management": {
      "task": "cmmlu_management",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "management",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_marketing": {
      "task": "cmmlu_marketing",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "marketing",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_marxist_theory": {
      "task": "cmmlu_marxist_theory",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "marxist_theory",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_modern_chinese": {
      "task": "cmmlu_modern_chinese",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "modern_chinese",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_nutrition": {
      "task": "cmmlu_nutrition",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "nutrition",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_philosophy": {
      "task": "cmmlu_philosophy",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "philosophy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_professional_accounting": {
      "task": "cmmlu_professional_accounting",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "professional_accounting",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_professional_law": {
      "task": "cmmlu_professional_law",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "professional_law",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_professional_medicine": {
      "task": "cmmlu_professional_medicine",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "professional_medicine",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_professional_psychology": {
      "task": "cmmlu_professional_psychology",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "professional_psychology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_public_relations": {
      "task": "cmmlu_public_relations",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "public_relations",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_security_study": {
      "task": "cmmlu_security_study",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "security_study",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_sociology": {
      "task": "cmmlu_sociology",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "sociology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_sports_science": {
      "task": "cmmlu_sports_science",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "sports_science",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_traditional_chinese_medicine": {
      "task": "cmmlu_traditional_chinese_medicine",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "traditional_chinese_medicine",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_virology": {
      "task": "cmmlu_virology",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "virology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_world_history": {
      "task": "cmmlu_world_history",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "world_history",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "cmmlu_world_religions": {
      "task": "cmmlu_world_religions",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "world_religions",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "dev",
        "process_docs": null,
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_accounting": {
      "task": "tmmluplus_accounting",
      "task_alias": "accounting",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "accounting",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2fe29d900>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_administrative_law": {
      "task": "tmmluplus_administrative_law",
      "task_alias": "administrative law",
      "tag": "tmmluplus_humanities_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "administrative_law",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feb627a0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_advance_chemistry": {
      "task": "tmmluplus_advance_chemistry",
      "task_alias": "advance chemistry",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "advance_chemistry",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feb60820>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_agriculture": {
      "task": "tmmluplus_agriculture",
      "task_alias": "agriculture",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "agriculture",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2fe29d510>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_anti_money_laundering": {
      "task": "tmmluplus_anti_money_laundering",
      "task_alias": "anti money laundering",
      "tag": "tmmluplus_humanities_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "anti_money_laundering",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feb624d0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_auditing": {
      "task": "tmmluplus_auditing",
      "task_alias": "auditing",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "auditing",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2fe29cee0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_basic_medical_science": {
      "task": "tmmluplus_basic_medical_science",
      "task_alias": "basic medical science",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "basic_medical_science",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feb600d0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_business_management": {
      "task": "tmmluplus_business_management",
      "task_alias": "business management",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "business_management",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2fe29cd30>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_chinese_language_and_literature": {
      "task": "tmmluplus_chinese_language_and_literature",
      "task_alias": "chinese language and literature",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "chinese_language_and_literature",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2febdb370>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_clinical_psychology": {
      "task": "tmmluplus_clinical_psychology",
      "task_alias": "clinical psychology",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "clinical_psychology",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2febdb130>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_computer_science": {
      "task": "tmmluplus_computer_science",
      "task_alias": "computer science",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "computer_science",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2fefffd00>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_culinary_skills": {
      "task": "tmmluplus_culinary_skills",
      "task_alias": "culinary skills",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "culinary_skills",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2fe29ca60>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_dentistry": {
      "task": "tmmluplus_dentistry",
      "task_alias": "dentistry",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "dentistry",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2fe29c4c0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_economics": {
      "task": "tmmluplus_economics",
      "task_alias": "economics",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "economics",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2febdaa70>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_education": {
      "task": "tmmluplus_education",
      "task_alias": "education",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "education",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2febda320>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_education_(profession_level)": {
      "task": "tmmluplus_education_(profession_level)",
      "task_alias": "education (profession level)",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "education_(profession_level)",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2febd9e10>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_educational_psychology": {
      "task": "tmmluplus_educational_psychology",
      "task_alias": "educational psychology",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "educational_psychology",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2febd9ab0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_engineering_math": {
      "task": "tmmluplus_engineering_math",
      "task_alias": "engineering math",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "engineering_math",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2fefffac0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_finance_banking": {
      "task": "tmmluplus_finance_banking",
      "task_alias": "finance banking",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "finance_banking",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feaffd00>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_financial_analysis": {
      "task": "tmmluplus_financial_analysis",
      "task_alias": "financial analysis",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "financial_analysis",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feaff2e0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_fire_science": {
      "task": "tmmluplus_fire_science",
      "task_alias": "fire science",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "fire_science",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feaff880>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_general_principles_of_law": {
      "task": "tmmluplus_general_principles_of_law",
      "task_alias": "general principles of law",
      "tag": "tmmluplus_humanities_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "general_principles_of_law",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feb62050>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_geography_of_taiwan": {
      "task": "tmmluplus_geography_of_taiwan",
      "task_alias": "geography of taiwan",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "geography_of_taiwan",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2febd9870>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_human_behavior": {
      "task": "tmmluplus_human_behavior",
      "task_alias": "human behavior",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "human_behavior",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2febd9240>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_insurance_studies": {
      "task": "tmmluplus_insurance_studies",
      "task_alias": "insurance studies",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "insurance_studies",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feaff400>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_introduction_to_law": {
      "task": "tmmluplus_introduction_to_law",
      "task_alias": "introduction to law",
      "tag": "tmmluplus_humanities_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "introduction_to_law",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feffd6c0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_jce_humanities": {
      "task": "tmmluplus_jce_humanities",
      "task_alias": "jce humanities",
      "tag": "tmmluplus_humanities_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "jce_humanities",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feb61090>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_junior_chemistry": {
      "task": "tmmluplus_junior_chemistry",
      "task_alias": "junior chemistry",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "junior_chemistry",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2fefff910>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_junior_chinese_exam": {
      "task": "tmmluplus_junior_chinese_exam",
      "task_alias": "junior chinese exam",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "junior_chinese_exam",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2febd9000>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_junior_math_exam": {
      "task": "tmmluplus_junior_math_exam",
      "task_alias": "junior math exam",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "junior_math_exam",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2fefff760>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_junior_science_exam": {
      "task": "tmmluplus_junior_science_exam",
      "task_alias": "junior science exam",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "junior_science_exam",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2fefff400>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_junior_social_studies": {
      "task": "tmmluplus_junior_social_studies",
      "task_alias": "junior social studies",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "junior_social_studies",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feaff130>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_linear_algebra": {
      "task": "tmmluplus_linear_algebra",
      "task_alias": "linear algebra",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "linear_algebra",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2fefff1c0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_logic_reasoning": {
      "task": "tmmluplus_logic_reasoning",
      "task_alias": "logic reasoning",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "logic_reasoning",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feafef80>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_macroeconomics": {
      "task": "tmmluplus_macroeconomics",
      "task_alias": "macroeconomics",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "macroeconomics",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2febd8dc0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_management_accounting": {
      "task": "tmmluplus_management_accounting",
      "task_alias": "management accounting",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "management_accounting",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feafecb0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_marketing_management": {
      "task": "tmmluplus_marketing_management",
      "task_alias": "marketing management",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "marketing_management",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feafe8c0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_mechanical": {
      "task": "tmmluplus_mechanical",
      "task_alias": "mechanical",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "mechanical",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feafe5f0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_music": {
      "task": "tmmluplus_music",
      "task_alias": "music",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "music",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feafdf30>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_national_protection": {
      "task": "tmmluplus_national_protection",
      "task_alias": "national protection",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "national_protection",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2febd8af0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_nautical_science": {
      "task": "tmmluplus_nautical_science",
      "task_alias": "nautical science",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "nautical_science",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feafdc60>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_occupational_therapy_for_psychological_disorders": {
      "task": "tmmluplus_occupational_therapy_for_psychological_disorders",
      "task_alias": "occupational therapy for psychological disorders",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "occupational_therapy_for_psychological_disorders",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2febd84c0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_official_document_management": {
      "task": "tmmluplus_official_document_management",
      "task_alias": "official document management",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "official_document_management",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feafd750>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_optometry": {
      "task": "tmmluplus_optometry",
      "task_alias": "optometry",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "optometry",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feafd510>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_organic_chemistry": {
      "task": "tmmluplus_organic_chemistry",
      "task_alias": "organic chemistry",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "organic_chemistry",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feffeef0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_pharmacology": {
      "task": "tmmluplus_pharmacology",
      "task_alias": "pharmacology",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "pharmacology",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feafd1b0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_pharmacy": {
      "task": "tmmluplus_pharmacy",
      "task_alias": "pharmacy",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "pharmacy",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feffeb00>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_physical_education": {
      "task": "tmmluplus_physical_education",
      "task_alias": "physical education",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "physical_education",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2febd8280>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_physics": {
      "task": "tmmluplus_physics",
      "task_alias": "physics",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "physics",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feffe8c0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_politic_science": {
      "task": "tmmluplus_politic_science",
      "task_alias": "politic science",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "politic_science",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2febd80d0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_real_estate": {
      "task": "tmmluplus_real_estate",
      "task_alias": "real estate",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "real_estate",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feafcb80>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_secondary_physics": {
      "task": "tmmluplus_secondary_physics",
      "task_alias": "secondary physics",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "secondary_physics",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feffe560>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_statistics_and_machine_learning": {
      "task": "tmmluplus_statistics_and_machine_learning",
      "task_alias": "statistics and machine learning",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "statistics_and_machine_learning",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feffe3b0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_taiwanese_hokkien": {
      "task": "tmmluplus_taiwanese_hokkien",
      "task_alias": "taiwanese hokkien",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "taiwanese_hokkien",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feb63eb0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_taxation": {
      "task": "tmmluplus_taxation",
      "task_alias": "taxation",
      "tag": "tmmluplus_humanities_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "taxation",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feb60ee0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_technical": {
      "task": "tmmluplus_technical",
      "task_alias": "technical",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "technical",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feb63ac0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_three_principles_of_people": {
      "task": "tmmluplus_three_principles_of_people",
      "task_alias": "three principles of people",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "three_principles_of_people",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feb63d00>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_trade": {
      "task": "tmmluplus_trade",
      "task_alias": "trade",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "trade",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feafc790>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_traditional_chinese_medicine_clinical_medicine": {
      "task": "tmmluplus_traditional_chinese_medicine_clinical_medicine",
      "task_alias": "traditional chinese medicine clinical medicine",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "traditional_chinese_medicine_clinical_medicine",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feafc310>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_trust_practice": {
      "task": "tmmluplus_trust_practice",
      "task_alias": "trust practice",
      "tag": "tmmluplus_humanities_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "trust_practice",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feb60af0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_ttqav2": {
      "task": "tmmluplus_ttqav2",
      "task_alias": "ttqav2",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "ttqav2",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feb63b50>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_tve_chinese_language": {
      "task": "tmmluplus_tve_chinese_language",
      "task_alias": "tve chinese language",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "tve_chinese_language",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feffd750>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_tve_design": {
      "task": "tmmluplus_tve_design",
      "task_alias": "tve design",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "tve_design",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": " \n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feafc040>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_tve_mathematics": {
      "task": "tmmluplus_tve_mathematics",
      "task_alias": "tve mathematics",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "tve_mathematics",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feffdcf0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_tve_natural_sciences": {
      "task": "tmmluplus_tve_natural_sciences",
      "task_alias": "tve natural sciences",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "tve_natural_sciences",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2feffd1b0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_veterinary_pathology": {
      "task": "tmmluplus_veterinary_pathology",
      "task_alias": "veterinary pathology",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "veterinary_pathology",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2febdbe20>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    },
    "tmmluplus_veterinary_pharmacology": {
      "task": "tmmluplus_veterinary_pharmacology",
      "task_alias": "veterinary pharmacology",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "veterinary_pharmacology",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n",
        "split": "train",
        "process_docs": "<function process_docs at 0x7fb2febdbbe0>",
        "fewshot_indices": null,
        "samples": null,
        "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
        "doc_to_choice": [
          "A",
          "B",
          "C",
          "D"
        ],
        "doc_to_target": "answer",
        "gen_prefix": null,
        "fewshot_delimiter": "\n\n",
        "target_delimiter": " "
      },
      "num_fewshot": 3,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "model": "vermind",
        "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
        "base_url": "http://127.0.0.1:8000/v1/completions",
        "num_concurrent": 64,
        "timeout": 120
      }
    }
  },
  "versions": {
    "aclue": 1.0,
    "aclue_ancient_chinese_culture": 1.0,
    "aclue_ancient_literature": 1.0,
    "aclue_ancient_medical": 1.0,
    "aclue_ancient_phonetics": 1.0,
    "aclue_basic_ancient_chinese": 1.0,
    "aclue_couplet_prediction": 1.0,
    "aclue_homographic_character_resolution": 1.0,
    "aclue_named_entity_recognition": 1.0,
    "aclue_poetry_appreciate": 1.0,
    "aclue_poetry_context_prediction": 1.0,
    "aclue_poetry_quality_assessment": 1.0,
    "aclue_poetry_sentiment_analysis": 1.0,
    "aclue_polysemy_resolution": 1.0,
    "aclue_reading_comprehension": 1.0,
    "aclue_sentence_segmentation": 1.0,
    "ceval-valid": 2.0,
    "ceval-valid_accountant": 2.0,
    "ceval-valid_advanced_mathematics": 2.0,
    "ceval-valid_art_studies": 2.0,
    "ceval-valid_basic_medicine": 2.0,
    "ceval-valid_business_administration": 2.0,
    "ceval-valid_chinese_language_and_literature": 2.0,
    "ceval-valid_civil_servant": 2.0,
    "ceval-valid_clinical_medicine": 2.0,
    "ceval-valid_college_chemistry": 2.0,
    "ceval-valid_college_economics": 2.0,
    "ceval-valid_college_physics": 2.0,
    "ceval-valid_college_programming": 2.0,
    "ceval-valid_computer_architecture": 2.0,
    "ceval-valid_computer_network": 2.0,
    "ceval-valid_discrete_mathematics": 2.0,
    "ceval-valid_education_science": 2.0,
    "ceval-valid_electrical_engineer": 2.0,
    "ceval-valid_environmental_impact_assessment_engineer": 2.0,
    "ceval-valid_fire_engineer": 2.0,
    "ceval-valid_high_school_biology": 2.0,
    "ceval-valid_high_school_chemistry": 2.0,
    "ceval-valid_high_school_chinese": 2.0,
    "ceval-valid_high_school_geography": 2.0,
    "ceval-valid_high_school_history": 2.0,
    "ceval-valid_high_school_mathematics": 2.0,
    "ceval-valid_high_school_physics": 2.0,
    "ceval-valid_high_school_politics": 2.0,
    "ceval-valid_ideological_and_moral_cultivation": 2.0,
    "ceval-valid_law": 2.0,
    "ceval-valid_legal_professional": 2.0,
    "ceval-valid_logic": 2.0,
    "ceval-valid_mao_zedong_thought": 2.0,
    "ceval-valid_marxism": 2.0,
    "ceval-valid_metrology_engineer": 2.0,
    "ceval-valid_middle_school_biology": 2.0,
    "ceval-valid_middle_school_chemistry": 2.0,
    "ceval-valid_middle_school_geography": 2.0,
    "ceval-valid_middle_school_history": 2.0,
    "ceval-valid_middle_school_mathematics": 2.0,
    "ceval-valid_middle_school_physics": 2.0,
    "ceval-valid_middle_school_politics": 2.0,
    "ceval-valid_modern_chinese_history": 2.0,
    "ceval-valid_operating_system": 2.0,
    "ceval-valid_physician": 2.0,
    "ceval-valid_plant_protection": 2.0,
    "ceval-valid_probability_and_statistics": 2.0,
    "ceval-valid_professional_tour_guide": 2.0,
    "ceval-valid_sports_science": 2.0,
    "ceval-valid_tax_accountant": 2.0,
    "ceval-valid_teacher_qualification": 2.0,
    "ceval-valid_urban_and_rural_planner": 2.0,
    "ceval-valid_veterinary_medicine": 2.0,
    "cmmlu": 1.0,
    "cmmlu_agronomy": 1.0,
    "cmmlu_anatomy": 1.0,
    "cmmlu_ancient_chinese": 1.0,
    "cmmlu_arts": 1.0,
    "cmmlu_astronomy": 1.0,
    "cmmlu_business_ethics": 1.0,
    "cmmlu_chinese_civil_service_exam": 1.0,
    "cmmlu_chinese_driving_rule": 1.0,
    "cmmlu_chinese_food_culture": 1.0,
    "cmmlu_chinese_foreign_policy": 1.0,
    "cmmlu_chinese_history": 1.0,
    "cmmlu_chinese_literature": 1.0,
    "cmmlu_chinese_teacher_qualification": 1.0,
    "cmmlu_clinical_knowledge": 1.0,
    "cmmlu_college_actuarial_science": 1.0,
    "cmmlu_college_education": 1.0,
    "cmmlu_college_engineering_hydrology": 1.0,
    "cmmlu_college_law": 1.0,
    "cmmlu_college_mathematics": 1.0,
    "cmmlu_college_medical_statistics": 1.0,
    "cmmlu_college_medicine": 1.0,
    "cmmlu_computer_science": 1.0,
    "cmmlu_computer_security": 1.0,
    "cmmlu_conceptual_physics": 1.0,
    "cmmlu_construction_project_management": 1.0,
    "cmmlu_economics": 1.0,
    "cmmlu_education": 1.0,
    "cmmlu_electrical_engineering": 1.0,
    "cmmlu_elementary_chinese": 1.0,
    "cmmlu_elementary_commonsense": 1.0,
    "cmmlu_elementary_information_and_technology": 1.0,
    "cmmlu_elementary_mathematics": 1.0,
    "cmmlu_ethnology": 1.0,
    "cmmlu_food_science": 1.0,
    "cmmlu_genetics": 1.0,
    "cmmlu_global_facts": 1.0,
    "cmmlu_high_school_biology": 1.0,
    "cmmlu_high_school_chemistry": 1.0,
    "cmmlu_high_school_geography": 1.0,
    "cmmlu_high_school_mathematics": 1.0,
    "cmmlu_high_school_physics": 1.0,
    "cmmlu_high_school_politics": 1.0,
    "cmmlu_human_sexuality": 1.0,
    "cmmlu_international_law": 1.0,
    "cmmlu_journalism": 1.0,
    "cmmlu_jurisprudence": 1.0,
    "cmmlu_legal_and_moral_basis": 1.0,
    "cmmlu_logical": 1.0,
    "cmmlu_machine_learning": 1.0,
    "cmmlu_management": 1.0,
    "cmmlu_marketing": 1.0,
    "cmmlu_marxist_theory": 1.0,
    "cmmlu_modern_chinese": 1.0,
    "cmmlu_nutrition": 1.0,
    "cmmlu_philosophy": 1.0,
    "cmmlu_professional_accounting": 1.0,
    "cmmlu_professional_law": 1.0,
    "cmmlu_professional_medicine": 1.0,
    "cmmlu_professional_psychology": 1.0,
    "cmmlu_public_relations": 1.0,
    "cmmlu_security_study": 1.0,
    "cmmlu_sociology": 1.0,
    "cmmlu_sports_science": 1.0,
    "cmmlu_traditional_chinese_medicine": 1.0,
    "cmmlu_virology": 1.0,
    "cmmlu_world_history": 1.0,
    "cmmlu_world_religions": 1.0,
    "tmmluplus": 2.0,
    "tmmluplus_STEM": 2.0,
    "tmmluplus_accounting": 2.0,
    "tmmluplus_administrative_law": 2.0,
    "tmmluplus_advance_chemistry": 2.0,
    "tmmluplus_agriculture": 2.0,
    "tmmluplus_anti_money_laundering": 2.0,
    "tmmluplus_auditing": 2.0,
    "tmmluplus_basic_medical_science": 2.0,
    "tmmluplus_business_management": 2.0,
    "tmmluplus_chinese_language_and_literature": 2.0,
    "tmmluplus_clinical_psychology": 2.0,
    "tmmluplus_computer_science": 2.0,
    "tmmluplus_culinary_skills": 2.0,
    "tmmluplus_dentistry": 2.0,
    "tmmluplus_economics": 2.0,
    "tmmluplus_education": 2.0,
    "tmmluplus_education_(profession_level)": 2.0,
    "tmmluplus_educational_psychology": 2.0,
    "tmmluplus_engineering_math": 2.0,
    "tmmluplus_finance_banking": 2.0,
    "tmmluplus_financial_analysis": 2.0,
    "tmmluplus_fire_science": 2.0,
    "tmmluplus_general_principles_of_law": 2.0,
    "tmmluplus_geography_of_taiwan": 2.0,
    "tmmluplus_human_behavior": 2.0,
    "tmmluplus_humanities": 2.0,
    "tmmluplus_insurance_studies": 2.0,
    "tmmluplus_introduction_to_law": 2.0,
    "tmmluplus_jce_humanities": 2.0,
    "tmmluplus_junior_chemistry": 2.0,
    "tmmluplus_junior_chinese_exam": 2.0,
    "tmmluplus_junior_math_exam": 2.0,
    "tmmluplus_junior_science_exam": 2.0,
    "tmmluplus_junior_social_studies": 2.0,
    "tmmluplus_linear_algebra": 2.0,
    "tmmluplus_logic_reasoning": 2.0,
    "tmmluplus_macroeconomics": 2.0,
    "tmmluplus_management_accounting": 2.0,
    "tmmluplus_marketing_management": 2.0,
    "tmmluplus_mechanical": 2.0,
    "tmmluplus_music": 2.0,
    "tmmluplus_national_protection": 2.0,
    "tmmluplus_nautical_science": 2.0,
    "tmmluplus_occupational_therapy_for_psychological_disorders": 2.0,
    "tmmluplus_official_document_management": 2.0,
    "tmmluplus_optometry": 2.0,
    "tmmluplus_organic_chemistry": 2.0,
    "tmmluplus_other": 2.0,
    "tmmluplus_pharmacology": 2.0,
    "tmmluplus_pharmacy": 2.0,
    "tmmluplus_physical_education": 2.0,
    "tmmluplus_physics": 2.0,
    "tmmluplus_politic_science": 2.0,
    "tmmluplus_real_estate": 2.0,
    "tmmluplus_secondary_physics": 2.0,
    "tmmluplus_social_sciences": 2.0,
    "tmmluplus_statistics_and_machine_learning": 2.0,
    "tmmluplus_taiwanese_hokkien": 2.0,
    "tmmluplus_taxation": 2.0,
    "tmmluplus_technical": 2.0,
    "tmmluplus_three_principles_of_people": 2.0,
    "tmmluplus_trade": 2.0,
    "tmmluplus_traditional_chinese_medicine_clinical_medicine": 2.0,
    "tmmluplus_trust_practice": 2.0,
    "tmmluplus_ttqav2": 2.0,
    "tmmluplus_tve_chinese_language": 2.0,
    "tmmluplus_tve_design": 2.0,
    "tmmluplus_tve_mathematics": 2.0,
    "tmmluplus_tve_natural_sciences": 2.0,
    "tmmluplus_veterinary_pathology": 2.0,
    "tmmluplus_veterinary_pharmacology": 2.0
  },
  "n-shot": {
    "aclue_ancient_chinese_culture": 3,
    "aclue_ancient_literature": 3,
    "aclue_ancient_medical": 3,
    "aclue_ancient_phonetics": 3,
    "aclue_basic_ancient_chinese": 3,
    "aclue_couplet_prediction": 3,
    "aclue_homographic_character_resolution": 3,
    "aclue_named_entity_recognition": 3,
    "aclue_poetry_appreciate": 3,
    "aclue_poetry_context_prediction": 3,
    "aclue_poetry_quality_assessment": 3,
    "aclue_poetry_sentiment_analysis": 3,
    "aclue_polysemy_resolution": 3,
    "aclue_reading_comprehension": 3,
    "aclue_sentence_segmentation": 3,
    "ceval-valid_accountant": 3,
    "ceval-valid_advanced_mathematics": 3,
    "ceval-valid_art_studies": 3,
    "ceval-valid_basic_medicine": 3,
    "ceval-valid_business_administration": 3,
    "ceval-valid_chinese_language_and_literature": 3,
    "ceval-valid_civil_servant": 3,
    "ceval-valid_clinical_medicine": 3,
    "ceval-valid_college_chemistry": 3,
    "ceval-valid_college_economics": 3,
    "ceval-valid_college_physics": 3,
    "ceval-valid_college_programming": 3,
    "ceval-valid_computer_architecture": 3,
    "ceval-valid_computer_network": 3,
    "ceval-valid_discrete_mathematics": 3,
    "ceval-valid_education_science": 3,
    "ceval-valid_electrical_engineer": 3,
    "ceval-valid_environmental_impact_assessment_engineer": 3,
    "ceval-valid_fire_engineer": 3,
    "ceval-valid_high_school_biology": 3,
    "ceval-valid_high_school_chemistry": 3,
    "ceval-valid_high_school_chinese": 3,
    "ceval-valid_high_school_geography": 3,
    "ceval-valid_high_school_history": 3,
    "ceval-valid_high_school_mathematics": 3,
    "ceval-valid_high_school_physics": 3,
    "ceval-valid_high_school_politics": 3,
    "ceval-valid_ideological_and_moral_cultivation": 3,
    "ceval-valid_law": 3,
    "ceval-valid_legal_professional": 3,
    "ceval-valid_logic": 3,
    "ceval-valid_mao_zedong_thought": 3,
    "ceval-valid_marxism": 3,
    "ceval-valid_metrology_engineer": 3,
    "ceval-valid_middle_school_biology": 3,
    "ceval-valid_middle_school_chemistry": 3,
    "ceval-valid_middle_school_geography": 3,
    "ceval-valid_middle_school_history": 3,
    "ceval-valid_middle_school_mathematics": 3,
    "ceval-valid_middle_school_physics": 3,
    "ceval-valid_middle_school_politics": 3,
    "ceval-valid_modern_chinese_history": 3,
    "ceval-valid_operating_system": 3,
    "ceval-valid_physician": 3,
    "ceval-valid_plant_protection": 3,
    "ceval-valid_probability_and_statistics": 3,
    "ceval-valid_professional_tour_guide": 3,
    "ceval-valid_sports_science": 3,
    "ceval-valid_tax_accountant": 3,
    "ceval-valid_teacher_qualification": 3,
    "ceval-valid_urban_and_rural_planner": 3,
    "ceval-valid_veterinary_medicine": 3,
    "cmmlu_agronomy": 3,
    "cmmlu_anatomy": 3,
    "cmmlu_ancient_chinese": 3,
    "cmmlu_arts": 3,
    "cmmlu_astronomy": 3,
    "cmmlu_business_ethics": 3,
    "cmmlu_chinese_civil_service_exam": 3,
    "cmmlu_chinese_driving_rule": 3,
    "cmmlu_chinese_food_culture": 3,
    "cmmlu_chinese_foreign_policy": 3,
    "cmmlu_chinese_history": 3,
    "cmmlu_chinese_literature": 3,
    "cmmlu_chinese_teacher_qualification": 3,
    "cmmlu_clinical_knowledge": 3,
    "cmmlu_college_actuarial_science": 3,
    "cmmlu_college_education": 3,
    "cmmlu_college_engineering_hydrology": 3,
    "cmmlu_college_law": 3,
    "cmmlu_college_mathematics": 3,
    "cmmlu_college_medical_statistics": 3,
    "cmmlu_college_medicine": 3,
    "cmmlu_computer_science": 3,
    "cmmlu_computer_security": 3,
    "cmmlu_conceptual_physics": 3,
    "cmmlu_construction_project_management": 3,
    "cmmlu_economics": 3,
    "cmmlu_education": 3,
    "cmmlu_electrical_engineering": 3,
    "cmmlu_elementary_chinese": 3,
    "cmmlu_elementary_commonsense": 3,
    "cmmlu_elementary_information_and_technology": 3,
    "cmmlu_elementary_mathematics": 3,
    "cmmlu_ethnology": 3,
    "cmmlu_food_science": 3,
    "cmmlu_genetics": 3,
    "cmmlu_global_facts": 3,
    "cmmlu_high_school_biology": 3,
    "cmmlu_high_school_chemistry": 3,
    "cmmlu_high_school_geography": 3,
    "cmmlu_high_school_mathematics": 3,
    "cmmlu_high_school_physics": 3,
    "cmmlu_high_school_politics": 3,
    "cmmlu_human_sexuality": 3,
    "cmmlu_international_law": 3,
    "cmmlu_journalism": 3,
    "cmmlu_jurisprudence": 3,
    "cmmlu_legal_and_moral_basis": 3,
    "cmmlu_logical": 3,
    "cmmlu_machine_learning": 3,
    "cmmlu_management": 3,
    "cmmlu_marketing": 3,
    "cmmlu_marxist_theory": 3,
    "cmmlu_modern_chinese": 3,
    "cmmlu_nutrition": 3,
    "cmmlu_philosophy": 3,
    "cmmlu_professional_accounting": 3,
    "cmmlu_professional_law": 3,
    "cmmlu_professional_medicine": 3,
    "cmmlu_professional_psychology": 3,
    "cmmlu_public_relations": 3,
    "cmmlu_security_study": 3,
    "cmmlu_sociology": 3,
    "cmmlu_sports_science": 3,
    "cmmlu_traditional_chinese_medicine": 3,
    "cmmlu_virology": 3,
    "cmmlu_world_history": 3,
    "cmmlu_world_religions": 3,
    "tmmluplus_accounting": 3,
    "tmmluplus_administrative_law": 3,
    "tmmluplus_advance_chemistry": 3,
    "tmmluplus_agriculture": 3,
    "tmmluplus_anti_money_laundering": 3,
    "tmmluplus_auditing": 3,
    "tmmluplus_basic_medical_science": 3,
    "tmmluplus_business_management": 3,
    "tmmluplus_chinese_language_and_literature": 3,
    "tmmluplus_clinical_psychology": 3,
    "tmmluplus_computer_science": 3,
    "tmmluplus_culinary_skills": 3,
    "tmmluplus_dentistry": 3,
    "tmmluplus_economics": 3,
    "tmmluplus_education": 3,
    "tmmluplus_education_(profession_level)": 3,
    "tmmluplus_educational_psychology": 3,
    "tmmluplus_engineering_math": 3,
    "tmmluplus_finance_banking": 3,
    "tmmluplus_financial_analysis": 3,
    "tmmluplus_fire_science": 3,
    "tmmluplus_general_principles_of_law": 3,
    "tmmluplus_geography_of_taiwan": 3,
    "tmmluplus_human_behavior": 3,
    "tmmluplus_insurance_studies": 3,
    "tmmluplus_introduction_to_law": 3,
    "tmmluplus_jce_humanities": 3,
    "tmmluplus_junior_chemistry": 3,
    "tmmluplus_junior_chinese_exam": 3,
    "tmmluplus_junior_math_exam": 3,
    "tmmluplus_junior_science_exam": 3,
    "tmmluplus_junior_social_studies": 3,
    "tmmluplus_linear_algebra": 3,
    "tmmluplus_logic_reasoning": 3,
    "tmmluplus_macroeconomics": 3,
    "tmmluplus_management_accounting": 3,
    "tmmluplus_marketing_management": 3,
    "tmmluplus_mechanical": 3,
    "tmmluplus_music": 3,
    "tmmluplus_national_protection": 3,
    "tmmluplus_nautical_science": 3,
    "tmmluplus_occupational_therapy_for_psychological_disorders": 3,
    "tmmluplus_official_document_management": 3,
    "tmmluplus_optometry": 3,
    "tmmluplus_organic_chemistry": 3,
    "tmmluplus_pharmacology": 3,
    "tmmluplus_pharmacy": 3,
    "tmmluplus_physical_education": 3,
    "tmmluplus_physics": 3,
    "tmmluplus_politic_science": 3,
    "tmmluplus_real_estate": 3,
    "tmmluplus_secondary_physics": 3,
    "tmmluplus_statistics_and_machine_learning": 3,
    "tmmluplus_taiwanese_hokkien": 3,
    "tmmluplus_taxation": 3,
    "tmmluplus_technical": 3,
    "tmmluplus_three_principles_of_people": 3,
    "tmmluplus_trade": 3,
    "tmmluplus_traditional_chinese_medicine_clinical_medicine": 3,
    "tmmluplus_trust_practice": 3,
    "tmmluplus_ttqav2": 3,
    "tmmluplus_tve_chinese_language": 3,
    "tmmluplus_tve_design": 3,
    "tmmluplus_tve_mathematics": 3,
    "tmmluplus_tve_natural_sciences": 3,
    "tmmluplus_veterinary_pathology": 3,
    "tmmluplus_veterinary_pharmacology": 3
  },
  "higher_is_better": {
    "aclue": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_ancient_chinese_culture": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_ancient_literature": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_ancient_medical": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_ancient_phonetics": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_basic_ancient_chinese": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_couplet_prediction": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_homographic_character_resolution": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_named_entity_recognition": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_poetry_appreciate": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_poetry_context_prediction": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_poetry_quality_assessment": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_poetry_sentiment_analysis": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_polysemy_resolution": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_reading_comprehension": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_sentence_segmentation": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_accountant": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_advanced_mathematics": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_art_studies": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_basic_medicine": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_business_administration": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_chinese_language_and_literature": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_civil_servant": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_clinical_medicine": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_college_chemistry": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_college_economics": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_college_physics": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_college_programming": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_computer_architecture": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_computer_network": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_discrete_mathematics": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_education_science": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_electrical_engineer": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_environmental_impact_assessment_engineer": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_fire_engineer": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_high_school_biology": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_high_school_chemistry": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_high_school_chinese": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_high_school_geography": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_high_school_history": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_high_school_mathematics": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_high_school_physics": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_high_school_politics": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_ideological_and_moral_cultivation": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_law": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_legal_professional": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_logic": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_mao_zedong_thought": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_marxism": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_metrology_engineer": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_middle_school_biology": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_middle_school_chemistry": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_middle_school_geography": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_middle_school_history": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_middle_school_mathematics": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_middle_school_physics": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_middle_school_politics": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_modern_chinese_history": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_operating_system": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_physician": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_plant_protection": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_probability_and_statistics": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_professional_tour_guide": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_sports_science": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_tax_accountant": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_teacher_qualification": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_urban_and_rural_planner": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_veterinary_medicine": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_agronomy": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_anatomy": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_ancient_chinese": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_arts": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_astronomy": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_business_ethics": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_chinese_civil_service_exam": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_chinese_driving_rule": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_chinese_food_culture": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_chinese_foreign_policy": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_chinese_history": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_chinese_literature": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_chinese_teacher_qualification": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_clinical_knowledge": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_college_actuarial_science": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_college_education": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_college_engineering_hydrology": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_college_law": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_college_mathematics": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_college_medical_statistics": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_college_medicine": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_computer_science": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_computer_security": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_conceptual_physics": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_construction_project_management": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_economics": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_education": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_electrical_engineering": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_elementary_chinese": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_elementary_commonsense": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_elementary_information_and_technology": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_elementary_mathematics": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_ethnology": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_food_science": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_genetics": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_global_facts": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_high_school_biology": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_high_school_chemistry": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_high_school_geography": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_high_school_mathematics": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_high_school_physics": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_high_school_politics": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_human_sexuality": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_international_law": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_journalism": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_jurisprudence": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_legal_and_moral_basis": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_logical": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_machine_learning": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_management": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_marketing": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_marxist_theory": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_modern_chinese": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_nutrition": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_philosophy": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_professional_accounting": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_professional_law": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_professional_medicine": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_professional_psychology": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_public_relations": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_security_study": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_sociology": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_sports_science": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_traditional_chinese_medicine": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_virology": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_world_history": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_world_religions": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_STEM": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_accounting": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_administrative_law": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_advance_chemistry": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_agriculture": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_anti_money_laundering": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_auditing": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_basic_medical_science": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_business_management": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_chinese_language_and_literature": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_clinical_psychology": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_computer_science": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_culinary_skills": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_dentistry": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_economics": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_education": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_education_(profession_level)": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_educational_psychology": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_engineering_math": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_finance_banking": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_financial_analysis": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_fire_science": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_general_principles_of_law": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_geography_of_taiwan": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_human_behavior": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_humanities": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_insurance_studies": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_introduction_to_law": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_jce_humanities": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_junior_chemistry": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_junior_chinese_exam": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_junior_math_exam": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_junior_science_exam": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_junior_social_studies": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_linear_algebra": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_logic_reasoning": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_macroeconomics": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_management_accounting": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_marketing_management": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_mechanical": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_music": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_national_protection": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_nautical_science": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_occupational_therapy_for_psychological_disorders": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_official_document_management": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_optometry": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_organic_chemistry": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_other": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_pharmacology": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_pharmacy": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_physical_education": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_physics": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_politic_science": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_real_estate": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_secondary_physics": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_social_sciences": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_statistics_and_machine_learning": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_taiwanese_hokkien": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_taxation": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_technical": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_three_principles_of_people": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_trade": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_traditional_chinese_medicine_clinical_medicine": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_trust_practice": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_ttqav2": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_tve_chinese_language": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_tve_design": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_tve_mathematics": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_tve_natural_sciences": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_veterinary_pathology": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_veterinary_pharmacology": {
      "acc": true,
      "acc_norm": true
    }
  },
  "n-samples": {
    "tmmluplus_veterinary_pharmacology": {
      "original": 540,
      "effective": 540
    },
    "tmmluplus_veterinary_pathology": {
      "original": 283,
      "effective": 283
    },
    "tmmluplus_tve_natural_sciences": {
      "original": 424,
      "effective": 424
    },
    "tmmluplus_tve_mathematics": {
      "original": 150,
      "effective": 150
    },
    "tmmluplus_tve_design": {
      "original": 480,
      "effective": 480
    },
    "tmmluplus_tve_chinese_language": {
      "original": 483,
      "effective": 483
    },
    "tmmluplus_ttqav2": {
      "original": 113,
      "effective": 113
    },
    "tmmluplus_trust_practice": {
      "original": 401,
      "effective": 401
    },
    "tmmluplus_traditional_chinese_medicine_clinical_medicine": {
      "original": 278,
      "effective": 278
    },
    "tmmluplus_trade": {
      "original": 502,
      "effective": 502
    },
    "tmmluplus_three_principles_of_people": {
      "original": 139,
      "effective": 139
    },
    "tmmluplus_technical": {
      "original": 402,
      "effective": 402
    },
    "tmmluplus_taxation": {
      "original": 375,
      "effective": 375
    },
    "tmmluplus_taiwanese_hokkien": {
      "original": 129,
      "effective": 129
    },
    "tmmluplus_statistics_and_machine_learning": {
      "original": 224,
      "effective": 224
    },
    "tmmluplus_chinese_language_and_literature": {
      "original": 199,
      "effective": 199
    },
    "tmmluplus_clinical_psychology": {
      "original": 125,
      "effective": 125
    },
    "tmmluplus_economics": {
      "original": 393,
      "effective": 393
    },
    "tmmluplus_education": {
      "original": 124,
      "effective": 124
    },
    "tmmluplus_education_(profession_level)": {
      "original": 486,
      "effective": 486
    },
    "tmmluplus_educational_psychology": {
      "original": 176,
      "effective": 176
    },
    "tmmluplus_geography_of_taiwan": {
      "original": 768,
      "effective": 768
    },
    "tmmluplus_human_behavior": {
      "original": 309,
      "effective": 309
    },
    "tmmluplus_junior_chinese_exam": {
      "original": 175,
      "effective": 175
    },
    "tmmluplus_macroeconomics": {
      "original": 411,
      "effective": 411
    },
    "tmmluplus_national_protection": {
      "original": 211,
      "effective": 211
    },
    "tmmluplus_occupational_therapy_for_psychological_disorders": {
      "original": 543,
      "effective": 543
    },
    "tmmluplus_physical_education": {
      "original": 179,
      "effective": 179
    },
    "tmmluplus_politic_science": {
      "original": 995,
      "effective": 995
    },
    "tmmluplus_secondary_physics": {
      "original": 112,
      "effective": 112
    },
    "tmmluplus_real_estate": {
      "original": 92,
      "effective": 92
    },
    "tmmluplus_physics": {
      "original": 97,
      "effective": 97
    },
    "tmmluplus_pharmacy": {
      "original": 391,
      "effective": 391
    },
    "tmmluplus_pharmacology": {
      "original": 577,
      "effective": 577
    },
    "tmmluplus_accounting": {
      "original": 191,
      "effective": 191
    },
    "tmmluplus_agriculture": {
      "original": 151,
      "effective": 151
    },
    "tmmluplus_auditing": {
      "original": 550,
      "effective": 550
    },
    "tmmluplus_business_management": {
      "original": 139,
      "effective": 139
    },
    "tmmluplus_culinary_skills": {
      "original": 292,
      "effective": 292
    },
    "tmmluplus_dentistry": {
      "original": 399,
      "effective": 399
    },
    "tmmluplus_finance_banking": {
      "original": 135,
      "effective": 135
    },
    "tmmluplus_financial_analysis": {
      "original": 382,
      "effective": 382
    },
    "tmmluplus_fire_science": {
      "original": 124,
      "effective": 124
    },
    "tmmluplus_insurance_studies": {
      "original": 760,
      "effective": 760
    },
    "tmmluplus_junior_social_studies": {
      "original": 126,
      "effective": 126
    },
    "tmmluplus_logic_reasoning": {
      "original": 139,
      "effective": 139
    },
    "tmmluplus_management_accounting": {
      "original": 215,
      "effective": 215
    },
    "tmmluplus_marketing_management": {
      "original": 93,
      "effective": 93
    },
    "tmmluplus_mechanical": {
      "original": 118,
      "effective": 118
    },
    "tmmluplus_music": {
      "original": 278,
      "effective": 278
    },
    "tmmluplus_nautical_science": {
      "original": 551,
      "effective": 551
    },
    "tmmluplus_official_document_management": {
      "original": 222,
      "effective": 222
    },
    "tmmluplus_optometry": {
      "original": 920,
      "effective": 920
    },
    "tmmluplus_organic_chemistry": {
      "original": 109,
      "effective": 109
    },
    "tmmluplus_linear_algebra": {
      "original": 42,
      "effective": 42
    },
    "tmmluplus_junior_science_exam": {
      "original": 213,
      "effective": 213
    },
    "tmmluplus_junior_math_exam": {
      "original": 175,
      "effective": 175
    },
    "tmmluplus_junior_chemistry": {
      "original": 209,
      "effective": 209
    },
    "tmmluplus_jce_humanities": {
      "original": 90,
      "effective": 90
    },
    "tmmluplus_introduction_to_law": {
      "original": 237,
      "effective": 237
    },
    "tmmluplus_administrative_law": {
      "original": 420,
      "effective": 420
    },
    "tmmluplus_anti_money_laundering": {
      "original": 134,
      "effective": 134
    },
    "tmmluplus_general_principles_of_law": {
      "original": 106,
      "effective": 106
    },
    "tmmluplus_engineering_math": {
      "original": 103,
      "effective": 103
    },
    "tmmluplus_computer_science": {
      "original": 174,
      "effective": 174
    },
    "tmmluplus_basic_medical_science": {
      "original": 954,
      "effective": 954
    },
    "tmmluplus_advance_chemistry": {
      "original": 123,
      "effective": 123
    },
    "aclue_sentence_segmentation": {
      "original": 500,
      "effective": 500
    },
    "aclue_reading_comprehension": {
      "original": 101,
      "effective": 101
    },
    "aclue_polysemy_resolution": {
      "original": 500,
      "effective": 500
    },
    "aclue_poetry_sentiment_analysis": {
      "original": 500,
      "effective": 500
    },
    "aclue_poetry_quality_assessment": {
      "original": 406,
      "effective": 406
    },
    "aclue_poetry_context_prediction": {
      "original": 500,
      "effective": 500
    },
    "aclue_poetry_appreciate": {
      "original": 103,
      "effective": 103
    },
    "aclue_named_entity_recognition": {
      "original": 500,
      "effective": 500
    },
    "aclue_homographic_character_resolution": {
      "original": 500,
      "effective": 500
    },
    "aclue_couplet_prediction": {
      "original": 500,
      "effective": 500
    },
    "aclue_basic_ancient_chinese": {
      "original": 249,
      "effective": 249
    },
    "aclue_ancient_phonetics": {
      "original": 100,
      "effective": 100
    },
    "aclue_ancient_medical": {
      "original": 211,
      "effective": 211
    },
    "aclue_ancient_literature": {
      "original": 160,
      "effective": 160
    },
    "aclue_ancient_chinese_culture": {
      "original": 136,
      "effective": 136
    },
    "cmmlu_world_religions": {
      "original": 160,
      "effective": 160
    },
    "cmmlu_world_history": {
      "original": 161,
      "effective": 161
    },
    "cmmlu_virology": {
      "original": 169,
      "effective": 169
    },
    "cmmlu_traditional_chinese_medicine": {
      "original": 185,
      "effective": 185
    },
    "cmmlu_sports_science": {
      "original": 165,
      "effective": 165
    },
    "cmmlu_sociology": {
      "original": 226,
      "effective": 226
    },
    "cmmlu_security_study": {
      "original": 135,
      "effective": 135
    },
    "cmmlu_public_relations": {
      "original": 174,
      "effective": 174
    },
    "cmmlu_professional_psychology": {
      "original": 232,
      "effective": 232
    },
    "cmmlu_professional_medicine": {
      "original": 376,
      "effective": 376
    },
    "cmmlu_professional_law": {
      "original": 211,
      "effective": 211
    },
    "cmmlu_professional_accounting": {
      "original": 175,
      "effective": 175
    },
    "cmmlu_philosophy": {
      "original": 105,
      "effective": 105
    },
    "cmmlu_nutrition": {
      "original": 145,
      "effective": 145
    },
    "cmmlu_modern_chinese": {
      "original": 116,
      "effective": 116
    },
    "cmmlu_marxist_theory": {
      "original": 189,
      "effective": 189
    },
    "cmmlu_marketing": {
      "original": 180,
      "effective": 180
    },
    "cmmlu_management": {
      "original": 210,
      "effective": 210
    },
    "cmmlu_machine_learning": {
      "original": 122,
      "effective": 122
    },
    "cmmlu_logical": {
      "original": 123,
      "effective": 123
    },
    "cmmlu_legal_and_moral_basis": {
      "original": 214,
      "effective": 214
    },
    "cmmlu_jurisprudence": {
      "original": 411,
      "effective": 411
    },
    "cmmlu_journalism": {
      "original": 172,
      "effective": 172
    },
    "cmmlu_international_law": {
      "original": 185,
      "effective": 185
    },
    "cmmlu_human_sexuality": {
      "original": 126,
      "effective": 126
    },
    "cmmlu_high_school_politics": {
      "original": 143,
      "effective": 143
    },
    "cmmlu_high_school_physics": {
      "original": 110,
      "effective": 110
    },
    "cmmlu_high_school_mathematics": {
      "original": 164,
      "effective": 164
    },
    "cmmlu_high_school_geography": {
      "original": 118,
      "effective": 118
    },
    "cmmlu_high_school_chemistry": {
      "original": 132,
      "effective": 132
    },
    "cmmlu_high_school_biology": {
      "original": 169,
      "effective": 169
    },
    "cmmlu_global_facts": {
      "original": 149,
      "effective": 149
    },
    "cmmlu_genetics": {
      "original": 176,
      "effective": 176
    },
    "cmmlu_food_science": {
      "original": 143,
      "effective": 143
    },
    "cmmlu_ethnology": {
      "original": 135,
      "effective": 135
    },
    "cmmlu_elementary_mathematics": {
      "original": 230,
      "effective": 230
    },
    "cmmlu_elementary_information_and_technology": {
      "original": 238,
      "effective": 238
    },
    "cmmlu_elementary_commonsense": {
      "original": 198,
      "effective": 198
    },
    "cmmlu_elementary_chinese": {
      "original": 252,
      "effective": 252
    },
    "cmmlu_electrical_engineering": {
      "original": 172,
      "effective": 172
    },
    "cmmlu_education": {
      "original": 163,
      "effective": 163
    },
    "cmmlu_economics": {
      "original": 159,
      "effective": 159
    },
    "cmmlu_construction_project_management": {
      "original": 139,
      "effective": 139
    },
    "cmmlu_conceptual_physics": {
      "original": 147,
      "effective": 147
    },
    "cmmlu_computer_security": {
      "original": 171,
      "effective": 171
    },
    "cmmlu_computer_science": {
      "original": 204,
      "effective": 204
    },
    "cmmlu_college_medicine": {
      "original": 273,
      "effective": 273
    },
    "cmmlu_college_medical_statistics": {
      "original": 106,
      "effective": 106
    },
    "cmmlu_college_mathematics": {
      "original": 105,
      "effective": 105
    },
    "cmmlu_college_law": {
      "original": 108,
      "effective": 108
    },
    "cmmlu_college_engineering_hydrology": {
      "original": 106,
      "effective": 106
    },
    "cmmlu_college_education": {
      "original": 107,
      "effective": 107
    },
    "cmmlu_college_actuarial_science": {
      "original": 106,
      "effective": 106
    },
    "cmmlu_clinical_knowledge": {
      "original": 237,
      "effective": 237
    },
    "cmmlu_chinese_teacher_qualification": {
      "original": 179,
      "effective": 179
    },
    "cmmlu_chinese_literature": {
      "original": 204,
      "effective": 204
    },
    "cmmlu_chinese_history": {
      "original": 323,
      "effective": 323
    },
    "cmmlu_chinese_foreign_policy": {
      "original": 107,
      "effective": 107
    },
    "cmmlu_chinese_food_culture": {
      "original": 136,
      "effective": 136
    },
    "cmmlu_chinese_driving_rule": {
      "original": 131,
      "effective": 131
    },
    "cmmlu_chinese_civil_service_exam": {
      "original": 160,
      "effective": 160
    },
    "cmmlu_business_ethics": {
      "original": 209,
      "effective": 209
    },
    "cmmlu_astronomy": {
      "original": 165,
      "effective": 165
    },
    "cmmlu_arts": {
      "original": 160,
      "effective": 160
    },
    "cmmlu_ancient_chinese": {
      "original": 164,
      "effective": 164
    },
    "cmmlu_anatomy": {
      "original": 148,
      "effective": 148
    },
    "cmmlu_agronomy": {
      "original": 169,
      "effective": 169
    },
    "ceval-valid_veterinary_medicine": {
      "original": 23,
      "effective": 23
    },
    "ceval-valid_urban_and_rural_planner": {
      "original": 46,
      "effective": 46
    },
    "ceval-valid_teacher_qualification": {
      "original": 44,
      "effective": 44
    },
    "ceval-valid_tax_accountant": {
      "original": 49,
      "effective": 49
    },
    "ceval-valid_sports_science": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_professional_tour_guide": {
      "original": 29,
      "effective": 29
    },
    "ceval-valid_probability_and_statistics": {
      "original": 18,
      "effective": 18
    },
    "ceval-valid_plant_protection": {
      "original": 22,
      "effective": 22
    },
    "ceval-valid_physician": {
      "original": 49,
      "effective": 49
    },
    "ceval-valid_operating_system": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_modern_chinese_history": {
      "original": 23,
      "effective": 23
    },
    "ceval-valid_middle_school_politics": {
      "original": 21,
      "effective": 21
    },
    "ceval-valid_middle_school_physics": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_middle_school_mathematics": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_middle_school_history": {
      "original": 22,
      "effective": 22
    },
    "ceval-valid_middle_school_geography": {
      "original": 12,
      "effective": 12
    },
    "ceval-valid_middle_school_chemistry": {
      "original": 20,
      "effective": 20
    },
    "ceval-valid_middle_school_biology": {
      "original": 21,
      "effective": 21
    },
    "ceval-valid_metrology_engineer": {
      "original": 24,
      "effective": 24
    },
    "ceval-valid_marxism": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_mao_zedong_thought": {
      "original": 24,
      "effective": 24
    },
    "ceval-valid_logic": {
      "original": 22,
      "effective": 22
    },
    "ceval-valid_legal_professional": {
      "original": 23,
      "effective": 23
    },
    "ceval-valid_law": {
      "original": 24,
      "effective": 24
    },
    "ceval-valid_ideological_and_moral_cultivation": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_high_school_politics": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_high_school_physics": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_high_school_mathematics": {
      "original": 18,
      "effective": 18
    },
    "ceval-valid_high_school_history": {
      "original": 20,
      "effective": 20
    },
    "ceval-valid_high_school_geography": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_high_school_chinese": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_high_school_chemistry": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_high_school_biology": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_fire_engineer": {
      "original": 31,
      "effective": 31
    },
    "ceval-valid_environmental_impact_assessment_engineer": {
      "original": 31,
      "effective": 31
    },
    "ceval-valid_electrical_engineer": {
      "original": 37,
      "effective": 37
    },
    "ceval-valid_education_science": {
      "original": 29,
      "effective": 29
    },
    "ceval-valid_discrete_mathematics": {
      "original": 16,
      "effective": 16
    },
    "ceval-valid_computer_network": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_computer_architecture": {
      "original": 21,
      "effective": 21
    },
    "ceval-valid_college_programming": {
      "original": 37,
      "effective": 37
    },
    "ceval-valid_college_physics": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_college_economics": {
      "original": 55,
      "effective": 55
    },
    "ceval-valid_college_chemistry": {
      "original": 24,
      "effective": 24
    },
    "ceval-valid_clinical_medicine": {
      "original": 22,
      "effective": 22
    },
    "ceval-valid_civil_servant": {
      "original": 47,
      "effective": 47
    },
    "ceval-valid_chinese_language_and_literature": {
      "original": 23,
      "effective": 23
    },
    "ceval-valid_business_administration": {
      "original": 33,
      "effective": 33
    },
    "ceval-valid_basic_medicine": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_art_studies": {
      "original": 33,
      "effective": 33
    },
    "ceval-valid_advanced_mathematics": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_accountant": {
      "original": 49,
      "effective": 49
    }
  },
  "config": {
    "model": "local-completions",
    "model_args": {
      "model": "vermind",
      "tokenizer": "./output/sft_packed/sft_packed_768_4/checkpoint_339044",
      "base_url": "http://127.0.0.1:8000/v1/completions",
      "num_concurrent": 64,
      "timeout": 120
    },
    "batch_size": "32",
    "batch_sizes": [],
    "device": "cuda:0",
    "use_cache": null,
    "limit": null,
    "bootstrap_iters": 100000,
    "gen_kwargs": {},
    "random_seed": 0,
    "numpy_seed": 1234,
    "torch_seed": 1234,
    "fewshot_seed": 1234
  },
  "git_hash": "3d0dc94",
  "date": 1769675635.4657702,
  "pretty_env_info": "PyTorch version: 2.8.0+cu128\nIs debug build: False\nCUDA used to build PyTorch: 12.8\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.35\n\nPython version: 3.10.19 (main, Oct 21 2025, 16:43:05) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.10.134-16.3.al8.x86_64-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA A100-SXM4-80GB\nNvidia driver version: 570.133.20\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.3.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.3.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                    x86_64\nCPU op-mode(s):                  32-bit, 64-bit\nAddress sizes:                   46 bits physical, 57 bits virtual\nByte Order:                      Little Endian\nCPU(s):                          128\nOn-line CPU(s) list:             0-127\nVendor ID:                       GenuineIntel\nModel name:                      Intel(R) Xeon(R) Platinum 8369B CPU @ 2.90GHz\nCPU family:                      6\nModel:                           106\nThread(s) per core:              2\nCore(s) per socket:              32\nSocket(s):                       2\nStepping:                        6\nCPU max MHz:                     3500.0000\nCPU min MHz:                     800.0000\nBogoMIPS:                        5800.00\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\nVirtualization:                  VT-x\nL1d cache:                       3 MiB (64 instances)\nL1i cache:                       2 MiB (64 instances)\nL2 cache:                        80 MiB (64 instances)\nL3 cache:                        96 MiB (2 instances)\nNUMA node(s):                    2\nNUMA node0 CPU(s):               0-31,64-95\nNUMA node1 CPU(s):               32-63,96-127\nVulnerability Itlb multihit:     Not affected\nVulnerability L1tf:              Not affected\nVulnerability Mds:               Not affected\nVulnerability Meltdown:          Not affected\nVulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Retbleed:          Not affected\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:             Not affected\nVulnerability Tsx async abort:   Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==2.2.6\n[pip3] nvidia-cublas-cu12==12.8.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.8.90\n[pip3] nvidia-cuda-nvrtc-cu12==12.8.93\n[pip3] nvidia-cuda-runtime-cu12==12.8.90\n[pip3] nvidia-cudnn-cu12==9.10.2.21\n[pip3] nvidia-cufft-cu12==11.3.3.83\n[pip3] nvidia-curand-cu12==10.3.9.90\n[pip3] nvidia-cusolver-cu12==11.7.3.90\n[pip3] nvidia-cusparse-cu12==12.5.8.93\n[pip3] nvidia-cusparselt-cu12==0.7.1\n[pip3] nvidia-nccl-cu12==2.27.3\n[pip3] nvidia-nvjitlink-cu12==12.8.93\n[pip3] nvidia-nvtx-cu12==12.8.90\n[pip3] torch==2.8.0\n[pip3] triton==3.4.0\n[conda] numpy                     2.2.6                    pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.8.4.1                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.8.90                  pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.8.93                  pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.8.90                  pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.10.2.21                pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.3.3.83                pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.9.90                pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.7.3.90                pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.5.8.93                pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.7.1                    pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.27.3                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.8.93                  pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.8.90                  pypi_0    pypi\n[conda] torch                     2.8.0                    pypi_0    pypi\n[conda] triton                    3.4.0                    pypi_0    pypi",
  "transformers_version": "5.0.0",
  "lm_eval_version": "0.4.10",
  "upper_git_hash": null,
  "tokenizer_pad_token": [
    "<|endoftext|>",
    "0"
  ],
  "tokenizer_eos_token": [
    "<|im_end|>",
    "2"
  ],
  "tokenizer_bos_token": [
    "<|im_start|>",
    "1"
  ],
  "eot_token_id": 2,
  "max_length": 2047,
  "task_hashes": {},
  "model_source": "local-completions",
  "model_name": "vermind",
  "model_name_sanitized": "vermind",
  "system_instruction": null,
  "system_instruction_sha": null,
  "fewshot_as_multiturn": null,
  "chat_template": null,
  "chat_template_sha": null,
  "total_evaluation_time_seconds": "5404.22809829202"
}