#!/usr/bin/env python3
"""
VerMind æ¨¡å‹æ¨ç†ä¸å¯¹è¯è„šæœ¬
æ”¯æŒä» checkpoint ç›®å½•åŠ è½½æ¨¡å‹ï¼Œæ”¯æŒ chat template å’Œæµå¼è¾“å‡º
"""

import time
import argparse
import random
import warnings
import os
import sys
import glob

import torch
from transformers import AutoTokenizer, TextStreamer

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from train.utils import load_checkpoint, setup_seed

warnings.filterwarnings('ignore')


def find_latest_checkpoint(base_path):
    """
    ä»åŸºç¡€è·¯å¾„ä¸­æ‰¾åˆ°æœ€æ–°çš„ checkpoint
    
    Args:
        base_path: åŸºç¡€è·¯å¾„ï¼Œå¦‚ /root/vermind/output/pretrain/pretrain_768
    
    Returns:
        æœ€æ–°çš„ checkpoint è·¯å¾„ï¼Œå¦‚æœæ²¡æ‰¾åˆ°è¿”å› None
    """
    if not os.path.isdir(base_path):
        return None
    
    checkpoint_pattern = os.path.join(base_path, "checkpoint_*")
    checkpoints = [p for p in glob.glob(checkpoint_pattern) if os.path.isdir(p)]
    
    if checkpoints:
        checkpoints.sort(key=lambda x: int(os.path.basename(x).replace("checkpoint_", "")))
        return checkpoints[-1]
    return None


def init_model(args):
    """
    åˆå§‹åŒ–æ¨¡å‹å’Œ tokenizer
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
    
    Returns:
        model, tokenizer
    """
    model_path = args.load_from
    
    # å¦‚æœè·¯å¾„æ˜¯åŸºç¡€è·¯å¾„ï¼ˆåŒ…å« checkpoint_* å­ç›®å½•ï¼‰ï¼Œè‡ªåŠ¨æ‰¾æœ€æ–°çš„
    if os.path.isdir(model_path):
        checkpoint_pattern = os.path.join(model_path, "checkpoint_*")
        checkpoints = [p for p in glob.glob(checkpoint_pattern) if os.path.isdir(p)]
        if checkpoints:
            checkpoints.sort(key=lambda x: int(os.path.basename(x).replace("checkpoint_", "")))
            latest_checkpoint = checkpoints[-1]
            print(f"ğŸ“¦ æ‰¾åˆ° {len(checkpoints)} ä¸ª checkpointï¼Œä½¿ç”¨æœ€æ–°çš„: {os.path.basename(latest_checkpoint)}")
            model_path = latest_checkpoint
    
    # åŠ è½½æ¨¡å‹å’Œ tokenizer
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
    model, tokenizer, _ = load_checkpoint(model_path, device=args.device, load_training_state=False)
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # æ‰“å°æ¨¡å‹å‚æ•°ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°: æ€»è®¡ {total_params / 1e6:.2f}M, å¯è®­ç»ƒ {trainable_params / 1e6:.2f}M")
    
    return model.eval(), tokenizer


def main():
    parser = argparse.ArgumentParser(description="VerMind æ¨¡å‹æ¨ç†ä¸å¯¹è¯")
    parser.add_argument(
        '--load_from',
        default='/root/vermind/checkpoint_4000',
        type=str,
        help="æ¨¡å‹åŠ è½½è·¯å¾„ï¼ˆcheckpoint ç›®å½•æˆ–åŒ…å« checkpoint_* çš„åŸºç¡€è·¯å¾„ï¼‰"
    )
    parser.add_argument(
        '--max_new_tokens',
        default=2048,
        type=int,
        help="æœ€å¤§ç”Ÿæˆé•¿åº¦"
    )
    parser.add_argument(
        '--temperature',
        default=0.85,
        type=float,
        help="ç”Ÿæˆæ¸©åº¦ï¼Œæ§åˆ¶éšæœºæ€§ï¼ˆ0-1ï¼Œè¶Šå¤§è¶Šéšæœºï¼‰"
    )
    parser.add_argument(
        '--top_p',
        default=0.85,
        type=float,
        help="nucleus é‡‡æ ·é˜ˆå€¼ï¼ˆ0-1ï¼‰"
    )
    parser.add_argument(
        '--repetition_penalty',
        default=1.0,
        type=float,
        help="é‡å¤æƒ©ç½šç³»æ•°ï¼ˆ>1.0 å‡å°‘é‡å¤ï¼‰"
    )
    parser.add_argument(
        '--historys',
        default=0,
        type=int,
        help="æºå¸¦å†å²å¯¹è¯è½®æ•°ï¼ˆéœ€ä¸ºå¶æ•°ï¼Œ0è¡¨ç¤ºä¸æºå¸¦å†å²ï¼‰"
    )
    parser.add_argument(
        '--show_speed',
        default=1,
        type=int,
        choices=[0, 1],
        help="æ˜¾ç¤º decode é€Ÿåº¦ï¼ˆtokens/sï¼‰"
    )
    parser.add_argument(
        '--use_chat_template',
        default=1,
        type=int,
        choices=[0, 1],
        help="æ˜¯å¦ä½¿ç”¨ chat templateï¼ˆ1=ä½¿ç”¨ï¼Œ0=ä¸ä½¿ç”¨ï¼Œç›´æ¥æ‹¼æ¥ promptï¼‰"
    )
    parser.add_argument(
        '--device',
        default='cuda' if torch.cuda.is_available() else 'cpu',
        type=str,
        help="è¿è¡Œè®¾å¤‡"
    )
    parser.add_argument(
        '--seed',
        default=None,
        type=int,
        help="éšæœºç§å­ï¼ˆNone è¡¨ç¤ºéšæœºï¼‰"
    )
    args = parser.parse_args()
    
    # é¢„è®¾æµ‹è¯•æç¤ºè¯
    prompts = [
        'ä½ æœ‰ä»€ä¹ˆç‰¹é•¿ï¼Ÿ',
        'ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ï¼Ÿ',
        'è¯·ç”¨Pythonå†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°',
        'è§£é‡Šä¸€ä¸‹"å…‰åˆä½œç”¨"çš„åŸºæœ¬è¿‡ç¨‹',
        'å¦‚æœæ˜å¤©ä¸‹é›¨ï¼Œæˆ‘åº”è¯¥å¦‚ä½•å‡ºé—¨ï¼Ÿ',
        'æ¯”è¾ƒä¸€ä¸‹çŒ«å’Œç‹—ä½œä¸ºå® ç‰©çš„ä¼˜ç¼ºç‚¹',
        'è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ',
        'æ¨èä¸€äº›ä¸­å›½çš„ç¾é£Ÿ'
    ]
    
    # åˆå§‹åŒ–æ¨¡å‹
    conversation = []
    model, tokenizer = init_model(args)
    
    # é€‰æ‹©è¾“å…¥æ¨¡å¼
    print("\n" + "=" * 60)
    input_mode = int(input('[0] è‡ªåŠ¨æµ‹è¯•\n[1] æ‰‹åŠ¨è¾“å…¥\nè¯·é€‰æ‹©: '))
    print("=" * 60 + "\n")
    
    # è®¾ç½®æµå¼è¾“å‡º
    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
    
    # åˆ›å»ºæç¤ºè¯è¿­ä»£å™¨
    prompt_iter = prompts if input_mode == 0 else iter(lambda: input('ğŸ’¬: '), '')
    
    for prompt in prompt_iter:
        # è®¾ç½®éšæœºç§å­
        if args.seed is not None:
            setup_seed(args.seed)
        else:
            setup_seed(random.randint(0, 2048))
        
        if input_mode == 0:
            print(f'ğŸ’¬: {prompt}')
        
        # ç®¡ç†å¯¹è¯å†å²
        conversation = conversation[-args.historys:] if args.historys else []
        conversation.append({"role": "user", "content": prompt})
        
        # å‡†å¤‡è¾“å…¥
        if args.use_chat_template:
            # ä½¿ç”¨ chat template
            try:
                inputs_text = tokenizer.apply_chat_template(
                    conversation,
                    tokenize=False,
                    add_generation_prompt=True
                )
            except Exception as e:
                print(f"âš ï¸  Chat template åº”ç”¨å¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ‹¼æ¥: {e}")
                inputs_text = tokenizer.bos_token + prompt
        else:
            # ä¸ä½¿ç”¨ chat templateï¼Œç›´æ¥æ‹¼æ¥
            inputs_text = tokenizer.bos_token + prompt if hasattr(tokenizer, 'bos_token') and tokenizer.bos_token else prompt
        
        # Tokenize
        inputs = tokenizer(
            inputs_text,
            return_tensors="pt",
            truncation=True,
            max_length=model.config.max_position_embeddings if hasattr(model.config, 'max_position_embeddings') else 32768
        ).to(args.device)
        
        # ç”Ÿæˆå›å¤
        print('ğŸ¤–: ', end='', flush=True)
        st = time.time()
        
        with torch.no_grad():
            generated_ids = model.generate(
                inputs=inputs["input_ids"],
                attention_mask=inputs.get("attention_mask"),
                max_new_tokens=args.max_new_tokens,
                do_sample=True,
                streamer=streamer,
                pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                top_p=args.top_p,
                temperature=args.temperature,
                repetition_penalty=args.repetition_penalty
            )
        
        # è§£ç å›å¤
        response = tokenizer.decode(
            generated_ids[0][len(inputs["input_ids"][0]):],
            skip_special_tokens=True
        )
        
        # æ·»åŠ åˆ°å¯¹è¯å†å²
        conversation.append({"role": "assistant", "content": response})
        
        # æ˜¾ç¤ºé€Ÿåº¦
        gen_tokens = len(generated_ids[0]) - len(inputs["input_ids"][0])
        elapsed = time.time() - st
        if args.show_speed:
            print(f'\n[Speed]: {gen_tokens / elapsed:.2f} tokens/s ({gen_tokens} tokens in {elapsed:.2f}s)\n')
        else:
            print('\n')
        
        print("-" * 60 + "\n")


if __name__ == "__main__":
    main()
