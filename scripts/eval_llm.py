#!/usr/bin/env python3
"""
VerMind æ¨¡åž‹æŽ¨ç†ä¸Žå¯¹è¯è„šæœ¬
ä½¿ç”¨ OpenAI å…¼å®¹æŽ¥å£è¿›è¡Œæµ‹è¯•ï¼ˆæœ¬åœ° 8000 ç«¯å£ï¼‰
"""

import time
import argparse
import warnings

from openai import OpenAI

warnings.filterwarnings('ignore')


def main():
    parser = argparse.ArgumentParser(description="VerMind æ¨¡åž‹æŽ¨ç†ä¸Žå¯¹è¯ï¼ˆä½¿ç”¨ OpenAI æŽ¥å£ï¼‰")
    parser.add_argument(
        '--api_base',
        default='http://localhost:8000/v1',
        type=str,
        help="OpenAI API åŸºç¡€ URL"
    )
    parser.add_argument(
        '--api_key',
        default='sk-no-key-required',
        type=str,
        help="API Keyï¼ˆæœ¬åœ°æœåŠ¡é€šå¸¸ä¸éœ€è¦ï¼‰"
    )
    parser.add_argument(
        '--model',
        default='vermind',
        type=str,
        help="æ¨¡åž‹åç§°"
    )
    parser.add_argument(
        '--max_tokens',
        default=2048,
        type=int,
        help="æœ€å¤§ç”Ÿæˆé•¿åº¦"
    )
    parser.add_argument(
        '--temperature',
        default=0.85,
        type=float,
        help="ç”Ÿæˆæ¸©åº¦ï¼ŒæŽ§åˆ¶éšæœºæ€§ï¼ˆ0-1ï¼Œè¶Šå¤§è¶Šéšæœºï¼‰"
    )
    parser.add_argument(
        '--top_p',
        default=0.85,
        type=float,
        help="nucleus é‡‡æ ·é˜ˆå€¼ï¼ˆ0-1ï¼‰"
    )
    parser.add_argument(
        '--historys',
        default=0,
        type=int,
        help="æºå¸¦åŽ†å²å¯¹è¯è½®æ•°ï¼ˆéœ€ä¸ºå¶æ•°ï¼Œ0è¡¨ç¤ºä¸æºå¸¦åŽ†å²ï¼‰"
    )
    parser.add_argument(
        '--show_speed',
        default=1,
        type=int,
        choices=[0, 1],
        help="æ˜¾ç¤ºç”Ÿæˆé€Ÿåº¦ï¼ˆtokens/sï¼‰"
    )
    parser.add_argument(
        '--stream',
        default=1,
        type=int,
        choices=[0, 1],
        help="æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡ºï¼ˆ1=ä½¿ç”¨ï¼Œ0=ä¸ä½¿ç”¨ï¼‰"
    )
    args = parser.parse_args()
    
    # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
    client = OpenAI(
        api_key=args.api_key,
        base_url=args.api_base
    )
    print(f"ðŸ”— è¿žæŽ¥åˆ° API: {args.api_base}")
    print(f"ðŸ“¦ ä½¿ç”¨æ¨¡åž‹: {args.model}\n")
    
    # é¢„è®¾æµ‹è¯•æç¤ºè¯
    prompts = [
        'å†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ä»£ç ',
        'å†™ä¸€ä¸ªå¿«é€ŸæŽ’åºçš„ä»£ç ',
        'ä½ æœ‰ä»€ä¹ˆç‰¹é•¿ï¼Ÿ',
        'ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ï¼Ÿ',
        'ä¸­å›½æœ‰å“ªäº›æ¯”è¾ƒå¥½çš„å¤§å­¦',
        'ä½ çŸ¥é“å…‰é€Ÿæ˜¯å¤šå°‘å—?',
        'è§£é‡Šä¸€ä¸‹"å…‰åˆä½œç”¨"çš„åŸºæœ¬è¿‡ç¨‹',
        'å¦‚æžœæ˜Žå¤©ä¸‹é›¨ï¼Œæˆ‘åº”è¯¥å¦‚ä½•å‡ºé—¨ï¼Ÿ',
        'æ¯”è¾ƒä¸€ä¸‹çŒ«å’Œç‹—ä½œä¸ºå® ç‰©çš„ä¼˜ç¼ºç‚¹',
        'è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ',
        'æŽ¨èä¸€äº›ä¸­å›½çš„ç¾Žé£Ÿ',
        'ä½ æ˜¯è°ï¼Ÿ',
        'ä½ å«ä»€ä¹ˆåå­—',
        'ä½ æ˜¯chatgptå—ï¼Ÿ',
        'ä½ æ˜¯è°å¼€å‘çš„ï¼Ÿ'
    ]
    
    # åˆå§‹åŒ–å¯¹è¯åŽ†å²
    conversation = []
    
    # é€‰æ‹©è¾“å…¥æ¨¡å¼
    print("\n" + "=" * 60)
    input_mode = int(input('[0] è‡ªåŠ¨æµ‹è¯•\n[1] æ‰‹åŠ¨è¾“å…¥\nè¯·é€‰æ‹©: '))
    print("=" * 60 + "\n")
    
    # åˆ›å»ºæç¤ºè¯è¿­ä»£å™¨
    prompt_iter = prompts if input_mode == 0 else iter(lambda: input('ðŸ’¬: '), '')
    
    for prompt in prompt_iter:
        if input_mode == 0:
            print(f'ðŸ’¬: {prompt}')
        
        # ç®¡ç†å¯¹è¯åŽ†å²
        conversation = conversation[-args.historys:] if args.historys else []
        conversation.append({"role": "user", "content": prompt})
        
        # ç”Ÿæˆå›žå¤
        print('ðŸ¤–: ', end='', flush=True)
        st = time.time()
        response_text = ""
        gen_tokens = 0
        
        try:
            if args.stream:
                # æµå¼è¾“å‡º
                stream = client.chat.completions.create(
                    model=args.model,
                    messages=conversation,
                    max_tokens=args.max_tokens,
                    temperature=args.temperature,
                    top_p=args.top_p,
                    stream=True
                )
                
                for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        content = chunk.choices[0].delta.content
                        print(content, end='', flush=True)
                        response_text += content
                    # å°è¯•ä»Ž usage ä¸­èŽ·å– token è®¡æ•°ï¼ˆé€šå¸¸åœ¨æœ€åŽä¸€ä¸ª chunk ä¸­ï¼‰
                    if hasattr(chunk, 'usage') and chunk.usage:
                        gen_tokens = chunk.usage.completion_tokens if hasattr(chunk.usage, 'completion_tokens') else 0
            else:
                # éžæµå¼è¾“å‡º
                response = client.chat.completions.create(
                    model=args.model,
                    messages=conversation,
                    max_tokens=args.max_tokens,
                    temperature=args.temperature,
                    top_p=args.top_p,
                    stream=False
                )
                response_text = response.choices[0].message.content
                print(response_text, end='', flush=True)
                gen_tokens = response.usage.completion_tokens if hasattr(response, 'usage') and response.usage else 0
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")
            continue
        
        # æ·»åŠ åˆ°å¯¹è¯åŽ†å²
        conversation.append({"role": "assistant", "content": response_text})
        
        # æ˜¾ç¤ºé€Ÿåº¦
        elapsed = time.time() - st
        if args.show_speed and gen_tokens > 0:
            print(f'\n[Speed]: {gen_tokens / elapsed:.2f} tokens/s ({gen_tokens} tokens in {elapsed:.2f}s)\n')
        elif args.show_speed:
            print(f'\n[Time]: {elapsed:.2f}s\n')
        else:
            print('\n')
        
        print("-" * 60 + "\n")


if __name__ == "__main__":
    main()
