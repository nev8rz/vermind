#!/usr/bin/env python3
"""
VerMind-V è§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†ä¸å¯¹è¯è„šæœ¬
æ”¯æŒæœ¬åœ°æ¨ç†å’Œ vLLM API ä¸¤ç§æ¨¡å¼
"""

import os
import sys
import time
import argparse
import warnings
import base64
from pathlib import Path
from io import BytesIO

import torch
from PIL import Image

warnings.filterwarnings('ignore')


def encode_image_to_base64(image_path):
    """å°†å›¾ç‰‡ç¼–ç ä¸º base64"""
    with open(image_path, "rb") as f:
        return base64.b64encode(f.read()).decode('utf-8')


def list_images(image_dir):
    """åˆ—å‡ºè¯„ä¼°å›¾åƒç›®å½•ä¸­çš„æ‰€æœ‰å›¾ç‰‡"""
    image_dir = Path(image_dir)
    if not image_dir.exists():
        print(f"âŒ å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {image_dir}")
        return []
    
    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'}
    images = []
    
    for f in sorted(image_dir.iterdir()):
        if f.suffix.lower() in image_extensions:
            images.append(f)
    
    return images


def load_model_local(model_path, device='cuda'):
    """æœ¬åœ°åŠ è½½ VerMind-V æ¨¡å‹"""
    from transformers import AutoTokenizer
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
    from vermind_models import VerMindVLM
    
    print(f"ğŸ“¦ åŠ è½½æœ¬åœ°æ¨¡å‹: {model_path}")
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
        local_files_only=True
    )
    
    model = VerMindVLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        local_files_only=True
    )
    model = model.to(device).eval()
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {device}\n")
    return model, tokenizer


def generate_response_local(model, tokenizer, image, prompt, max_length=512, temperature=0.7, device='cuda'):
    """æœ¬åœ°ç”Ÿæˆå›å¤"""
    # æ„å»ºæ¶ˆæ¯
    messages = [
        {"role": "user", "content": f"<image>\n{prompt}"}
    ]
    
    # åº”ç”¨ chat template
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # ç¼–ç è¾“å…¥
    inputs = tokenizer(text, return_tensors="pt")
    input_ids = inputs.input_ids.to(device)
    
    # å¤„ç†å›¾åƒ
    pixel_values = model.image2tensor(image, model.processor)
    pixel_values = pixel_values.unsqueeze(0).to(device)
    
    # ç”Ÿæˆ
    with torch.no_grad():
        output = model.generate(
            input_ids=input_ids,
            pixel_values=pixel_values,
            max_new_tokens=max_length,
            temperature=temperature,
            do_sample=True,
            top_p=0.85,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    # è§£ç è¾“å‡º
    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
    
    # æå–åŠ©æ‰‹å›å¤
    if "assistant" in generated_text.lower():
        parts = generated_text.split("assistant")
        if len(parts) > 1:
            response = parts[-1].strip()
        else:
            response = generated_text[len(text):].strip()
    else:
        response = generated_text[len(text):].strip()
    
    return response


def generate_response_api(client, model, image_path, prompt, max_tokens=512, temperature=0.7):
    """é€šè¿‡ API ç”Ÿæˆå›å¤"""
    base64_image = encode_image_to_base64(image_path)
    
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{base64_image}"
                    }
                }
            ]
        }
    ]
    
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=0.85
    )
    
    return response.choices[0].message.content


def main():
    parser = argparse.ArgumentParser(description="VerMind-V VLM æ¨ç†ä¸å¯¹è¯")
    
    # æ¨¡å¼é€‰æ‹©
    parser.add_argument(
        '--use_api',
        action='store_true',
        help="ä½¿ç”¨ vLLM API æ¨¡å¼ï¼ˆé»˜è®¤ä½¿ç”¨æœ¬åœ°æ¨ç†ï¼‰"
    )
    
    # æœ¬åœ°æ¨ç†å‚æ•°
    parser.add_argument(
        '--model_path',
        type=str,
        default=None,
        help="æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°æ¨ç†æ¨¡å¼å¿…éœ€ï¼‰"
    )
    parser.add_argument(
        '--device',
        type=str,
        default='cuda' if torch.cuda.is_available() else 'cpu',
        help="æ¨ç†è®¾å¤‡ï¼ˆæœ¬åœ°æ¨¡å¼ï¼‰"
    )
    
    # API å‚æ•°
    parser.add_argument(
        '--api_base',
        default='http://localhost:8000/v1',
        type=str,
        help="OpenAI API åŸºç¡€ URL"
    )
    parser.add_argument(
        '--api_key',
        default='sk-no-key-required',
        type=str,
        help="API Key"
    )
    parser.add_argument(
        '--model',
        default='vermind-v',
        type=str,
        help="API æ¨¡å‹åç§°"
    )
    
    # é€šç”¨å‚æ•°
    parser.add_argument(
        '--image_dir',
        type=str,
        default='./dataset/eval_images',
        help="è¯„ä¼°å›¾ç‰‡ç›®å½•"
    )
    parser.add_argument(
        '--image',
        type=str,
        default=None,
        help="æŒ‡å®šå•å¼ å›¾ç‰‡è·¯å¾„"
    )
    parser.add_argument(
        '--max_length',
        type=int,
        default=512,
        help="æœ€å¤§ç”Ÿæˆé•¿åº¦"
    )
    parser.add_argument(
        '--temperature',
        type=float,
        default=0.7,
        help="ç”Ÿæˆæ¸©åº¦"
    )
    parser.add_argument(
        '--show_speed',
        default=1,
        type=int,
        choices=[0, 1],
        help="æ˜¾ç¤ºç”Ÿæˆé€Ÿåº¦"
    )
    
    args = parser.parse_args()
    
    # å‡†å¤‡å›¾ç‰‡åˆ—è¡¨
    if args.image:
        images = [Path(args.image)]
    else:
        images = list_images(args.image_dir)
    
    if not images:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡")
        return
    
    print(f"ğŸ–¼ï¸  æ‰¾åˆ° {len(images)} å¼ å›¾ç‰‡\n")
    print("=" * 60)
    for i, img_path in enumerate(images):
        print(f"[{i}] {img_path.name}")
    print("=" * 60 + "\n")
    
    # æ ¹æ®æ¨¡å¼åˆå§‹åŒ–
    if args.use_api:
        # API æ¨¡å¼
        from openai import OpenAI
        client = OpenAI(api_key=args.api_key, base_url=args.api_base)
        print(f"ğŸ”— API æ¨¡å¼: {args.api_base}")
        print(f"ğŸ“¦ æ¨¡å‹: {args.model}\n")
        generate_fn = lambda img_path, prompt: generate_response_api(
            client, args.model, img_path, prompt, args.max_length, args.temperature
        )
    else:
        # æœ¬åœ°æ¨¡å¼
        if not args.model_path:
            print("âŒ æœ¬åœ°æ¨ç†æ¨¡å¼éœ€è¦æŒ‡å®š --model_path")
            return
        
        model, tokenizer = load_model_local(args.model_path, args.device)
        print(f"ğŸ“ æœ¬åœ°æ¨¡å¼: {args.model_path}\n")
        
        def generate_fn(img_path, prompt):
            image = Image.open(img_path).convert('RGB')
            return generate_response_local(
                model, tokenizer, image, prompt,
                args.max_length, args.temperature, args.device
            )
    
    # äº¤äº’æ¨¡å¼
    while True:
        try:
            choice = input(f"é€‰æ‹©å›¾ç‰‡ [0-{len(images)-1}] æˆ– 'q' é€€å‡º: ").strip()
            if choice.lower() == 'q':
                break
            
            try:
                img_idx = int(choice)
                if img_idx < 0 or img_idx >= len(images):
                    print(f"âŒ æ— æ•ˆé€‰æ‹©")
                    continue
            except ValueError:
                print("âŒ æ— æ•ˆè¾“å…¥")
                continue
            
            image_path = images[img_idx]
            print(f"\nğŸ“· å›¾ç‰‡: {image_path.name}")
            
            # å¯¹è¯å¾ªç¯
            print("\nğŸ’¡ æç¤º: è¾“å…¥é—®é¢˜å¼€å§‹å¯¹è¯ï¼Œè¾“å…¥ 'next' åˆ‡æ¢å›¾ç‰‡ï¼Œè¾“å…¥ 'exit' é€€å‡º\n")
            
            while True:
                prompt = input('ğŸ’¬: ').strip()
                
                if prompt.lower() == 'exit':
                    return
                if prompt.lower() == 'next':
                    break
                if not prompt:
                    continue
                
                # è‡ªåŠ¨æµ‹è¯•
                if prompt == 'test':
                    test_prompts = [
                        'æè¿°è¿™å¼ å›¾ç‰‡',
                        'è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ',
                        'è¯·è¯¦ç»†æè¿°å›¾ç‰‡ä¸­çš„å†…å®¹',
                        'è¿™å¼ å›¾ç‰‡çš„ä¸»è¦å…ƒç´ æ˜¯ä»€ä¹ˆï¼Ÿ'
                    ]
                    print(f"\nğŸ“ è‡ªåŠ¨æµ‹è¯• {len(test_prompts)} ä¸ªæç¤º...\n")
                    for i, test_prompt in enumerate(test_prompts):
                        print(f"[{i+1}/{len(test_prompts)}] ğŸ’¬: {test_prompt}")
                        print('ğŸ¤–: ', end='', flush=True)
                        
                        try:
                            st = time.time()
                            response = generate_fn(image_path, test_prompt)
                            elapsed = time.time() - st
                            
                            print(response)
                            if args.show_speed:
                                print(f'\n[Time]: {elapsed:.2f}s')
                        except Exception as e:
                            print(f"âŒ é”™è¯¯: {e}")
                        
                        print("-" * 40 + "\n")
                    continue
                
                # ç”Ÿæˆå›å¤
                print('ğŸ¤–: ', end='', flush=True)
                st = time.time()
                
                try:
                    response = generate_fn(image_path, prompt)
                    print(response)
                    
                    elapsed = time.time() - st
                    if args.show_speed:
                        print(f'\n[Time]: {elapsed:.2f}s')
                    
                    print("\n" + "-" * 60 + "\n")
                    
                except Exception as e:
                    print(f"\nâŒ ç”Ÿæˆé”™è¯¯: {e}")
                    import traceback
                    traceback.print_exc()
        
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§!")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")


if __name__ == "__main__":
    main()
