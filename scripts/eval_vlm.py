#!/usr/bin/env python3
"""
VerMind-V è§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†ä¸å¯¹è¯è„šæœ¬
æ”¯æŒæœ¬åœ°æ¨ç†å’Œ vLLM API ä¸¤ç§æ¨¡å¼ï¼Œæ”¯æŒæµå¼è¾“å‡º
"""

import os
import sys
import time
import argparse
import warnings
import base64
from pathlib import Path
from threading import Thread

import torch
from PIL import Image

warnings.filterwarnings('ignore')


def encode_image_to_base64(image_path):
    """å°†å›¾ç‰‡ç¼–ç ä¸º base64"""
    with open(image_path, "rb") as f:
        return base64.b64encode(f.read()).decode('utf-8')


def list_images(image_dir):
    """åˆ—å‡ºè¯„ä¼°å›¾åƒç›®å½•ä¸­çš„æ‰€æœ‰å›¾ç‰‡"""
    image_dir = Path(image_dir)
    if not image_dir.exists():
        print(f"âŒ å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {image_dir}")
        return []
    
    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'}
    images = []
    
    for f in sorted(image_dir.iterdir()):
        if f.suffix.lower() in image_extensions:
            images.append(f)
    
    return images


def load_model_local(model_path, device='cuda'):
    """æœ¬åœ°åŠ è½½ VerMind-V æ¨¡å‹"""
    from transformers import AutoTokenizer
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
    from vermind_models import VerMindVLM
    
    print(f"ğŸ“¦ åŠ è½½æœ¬åœ°æ¨¡å‹: {model_path}")
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
        local_files_only=True
    )
    
    model = VerMindVLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        local_files_only=True
    )
    model = model.to(device).eval()
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {device}\n")
    return model, tokenizer


def insert_image_tokens(input_ids, image_token_ids, image_ids):
    """å°† <image> çš„ tokenizer è¾“å‡ºæ›¿æ¢ä¸º image_ids"""
    new_input_ids = []
    i = 0
    while i < len(input_ids):
        if input_ids[i:i+len(image_token_ids)] == image_token_ids:
            new_input_ids.extend(image_ids)
            i += len(image_token_ids)
        else:
            new_input_ids.append(input_ids[i])
            i += 1
    return new_input_ids


def generate_response_local(model, tokenizer, image, prompt, max_length=512, temperature=0.7, device='cuda', stream=False):
    """æœ¬åœ°ç”Ÿæˆå›å¤ï¼Œæ”¯æŒæµå¼è¾“å‡º"""

    messages = [
        {"role": "user", "content": f"<image>\n{prompt}"}
    ]
    

    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    

    inputs = tokenizer(text, return_tensors="pt")
    input_ids_list = inputs.input_ids[0].tolist()
    

    image_token_ids = tokenizer("<image>", add_special_tokens=False).input_ids
    image_ids = model.params.image_ids
    input_ids_list = insert_image_tokens(input_ids_list, image_token_ids, image_ids)
    

    input_ids = torch.tensor([input_ids_list], dtype=torch.long).to(device)
    

    pixel_values = model.image2tensor(image, model.processor)
    pixel_values = pixel_values.unsqueeze(0).to(device)
    
    if stream:

        from transformers import TextIteratorStreamer
        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        
        generation_kwargs = {
            'input_ids': input_ids,
            'pixel_values': pixel_values,
            'max_new_tokens': max_length,
            'temperature': temperature,
            'do_sample': True,
            'top_p': 0.85,
            'pad_token_id': tokenizer.pad_token_id,
            'eos_token_id': tokenizer.eos_token_id,
            'streamer': streamer
        }
        

        thread = Thread(target=model.generate, kwargs=generation_kwargs)
        thread.start()
        

        generated_text = ""
        for new_text in streamer:
            generated_text += new_text
            yield new_text
        
        thread.join()
    else:

        with torch.no_grad():
            output = model.generate(
                input_ids=input_ids,
                pixel_values=pixel_values,
                max_new_tokens=max_length,
                temperature=temperature,
                do_sample=True,
                top_p=0.85,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        

        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
        

        if "assistant" in generated_text.lower():
            parts = generated_text.split("assistant")
            if len(parts) > 1:
                response = parts[-1].strip()
            else:
                response = generated_text[len(text):].strip()
        else:
            response = generated_text[len(text):].strip()
        
        yield response


def generate_response_api(client, model, image_path, prompt, max_tokens=512, temperature=0.7, stream=False):
    """é€šè¿‡ API ç”Ÿæˆå›å¤ï¼Œæ”¯æŒæµå¼è¾“å‡º"""
    base64_image = encode_image_to_base64(image_path)
    
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{base64_image}"
                    }
                }
            ]
        }
    ]
    
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=0.85,
        stream=stream
    )
    
    if stream:

        for chunk in response:
            if chunk.choices and chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
    else:
        yield response.choices[0].message.content


def main():
    parser = argparse.ArgumentParser(description="VerMind-V VLM æ¨ç†ä¸å¯¹è¯")
    

    parser.add_argument(
        '--use_api',
        action='store_true',
        help="ä½¿ç”¨ vLLM API æ¨¡å¼ï¼ˆé»˜è®¤ä½¿ç”¨æœ¬åœ°æ¨ç†ï¼‰"
    )
    

    parser.add_argument(
        '--model_path',
        type=str,
        default=None,
        help="æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°æ¨ç†æ¨¡å¼å¿…éœ€ï¼‰"
    )
    parser.add_argument(
        '--device',
        type=str,
        default='cuda' if torch.cuda.is_available() else 'cpu',
        help="æ¨ç†è®¾å¤‡ï¼ˆæœ¬åœ°æ¨¡å¼ï¼‰"
    )
    

    parser.add_argument(
        '--api_base',
        default='http://localhost:8000/v1',
        type=str,
        help="OpenAI API åŸºç¡€ URL"
    )
    parser.add_argument(
        '--api_key',
        default='sk-no-key-required',
        type=str,
        help="API Key"
    )
    parser.add_argument(
        '--model',
        default='vermind-v',
        type=str,
        help="API æ¨¡å‹åç§°"
    )
    

    parser.add_argument(
        '--image_dir',
        type=str,
        default='./dataset/eval_images',
        help="è¯„ä¼°å›¾ç‰‡ç›®å½•"
    )
    parser.add_argument(
        '--image',
        type=str,
        default=None,
        help="æŒ‡å®šå•å¼ å›¾ç‰‡è·¯å¾„"
    )
    parser.add_argument(
        '--max_length',
        type=int,
        default=512,
        help="æœ€å¤§ç”Ÿæˆé•¿åº¦"
    )
    parser.add_argument(
        '--temperature',
        type=float,
        default=0.7,
        help="ç”Ÿæˆæ¸©åº¦"
    )
    parser.add_argument(
        '--show_speed',
        default=1,
        type=int,
        choices=[0, 1],
        help="æ˜¾ç¤ºç”Ÿæˆé€Ÿåº¦"
    )
    parser.add_argument(
        '--stream',
        action='store_true',
        help="å¯ç”¨æµå¼è¾“å‡º"
    )
    
    args = parser.parse_args()
    

    if args.image:
        images = [Path(args.image)]
    else:
        images = list_images(args.image_dir)
    
    if not images:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡")
        return
    
    print(f"ğŸ–¼ï¸  æ‰¾åˆ° {len(images)} å¼ å›¾ç‰‡\n")
    print("=" * 60)
    for i, img_path in enumerate(images):
        print(f"[{i}] {img_path.name}")
    print("=" * 60 + "\n")
    

    if args.use_api:

        from openai import OpenAI
        client = OpenAI(api_key=args.api_key, base_url=args.api_base)
        print(f"ğŸ”— API æ¨¡å¼: {args.api_base}")
        print(f"ğŸ“¦ æ¨¡å‹: {args.model}")
        print(f"ğŸŒŠ æµå¼è¾“å‡º: {'å¼€å¯' if args.stream else 'å…³é—­'}\n")
        
        def generate_fn(img_path, prompt):
            for chunk in generate_response_api(
                client, args.model, img_path, prompt, args.max_length, args.temperature, args.stream
            ):
                yield chunk
    else:

        if not args.model_path:
            print("âŒ æœ¬åœ°æ¨ç†æ¨¡å¼éœ€è¦æŒ‡å®š --model_path")
            return
        
        model, tokenizer = load_model_local(args.model_path, args.device)
        print(f"ğŸ“ æœ¬åœ°æ¨¡å¼: {args.model_path}")
        print(f"ğŸŒŠ æµå¼è¾“å‡º: {'å¼€å¯' if args.stream else 'å…³é—­'}\n")
        
        def generate_fn(img_path, prompt):
            image = Image.open(img_path).convert('RGB')
            for chunk in generate_response_local(
                model, tokenizer, image, prompt,
                args.max_length, args.temperature, args.device, args.stream
            ):
                yield chunk
    

    while True:
        try:
            choice = input(f"é€‰æ‹©å›¾ç‰‡ [0-{len(images)-1}] æˆ– 'q' é€€å‡º: ").strip()
            if choice.lower() == 'q':
                break
            
            try:
                img_idx = int(choice)
                if img_idx < 0 or img_idx >= len(images):
                    print(f"âŒ æ— æ•ˆé€‰æ‹©")
                    continue
            except ValueError:
                print("âŒ æ— æ•ˆè¾“å…¥")
                continue
            
            image_path = images[img_idx]
            print(f"\nğŸ“· å›¾ç‰‡: {image_path.name}")
            

            print("\nğŸ’¡ æç¤º: è¾“å…¥é—®é¢˜å¼€å§‹å¯¹è¯ï¼Œè¾“å…¥ 'next' åˆ‡æ¢å›¾ç‰‡ï¼Œè¾“å…¥ 'exit' é€€å‡º\n")
            
            while True:
                prompt = input('ğŸ’¬: ').strip()
                
                if prompt.lower() == 'exit':
                    return
                if prompt.lower() == 'next':
                    break
                if not prompt:
                    continue
                

                if prompt == 'test':
                    test_prompts = [
                        'æè¿°è¿™å¼ å›¾ç‰‡',
                        'è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ',
                        'è¯·è¯¦ç»†æè¿°å›¾ç‰‡ä¸­çš„å†…å®¹',
                        'è¿™å¼ å›¾ç‰‡çš„ä¸»è¦å…ƒç´ æ˜¯ä»€ä¹ˆï¼Ÿ'
                    ]
                    print(f"\nğŸ“ è‡ªåŠ¨æµ‹è¯• {len(test_prompts)} ä¸ªæç¤º...\n")
                    for i, test_prompt in enumerate(test_prompts):
                        print(f"[{i+1}/{len(test_prompts)}] ğŸ’¬: {test_prompt}")
                        print('ğŸ¤–: ', end='', flush=True)
                        
                        try:
                            st = time.time()
                            full_response = ""
                            for chunk in generate_fn(image_path, test_prompt):
                                print(chunk, end='', flush=True)
                                full_response += chunk
                            elapsed = time.time() - st
                            
                            if args.show_speed:
                                print(f'\n[Time]: {elapsed:.2f}s')
                        except Exception as e:
                            print(f"âŒ é”™è¯¯: {e}")
                        
                        print("-" * 40 + "\n")
                    continue
                

                print('ğŸ¤–: ', end='', flush=True)
                st = time.time()
                full_response = ""
                
                try:
                    for chunk in generate_fn(image_path, prompt):
                        print(chunk, end='', flush=True)
                        full_response += chunk
                    
                    elapsed = time.time() - st
                    if args.show_speed:
                        print(f'\n[Time]: {elapsed:.2f}s')
                    
                    print("\n" + "-" * 60 + "\n")
                    
                except Exception as e:
                    print(f"\nâŒ ç”Ÿæˆé”™è¯¯: {e}")
                    import traceback
                    traceback.print_exc()
        
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§!")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")


if __name__ == "__main__":
    main()
