#!/usr/bin/env python3
"""
ä¸Šä¼  VerMind æˆ– VerMind-V æ¨¡å‹åˆ° HuggingFace
è‡ªåŠ¨ä» vllm_adapter å¤åˆ¶å®Œæ•´çš„æ¨¡å‹å®šä¹‰æ–‡ä»¶
"""

import os
import shutil
import argparse
import tempfile
from pathlib import Path
from typing import Optional

PROJECT_ROOT = Path(__file__).resolve().parents[1]


def copy_model_files(model_path: str, temp_dir: str, model_type: str):
    """å¤åˆ¶æ¨¡å‹æ‰€éœ€çš„æ‰€æœ‰æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•"""
    model_path = Path(model_path)
    temp_path = Path(temp_dir)
    
    print(f"ğŸ“¦ æ­£åœ¨æ”¶é›†æ–‡ä»¶ä»: {model_path}")
    print(f"   æ¨¡å‹ç±»å‹: {model_type.upper()}")
    

    try:
        from safetensors.torch import load_file
        state_dict = load_file(model_path / "model.safetensors")
        
        llm_keys = [k for k in state_dict.keys() if k.startswith('model.') or k == 'lm_head.weight']
        vision_proj_keys = [k for k in state_dict.keys() if k.startswith('vision_proj.')]
        vision_encoder_keys = [k for k in state_dict.keys() if k.startswith('vision_encoder.')]
        
        print(f"\nğŸ“Š æ¨¡å‹æƒé‡ç»Ÿè®¡:")
        print(f"  - LLM: {len(llm_keys)} keys")
        
        if vision_proj_keys:
            print(f"  - Vision Projection: {len(vision_proj_keys)} keys")
        if vision_encoder_keys:
            print(f"  - Vision Encoder (SigLIP): {len(vision_encoder_keys)} keys")
        
        if model_type == 'vlm' and len(vision_encoder_keys) == 0:
            print(f"\nâš ï¸  è­¦å‘Š: VLM æ¨¡å‹ä¸­æ²¡æœ‰ Vision Encoder æƒé‡!")
            return False
        elif model_type == 'llm' and len(vision_encoder_keys) > 0:
            print(f"\nâ„¹ï¸  æ³¨æ„: LLM æ¨¡å‹ä¸­åŒ…å« Vision Encoder æƒé‡ (å…± {len(vision_encoder_keys)} keys)")
        else:
            print(f"\nâœ… æ¨¡å‹æƒé‡éªŒè¯é€šè¿‡")
    except Exception as e:
        print(f"\nâš ï¸  æ— æ³•éªŒè¯æƒé‡: {e}")
    

    print(f"\nğŸ“‹ å¤åˆ¶ checkpoint æ–‡ä»¶...")
    required_files = [
        "model.safetensors",
        "config.json",
        "tokenizer.json",
        "tokenizer_config.json",
        "special_tokens_map.json",
        "generation_config.json",
        "chat_template.jinja",
    ]
    
    for file_name in required_files:
        src = model_path / file_name
        if src.exists():
            shutil.copy2(src, temp_path / file_name)
            print(f"  âœ… {file_name}")
        else:
            print(f"  âš ï¸  è·³è¿‡ (ä¸å­˜åœ¨): {file_name}")
    

    config_path = temp_path / "config.json"
    if config_path.exists():
        import json
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        if model_type == 'vlm':

            print(f"\nğŸ“‹ æ³¨å…¥ VLM auto_map åˆ° config.json...")
            config["auto_map"] = {
                "AutoConfig": "configuration_vermind_v.VLMConfig",
                "AutoModelForCausalLM": "modeling_vermind_v.VerMindVLM"
            }
            print(f"  âœ… auto_map å·²æ³¨å…¥")
            

            print(f"\nğŸ“‹ å¤åˆ¶ VLM æ¨¡å‹å®šä¹‰æ–‡ä»¶...")
            vlm_files = [
                ("vllm_adapter/vlm/configuration_vermind_v.py", "configuration_vermind_v.py"),
                ("vllm_adapter/vlm/modeling_vermind_v.py", "modeling_vermind_v.py"),
            ]
            
            for src_rel, dst_name in vlm_files:
                src = PROJECT_ROOT / src_rel
                if src.exists():
                    shutil.copy2(src, temp_path / dst_name)
                    print(f"  âœ… {dst_name}")
                else:
                    print(f"  âŒ ç¼ºå¤±å…³é”®æ–‡ä»¶: {src_rel}")
                    return False
        else:

            print(f"\nğŸ“‹ å¤åˆ¶ LLM æ¨¡å‹å®šä¹‰æ–‡ä»¶...")
            base_files = [
                ("vllm_adapter/core/configuration_vermind.py", "configuration_vermind.py"),
                ("vllm_adapter/core/modeling_vermind.py", "modeling_vermind.py"),
            ]
            
            for src_rel, dst_name in base_files:
                src = PROJECT_ROOT / src_rel
                if src.exists():
                    shutil.copy2(src, temp_path / dst_name)
                    print(f"  âœ… {dst_name}")
                else:
                    print(f"  âŒ ç¼ºå¤±å…³é”®æ–‡ä»¶: {src_rel}")
                    return False
        

        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        print(f"  âœ… config.json å·²æ›´æ–°")
    
    print(f"\nğŸ“ ä¸´æ—¶ç›®å½•å‡†å¤‡å®Œæˆ: {temp_dir}")
    return True


def delete_remote_files(repo_id: str, files: list, token: Optional[str] = None, 
                        commit_message: Optional[str] = None):
    """åˆ é™¤ HuggingFace è¿œç¨‹ä»“åº“ä¸­çš„æ–‡ä»¶"""
    try:
        from huggingface_hub import HfApi
    except ImportError:
        print("é”™è¯¯: è¯·å…ˆå®‰è£… huggingface_hub: pip install huggingface_hub")
        return False
    
    api = HfApi(token=token)
    
    try:
        print(f"\nğŸ—‘ï¸  æ­£åœ¨åˆ é™¤è¿œç¨‹æ–‡ä»¶...")
        for file_path in files:
            try:
                api.delete_file(
                    path_in_repo=file_path,
                    repo_id=repo_id,
                    token=token,
                    commit_message=commit_message or f"Delete {file_path}"
                )
                print(f"  âœ… å·²åˆ é™¤: {file_path}")
            except Exception as e:
                if "404" in str(e):
                    print(f"  âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                else:
                    print(f"  âŒ åˆ é™¤å¤±è´¥ {file_path}: {e}")
        return True
    except Exception as e:
        print(f"âŒ æ“ä½œå¤±è´¥: {e}")
        return False


def upload_to_hf(repo_id: str, local_path: str, token: Optional[str] = None, 
                 private: bool = False, commit_message: Optional[str] = None):
    """ä¸Šä¼ åˆ° HuggingFace"""
    try:
        from huggingface_hub import HfApi, upload_folder
    except ImportError:
        print("é”™è¯¯: è¯·å…ˆå®‰è£… huggingface_hub: pip install huggingface_hub")
        return False


    api = HfApi(token=token)
    local_path = Path(local_path)


    try:
        api.create_repo(repo_id=repo_id, private=private, exist_ok=True)
        print(f"âœ… ä»“åº“å°±ç»ª: https://huggingface.co/{repo_id}")
    except Exception as e:
        print(f"âš ï¸  åˆ›å»ºä»“åº“å¤±è´¥: {e}")

        if "401" in str(e) or "Unauthorized" in str(e):
            print(f"\nğŸ’¡ æç¤º: è¯·ç¡®ä¿å·²ç™»å½• huggingface-cli:")
            print(f"   huggingface-cli login")
            print(f"   æˆ–æä¾› --token å‚æ•°")
        return False


    try:
        print(f"\nğŸ“¤ æ­£åœ¨ä¸Šä¼ æ–‡ä»¶...")
        upload_folder(
            folder_path=str(local_path),
            repo_id=repo_id,
            token=token,
            commit_message=commit_message or "Upload VerMind model",
            ignore_patterns=[".git", "__pycache__", "*.pyc", ".DS_Store", "training_state.pt"],
        )
        print(f"âœ… ä¸Šä¼ æˆåŠŸ!")
        print(f"ğŸŒ æ¨¡å‹åœ°å€: https://huggingface.co/{repo_id}")
        return True
    except Exception as e:
        print(f"âŒ ä¸Šä¼ å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    parser = argparse.ArgumentParser(
        description="ä¸Šä¼  VerMind/VerMind-V æ¨¡å‹åˆ° HuggingFace",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:

  python scripts/upload_hf.py \
      --model_path /root/vermind/output/dpo/dpo_768/checkpoint_1610 \
      --repo_id your_username/vermind-dpo \
      --model_type llm


  python scripts/upload_hf.py \
      --model_path /root/vermind/output/vlm_sft/vlm_sft_768/checkpoint_29753 \
      --repo_id your_username/vermind-v-sft \
      --model_type vlm
        """
    )
    
    parser.add_argument("--model_path", type=str, required=True, help="æ¨¡å‹ checkpoint è·¯å¾„")
    parser.add_argument("--repo_id", type=str, required=True, help="HuggingFace ä»“åº“ ID (æ ¼å¼: namespace/model_name)")
    parser.add_argument("--model_type", type=str, choices=["llm", "vlm"], required=True, help="æ¨¡å‹ç±»å‹")
    parser.add_argument("--token", type=str, default=None, help="HuggingFace API Token (é»˜è®¤ä» HF_TOKEN ç¯å¢ƒå˜é‡è¯»å–)")
    parser.add_argument("--private", action="store_true", help="åˆ›å»ºç§æœ‰ä»“åº“")
    parser.add_argument("--commit_message", type=str, default=None, help="æäº¤ä¿¡æ¯")

    args = parser.parse_args()


    model_path = Path(args.model_path)
    if not model_path.exists():
        print(f"âŒ é”™è¯¯: è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return 1
    
    if not (model_path / "model.safetensors").exists():
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ° model.safetensors")
        return 1


    token = args.token or os.getenv("HF_TOKEN") or os.getenv("HUGGINGFACE_TOKEN")
    if not token:
        print("â„¹ï¸  æœªæä¾› tokenï¼Œå°è¯•ä½¿ç”¨ huggingface-cli å·²ç™»å½•çš„å‡­è¯..."
    )


    with tempfile.TemporaryDirectory() as temp_dir:
        print("=" * 60)
        print("ğŸ“‹ æ­¥éª¤ 1/2: æ”¶é›†æ¨¡å‹æ–‡ä»¶")
        print("=" * 60)
        
        if not copy_model_files(args.model_path, temp_dir, args.model_type):
            print("âŒ æ–‡ä»¶æ”¶é›†å¤±è´¥")
            return 1
        
        print("\n" + "=" * 60)
        print("ğŸ“¤ æ­¥éª¤ 2/2: ä¸Šä¼ åˆ° HuggingFace")
        print("=" * 60)
        
        success = upload_to_hf(
            repo_id=args.repo_id,
            local_path=temp_dir,
            token=token,
            private=args.private,
            commit_message=args.commit_message,
        )
        
        if success:
            print("\n" + "=" * 60)
            print("ğŸ‰ ä¸Šä¼ å®Œæˆ!")
            print("=" * 60)
            print(f"\nåŠ è½½æ–¹å¼:")
            if args.model_type == 'vlm':
                print(f"```python")
                print(f"from transformers import AutoModelForCausalLM, AutoTokenizer")
                print(f"model = AutoModelForCausalLM.from_pretrained('{args.repo_id}', trust_remote_code=True)")
                print(f"tokenizer = AutoTokenizer.from_pretrained('{args.repo_id}', trust_remote_code=True)")
                print(f"```")
            else:
                print(f"```python")
                print(f"from transformers import AutoModelForCausalLM, AutoTokenizer")
                print(f"model = AutoModelForCausalLM.from_pretrained('{args.repo_id}', trust_remote_code=True)")
                print(f"tokenizer = AutoTokenizer.from_pretrained('{args.repo_id}', trust_remote_code=True)")
                print(f"```")
            return 0
        else:
            return 1


if __name__ == "__main__":
    exit(main())
