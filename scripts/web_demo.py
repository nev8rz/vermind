#!/usr/bin/env python3
"""
VerMind-V Web Demo
åŸºäº Gradio çš„è§†è§‰è¯­è¨€æ¨¡å‹äº¤äº’ç•Œé¢
"""

import os
import sys
import argparse
import warnings
import base64
import inspect
from pathlib import Path
from threading import Thread
from queue import Queue

import torch
from PIL import Image
from transformers import AutoTokenizer, TextIteratorStreamer

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
from vermind_models import VerMindVLM, VLMConfig

warnings.filterwarnings('ignore')

# å…¨å±€å˜é‡
model = None
tokenizer = None
preprocess = None
vision_model = None
lm_config = None
args = None


def init_model(model_path, device='cuda'):
    """åˆå§‹åŒ– VerMind-V æ¨¡å‹"""
    global model, tokenizer, preprocess, vision_model, lm_config
    
    print(f"ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
    
    # åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
        local_files_only=True
    )
    
    # åŠ è½½æ¨¡å‹
    model = VerMindVLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        local_files_only=True
    )
    model = model.to(device).eval()
    
    # è·å–è§†è§‰æ¨¡å‹å’Œé¢„å¤„ç†å™¨
    vision_model = model.vision_encoder
    preprocess = model.processor
    
    # è·å–é…ç½®
    lm_config = model.params
    
    # ç»Ÿè®¡å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"ğŸ“Š å¯è®­ç»ƒå‚æ•°é‡: {total_params / 1e6:.2f}M")
    print(f"ğŸ–¥ï¸  è®¾å¤‡: {device}\n")
    
    return model, tokenizer, vision_model, preprocess


def insert_image_tokens(input_ids, image_token_ids, image_ids):
    """å°† <image> çš„ tokenizer è¾“å‡ºæ›¿æ¢ä¸º image_ids"""
    new_input_ids = []
    i = 0
    while i < len(input_ids):
        if input_ids[i:i+len(image_token_ids)] == image_token_ids:
            new_input_ids.extend(image_ids)
            i += len(image_token_ids)
        else:
            new_input_ids.append(input_ids[i])
            i += 1
    return new_input_ids


def generate_response(image, prompt, temperature=0.7, top_p=0.85, max_new_tokens=512):
    """ç”Ÿæˆå›å¤ï¼Œæ”¯æŒæµå¼è¾“å‡º"""
    global model, tokenizer, preprocess, vision_model, lm_config, args
    
    device = args.device
    
    # æ„å»ºæ¶ˆæ¯
    messages = [
        {"role": "user", "content": f"<image>\n{prompt}"}
    ]
    
    # åº”ç”¨ chat template
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # ç¼–ç è¾“å…¥
    inputs = tokenizer(text, return_tensors="pt")
    input_ids_list = inputs.input_ids[0].tolist()
    
    # è·å– <image> çš„ token ids å¹¶æ›¿æ¢ä¸º image_ids
    image_token_ids = tokenizer("<image>", add_special_tokens=False).input_ids
    image_ids = lm_config.image_ids
    input_ids_list = insert_image_tokens(input_ids_list, image_token_ids, image_ids)
    
    # è½¬å› tensor
    input_ids = torch.tensor([input_ids_list], dtype=torch.long).to(device)
    
    # å¤„ç†å›¾åƒ
    if image is not None:
        image_pil = Image.open(image).convert('RGB')
        pixel_values = VerMindVLM.image2tensor(image_pil, preprocess)
        pixel_values = pixel_values.unsqueeze(0).to(device)
    else:
        pixel_values = None
    
    # åˆ›å»ºæµå¼ç”Ÿæˆå™¨
    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
    
    generation_kwargs = {
        'input_ids': input_ids,
        'pixel_values': pixel_values,
        'max_new_tokens': max_new_tokens,
        'temperature': temperature,
        'do_sample': True,
        'top_p': top_p,
        'pad_token_id': tokenizer.pad_token_id,
        'eos_token_id': tokenizer.eos_token_id,
        'streamer': streamer
    }
    
    # åœ¨æ–°çº¿ç¨‹ä¸­ç”Ÿæˆ
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()
    
    # æµå¼è¾“å‡º
    for new_text in streamer:
        yield new_text
    
    thread.join()


def create_demo():
    """åˆ›å»º Gradio ç•Œé¢"""
    
    # è¯»å– SVG logoï¼ˆä¼˜å…ˆä½¿ç”¨å½©è‰²ç‰ˆæœ¬ï¼‰
    logo_path = Path(__file__).parent.parent / "docs" / "assets" / "vermind_logo_color.svg"
    if not logo_path.exists():
        logo_path = Path(__file__).parent.parent / "docs" / "assets" / "vermind_logo.svg"
    logo_html = ''
    if logo_path.exists():
        try:
            with open(logo_path, 'r', encoding='utf-8') as f:
                logo_svg = f.read()
            # æå– SVG å†…å®¹ç”¨äºå†…åµŒæ˜¾ç¤º
            import re
            svg_match = re.search(r'(<svg.*?</svg>)', logo_svg, re.DOTALL)
            if svg_match:
                logo_html = svg_match.group(1)
                # è°ƒæ•´å¤§å°
                logo_html = re.sub(r'width="[^"]*"', 'width="60"', logo_html)
                logo_html = re.sub(r'height="[^"]*"', 'height="60"', logo_html)
        except Exception as e:
            print(f"âš ï¸  Logo åŠ è½½å¤±è´¥: {e}")
            logo_html = ''
    
    import gradio as gr
    
    with gr.Blocks(title="VerMind-V", css="""
        .container { max-width: 1200px; margin: 0 auto; }
        .header { text-align: center; margin-bottom: 20px; }
        .logo-container { display: flex; align-items: center; justify-content: center; gap: 15px; }
        .chat-container { height: 600px; }
        .input-container { margin-top: 10px; }
    """) as demo:
        
        # æ ‡é¢˜åŒºåŸŸ
        gr.HTML(f"""
            <div class="header">
                <div class="logo-container">
                    {logo_html}
                    <span style="font-size: 36px; font-weight: bold; 
                                 background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                                 -webkit-background-clip: text; -webkit-text-fill-color: transparent;">
                        VerMind-V
                    </span>
                </div>
                <p style="color: #666; margin-top: 10px;">å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹å¯¹è¯ç³»ç»Ÿ</p>
            </div>
        """)
        
        # çŠ¶æ€å˜é‡
        current_image = gr.State(value=None)
        
        with gr.Row():
            # å·¦ä¾§ï¼šå›¾ç‰‡ä¸Šä¼ å’Œå‚æ•°è®¾ç½®
            with gr.Column(scale=3):
                with gr.Blocks():
                    # å›¾ç‰‡ä¸Šä¼ 
                    image_input = gr.Image(
                        type="filepath",
                        label="ğŸ“· ä¸Šä¼ å›¾ç‰‡",
                        height=400
                    )
                    
                    # å‚æ•°è®¾ç½®
                    with gr.Group():
                        gr.Markdown("### âš™ï¸ ç”Ÿæˆå‚æ•°")
                        temperature_slider = gr.Slider(
                            label="Temperature",
                            minimum=0.1,
                            maximum=1.5,
                            value=0.7,
                            step=0.1,
                            info="æ§åˆ¶ç”Ÿæˆéšæœºæ€§ï¼Œè¶Šå¤§è¶Šéšæœº"
                        )
                        top_p_slider = gr.Slider(
                            label="Top-P",
                            minimum=0.1,
                            maximum=1.0,
                            value=0.85,
                            step=0.05,
                            info="Nucleus é‡‡æ ·é˜ˆå€¼"
                        )
                        max_tokens_slider = gr.Slider(
                            label="Max New Tokens",
                            minimum=64,
                            maximum=2048,
                            value=512,
                            step=64,
                            info="æœ€å¤§ç”Ÿæˆé•¿åº¦"
                        )
                    
                    # æ¸…ç©ºæŒ‰é’®
                    clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", variant="secondary")
            
            # å³ä¾§ï¼šå¯¹è¯åŒºåŸŸ
            with gr.Column(scale=5):
                chatbot_kwargs = {
                    "label": "ğŸ’¬ å¯¹è¯",
                    "height": 550,
                    "avatar_images": (None, None),
                }
                # gradio ç‰ˆæœ¬å…¼å®¹ï¼šæ—§ç‰ˆä¸æ”¯æŒ bubble_full_width
                if "bubble_full_width" in inspect.signature(gr.Chatbot.__init__).parameters:
                    chatbot_kwargs["bubble_full_width"] = False
                # gradio æ–°ç‰ˆé»˜è®¤ messagesï¼Œæ˜¾å¼ä½¿ç”¨ tuples å…¼å®¹æ—§æ ¼å¼
                if "type" in inspect.signature(gr.Chatbot.__init__).parameters:
                    chatbot_kwargs["type"] = "tuples"
                chatbot = gr.Chatbot(**chatbot_kwargs)
                chatbot_format = getattr(chatbot, "type", None) or "tuples"
                
                with gr.Row():
                    msg_input = gr.Textbox(
                        placeholder="è¯·è¾“å…¥ä½ çš„é—®é¢˜...",
                        show_label=False,
                        container=False,
                        scale=8
                    )
                    submit_btn = gr.Button("å‘é€", variant="primary", scale=1)
                
                # ç¤ºä¾‹é—®é¢˜
                gr.Examples(
                    examples=[
                        "æè¿°ä¸€ä¸‹è¿™å¼ å›¾ç‰‡çš„å†…å®¹",
                        "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ",
                        "å›¾ç‰‡ä¸­çš„ä¸»è¦å…ƒç´ æ˜¯ä»€ä¹ˆï¼Ÿ",
                        "è¯·è¯¦ç»†æè¿°å›¾ç‰‡ä¸­çš„åœºæ™¯",
                        "è¿™å¼ å›¾ç‰‡ä¼ è¾¾äº†ä»€ä¹ˆæƒ…æ„Ÿæˆ–æ°›å›´ï¼Ÿ"
                    ],
                    inputs=msg_input,
                    label="ğŸ’¡ ç¤ºä¾‹é—®é¢˜"
                )
        
        # äº¤äº’é€»è¾‘
        def _ensure_history(history):
            return history or []

        def user_message(message, history, image_path):
            """å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
            if not message.strip():
                return "", history, image_path
            
            history = _ensure_history(history)

            if image_path is None:
                if chatbot_format == "messages":
                    history = history + [
                        {"role": "user", "content": "è¯·å…ˆä¸Šä¼ å›¾ç‰‡"},
                        {"role": "assistant", "content": "âŒ è¯·å…ˆä¸Šä¼ ä¸€å¼ å›¾ç‰‡å†è¿›è¡Œå¯¹è¯"},
                    ]
                else:
                    history = history + [("è¯·å…ˆä¸Šä¼ å›¾ç‰‡", "âŒ è¯·å…ˆä¸Šä¼ ä¸€å¼ å›¾ç‰‡å†è¿›è¡Œå¯¹è¯")]
                return "", history, image_path
            
            # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯ï¼ˆåŒ…å«å›¾ç‰‡ï¼‰
            image_html = f'<img src="file/{image_path}" style="max-width:100px;max-height:100px;border-radius:8px;margin-bottom:5px;"><br>'
            if chatbot_format == "messages":
                history = history + [{"role": "user", "content": f"{image_html}{message}"}]
            else:
                history = history + [(f"{image_html}{message}", None)]
            return "", history, image_path
        
        def bot_response(history, image_path, temperature, top_p, max_tokens):
            """ç”Ÿæˆæœºå™¨äººå›å¤"""
            history = _ensure_history(history)
            if not history:
                return history

            if chatbot_format == "messages":
                last_msg = history[-1]
                if last_msg.get("role") == "assistant" and last_msg.get("content"):
                    return history
                if last_msg.get("role") == "assistant":
                    user_message_text = history[-2]["content"] if len(history) >= 2 else ""
                else:
                    user_message_text = last_msg.get("content", "")
                    history.append({"role": "assistant", "content": ""})
            else:
                if history[-1][1] is not None:
                    return history
                user_message_text = history[-1][0]

            # æå–çº¯æ–‡æœ¬ï¼ˆå»æ‰å›¾ç‰‡ HTMLï¼‰
            import re
            text_only = re.sub(r'<img[^>]*>', '', user_message_text).strip()
            
            # ç”Ÿæˆå›å¤
            response = ""
            for new_text in generate_response(image_path, text_only, temperature, top_p, max_tokens):
                response += new_text
                if chatbot_format == "messages":
                    history[-1]["content"] = response
                else:
                    history[-1] = (user_message_text, response)
                yield history
        
        def clear_chat():
            """æ¸…ç©ºå¯¹è¯"""
            return None, []
        
        # ç»‘å®šäº‹ä»¶
        msg_input.submit(
            user_message,
            [msg_input, chatbot, image_input],
            [msg_input, chatbot, current_image],
            queue=False
        ).then(
            bot_response,
            [chatbot, current_image, temperature_slider, top_p_slider, max_tokens_slider],
            chatbot
        )
        
        submit_btn.click(
            user_message,
            [msg_input, chatbot, image_input],
            [msg_input, chatbot, current_image],
            queue=False
        ).then(
            bot_response,
            [chatbot, current_image, temperature_slider, top_p_slider, max_tokens_slider],
            chatbot
        )
        
        clear_btn.click(
            clear_chat,
            None,
            [image_input, chatbot],
            queue=False
        )
        
        # å›¾ç‰‡æ›´æ–°æ—¶æ›´æ–°çŠ¶æ€
        image_input.change(
            lambda x: x,
            inputs=image_input,
            outputs=current_image
        )
    
    return demo


def main():
    global args
    
    parser = argparse.ArgumentParser(description="VerMind-V Web Demo")
    parser.add_argument(
        '--model_path',
        type=str,
        required=True,
        help="æ¨¡å‹è·¯å¾„ï¼ˆåŒ…å« config.json å’Œæ¨¡å‹æƒé‡çš„ç›®å½•ï¼‰"
    )
    parser.add_argument(
        '--device',
        type=str,
        default='cuda' if torch.cuda.is_available() else 'cpu',
        help="è¿è¡Œè®¾å¤‡ (cuda/cpu)"
    )
    parser.add_argument(
        '--host',
        type=str,
        default='0.0.0.0',
        help="æœåŠ¡å™¨ä¸»æœºåœ°å€"
    )
    parser.add_argument(
        '--port',
        type=int,
        default=7860,
        help="æœåŠ¡å™¨ç«¯å£"
    )
    parser.add_argument(
        '--share',
        action='store_true',
        help="åˆ›å»ºå…¬å¼€åˆ†äº«é“¾æ¥ï¼ˆGradio Tunnelï¼‰"
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    if not os.path.exists(args.model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
        sys.exit(1)
    
    # åˆå§‹åŒ–æ¨¡å‹
    init_model(args.model_path, args.device)
    
    # åˆ›å»ºå¹¶å¯åŠ¨ç•Œé¢
    demo = create_demo()
    
    print(f"\nğŸš€ å¯åŠ¨ Web Demo...")
    print(f"ğŸ”— è®¿é—®åœ°å€: http://{args.host}:{args.port}")
    if args.share:
        print("ğŸŒ å…¬å¼€åˆ†äº«é“¾æ¥å·²å¯ç”¨")
    
    demo.queue().launch(
        server_name=args.host,
        server_port=args.port,
        share=args.share,
        show_error=True
    )


if __name__ == "__main__":
    main()
